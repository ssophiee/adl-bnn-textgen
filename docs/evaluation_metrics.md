## BLEU
- Short [video](https://www.youtube.com/watch?v=M05L1DhFqcw)

Idea: BLEU (Bilingual Evaluation Understudy) compares the machine-generated translation to one or more reference translations and calculates a score based on the overlap of n-grams (contiguous sequences of n items) between the candidate and reference translations.

Note: Use SacreBLEU which handles tokenization.