{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7614a2dd",
   "metadata": {},
   "source": [
    "COLAB - https://colab.research.google.com/drive/1wFBi_V3qnOIT5e9CnqN3-IJSSltbIM2t?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0349b917",
   "metadata": {},
   "source": [
    "# Token-Level NanoGPT Training - Terminal Commands\n",
    "\n",
    "This notebook contains all the terminal commands needed to train a token-level NanoGPT model on Shakespeare data. Each cell can be executed to run the commands step by step.\n",
    "\n",
    "## ðŸŽ¯ **Purpose**\n",
    "- Fix the vocabulary mismatch issue (model vocab_size=65 vs data tokens>50,000)\n",
    "- Train with proper tiktoken GPT-2 BPE tokenization (vocab_size â‰ˆ 50,257)\n",
    "- Create a compatible model for evaluation\n",
    "\n",
    "## ðŸ“‹ **What We'll Do**\n",
    "1. Check dependencies and setup\n",
    "2. Prepare token-level Shakespeare data\n",
    "3. Create training configuration\n",
    "4. Run training\n",
    "5. Generate text samples\n",
    "6. Test the trained model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a49fc3",
   "metadata": {},
   "source": [
    "## Step 1: Check Environment and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71001ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current working directory and available resources\n",
    "import os\n",
    "import sys\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Check if we have the required packages\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch not found\")\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "    print(f\"tiktoken available\")\n",
    "except ImportError:\n",
    "    print(\"tiktoken not found\")\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(f\"NumPy version: {np.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"NumPy not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aa275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if missing\n",
    "!pip install torch numpy transformers datasets tiktoken wandb tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25ee1f8",
   "metadata": {},
   "source": [
    "## Step 2: Navigate to nanoGPT Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af313a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to nanoGPT directory\n",
    "import os\n",
    "os.chdir('nanoGPT')\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# List contents to verify we're in the right place\n",
    "print(\"\\nDirectory contents:\")\n",
    "for item in sorted(os.listdir('.')):\n",
    "    print(f\"  {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665bfe6a",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Token-Level Shakespeare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7c2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Shakespeare data with tiktoken BPE tokenization\n",
    "# This creates train.bin and val.bin with proper token-level encoding\n",
    "\n",
    "print(\"Preparing Shakespeare data with tiktoken BPE tokenization...\")\n",
    "!cd data/shakespeare && python prepare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f0cbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the data was created successfully\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "data_path = \"data/shakespeare\"\n",
    "train_path = os.path.join(data_path, \"train.bin\")\n",
    "val_path = os.path.join(data_path, \"val.bin\")\n",
    "\n",
    "if os.path.exists(train_path) and os.path.exists(val_path):\n",
    "    print(\"Data files created successfully!\")\n",
    "    \n",
    "    # Load and inspect the data\n",
    "    train_data = np.memmap(train_path, dtype=np.uint16, mode='r')\n",
    "    val_data = np.memmap(val_path, dtype=np.uint16, mode='r')\n",
    "    \n",
    "    print(f\"Training data: {len(train_data):,} tokens\")\n",
    "    print(f\"Validation data: {len(val_data):,} tokens\")\n",
    "    print(f\"Token range: {train_data.min()} to {train_data.max()}\")\n",
    "    print(f\"Data type: {train_data.dtype}\")\n",
    "    \n",
    "    # Verify this matches tiktoken vocab size\n",
    "    import tiktoken\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    print(f\"Tiktoken vocab size: {enc.n_vocab}\")\n",
    "    \n",
    "    if train_data.max() < enc.n_vocab:\n",
    "        print(\"Token range is compatible with tiktoken vocabulary!\")\n",
    "    else:\n",
    "        print(\"Token range exceeds tiktoken vocabulary!\")\n",
    "else:\n",
    "    print(\"Data files not found. Check the prepare.py script.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a09547",
   "metadata": {},
   "source": [
    "## Step 4: Create Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a28065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration file for token-level training\n",
    "config_content = '''# Token-level Shakespeare training configuration\n",
    "# This config is designed for GPT-2 BPE tokenization (vocab_size ~50257)\n",
    "\n",
    "import torch\n",
    "\n",
    "out_dir = 'out-shakespeare-token'\n",
    "eval_interval = 500\n",
    "eval_iters = 100\n",
    "log_interval = 10\n",
    "wandb_log = False  # Set to True if you want to use wandb\n",
    "\n",
    "# Dataset\n",
    "dataset = 'shakespeare'  # Uses tiktoken BPE tokenized data\n",
    "\n",
    "# Model architecture - adjusted for token-level training\n",
    "n_layer = 8          # Increased layers for token complexity\n",
    "n_head = 8           # Increased attention heads\n",
    "n_embd = 512         # Increased embedding dimension\n",
    "dropout = 0.1        # Slightly lower dropout\n",
    "bias = False         # No bias in linear layers (modern practice)\n",
    "\n",
    "# Training hyperparameters\n",
    "batch_size = 8                    # Smaller batch size due to larger vocab\n",
    "gradient_accumulation_steps = 8   # Effective batch size = 8 * 8 = 64\n",
    "max_iters = 3000                  # More iterations needed for convergence\n",
    "learning_rate = 3e-4              # Standard learning rate\n",
    "weight_decay = 1e-1               # L2 regularization\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0                   # Gradient clipping\n",
    "\n",
    "# Learning rate schedule\n",
    "decay_lr = True\n",
    "warmup_iters = 100\n",
    "lr_decay_iters = 3000  # Should be ~= max_iters\n",
    "min_lr = 3e-5          # min_lr = learning_rate / 10\n",
    "\n",
    "# Context length\n",
    "block_size = 512      # Moderate context length for token-level\n",
    "\n",
    "# System\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "compile = True        # PyTorch 2.0 compile for speed\n",
    "\n",
    "# Save checkpoint settings\n",
    "always_save_checkpoint = False  # Only save when validation improves\n",
    "'''\n",
    "\n",
    "# Write the configuration file\n",
    "config_path = \"config/train_shakespeare_token.py\"\n",
    "with open(config_path, 'w') as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "print(f\"Created configuration file: {config_path}\")\n",
    "print(\"Configuration summary:\")\n",
    "print(\"  - Model: 8 layers, 8 heads, 512 embedding\")\n",
    "print(\"  - Batch size: 8 (effective: 64 with gradient accumulation)\")\n",
    "print(\"  - Max iterations: 3000\")\n",
    "print(\"  - Block size: 512 tokens\")\n",
    "print(\"  - Output directory: out-shakespeare-token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7b14f9",
   "metadata": {},
   "source": [
    "## Step 5: Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf576fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training the token-level model\n",
    "print(\"Starting token-level NanoGPT training...\")\n",
    "print(\"This may take 30-60 minutes on GPU, or several hours on CPU\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run the training command\n",
    "!python train.py config/train_shakespeare_token.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9878a02",
   "metadata": {},
   "source": [
    "### Alternative: Training with Custom Parameters\n",
    "\n",
    "If you want to adjust training parameters or run on CPU/different hardware:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466fa4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION A: For CPU training (slower but works on any machine)\n",
    "# Uncomment the line below if you want to train on CPU\n",
    "# !python train.py config/train_shakespeare_token.py --device=cpu --compile=False --batch_size=4\n",
    "\n",
    "# OPTION B: For Apple Silicon Mac (M1/M2)\n",
    "# Uncomment the line below if you're on Apple Silicon\n",
    "# !python train.py config/train_shakespeare_token.py --device=mps --batch_size=4\n",
    "\n",
    "# OPTION C: Quick training (fewer iterations, for testing)\n",
    "# Uncomment the line below for a quick test run\n",
    "# !python train.py config/train_shakespeare_token.py --max_iters=500 --eval_interval=100\n",
    "\n",
    "print(\"TIP: Uncomment one of the alternative training commands above if needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc402b6d",
   "metadata": {},
   "source": [
    "## Step 6: Monitor Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86b3974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training progress and output directory\n",
    "import os\n",
    "import glob\n",
    "\n",
    "output_dir = \"out-shakespeare-token\"\n",
    "if os.path.exists(output_dir):\n",
    "    print(f\"Training output directory exists: {output_dir}\")\n",
    "    \n",
    "    # List files in output directory\n",
    "    files = os.listdir(output_dir)\n",
    "    print(f\"Files in output directory:\")\n",
    "    for file in sorted(files):\n",
    "        file_path = os.path.join(output_dir, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            size = os.path.getsize(file_path)\n",
    "            print(f\"  {file} ({size:,} bytes)\")\n",
    "    \n",
    "    # Check if training log exists\n",
    "    log_file = os.path.join(output_dir, \"log.txt\")\n",
    "    if os.path.exists(log_file):\n",
    "        print(f\"\\nLast few lines of training log:\")\n",
    "        with open(log_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines[-10:]:  # Show last 10 lines\n",
    "                print(f\"  {line.strip()}\")\n",
    "else:\n",
    "    print(f\"ERROR: Training output directory not found: {output_dir}\")\n",
    "    print(\"Training may still be in progress or failed to start.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991ea65a",
   "metadata": {},
   "source": [
    "## Step 7: Generate Text Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a896e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text samples from the trained model\n",
    "print(\"Generating text samples from trained model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "!python sample.py --out_dir=out-shakespeare-token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3a4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate custom text samples with specific prompts\n",
    "print(\"Generating custom text samples...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Sample with specific characters\n",
    "!python sample.py --out_dir=out-shakespeare-token --start=\"HAMLET:\" --num_samples=2 --max_new_tokens=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214058b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More creative prompts\n",
    "print(\"\\nMore creative generation examples...\")\n",
    "\n",
    "# Famous Shakespeare quote continuation\n",
    "!python sample.py --out_dir=out-shakespeare-token --start=\"To be or not to be,\" --num_samples=1 --max_new_tokens=100 --temperature=0.8\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "\n",
    "# Different character\n",
    "!python sample.py --out_dir=out-shakespeare-token --start=\"JULIET:\" --num_samples=1 --max_new_tokens=100 --temperature=0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4567d7db",
   "metadata": {},
   "source": [
    "## Step 8: Test Model Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ff69b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that our trained model is compatible with the evaluation notebook\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "output_dir = \"out-shakespeare-token\"\n",
    "model_path = f\"{output_dir}/ckpt.pt\"\n",
    "config_path = f\"{output_dir}/config.pkl\"\n",
    "\n",
    "if os.path.exists(model_path) and os.path.exists(config_path):\n",
    "    print(\"Model files found!\")\n",
    "    \n",
    "    # Load model configuration\n",
    "    with open(config_path, 'rb') as f:\n",
    "        config = pickle.load(f)\n",
    "    \n",
    "    print(f\"Model Configuration:\")\n",
    "    print(f\"  Vocabulary size: {config.vocab_size}\")\n",
    "    print(f\"  Block size: {config.block_size}\")\n",
    "    print(f\"  Number of layers: {config.n_layer}\")\n",
    "    print(f\"  Number of heads: {config.n_head}\")\n",
    "    print(f\"  Embedding dimension: {config.n_embd}\")\n",
    "    \n",
    "    # Load model checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    print(f\"Checkpoint info:\")\n",
    "    print(f\"  Training iteration: {checkpoint.get('iter_num', 'Unknown')}\")\n",
    "    print(f\"  Best validation loss: {checkpoint.get('best_val_loss', 'Unknown')}\")\n",
    "    \n",
    "    # Verify vocabulary compatibility\n",
    "    data_path = \"data/shakespeare/train.bin\"\n",
    "    if os.path.exists(data_path):\n",
    "        train_data = np.memmap(data_path, dtype=np.uint16, mode='r')\n",
    "        max_token = train_data.max()\n",
    "        \n",
    "        print(f\"Data compatibility check:\")\n",
    "        print(f\"  Data max token: {max_token}\")\n",
    "        print(f\"  Model vocab size: {config.vocab_size}\")\n",
    "        \n",
    "        if max_token < config.vocab_size:\n",
    "            print(\"COMPATIBILITY SUCCESS! Data tokens fit within model vocabulary.\")\n",
    "            print(\"This model can be used with the evaluation notebook!\")\n",
    "        else:\n",
    "            print(\"COMPATIBILITY ISSUE: Data tokens exceed model vocabulary.\")\n",
    "    \n",
    "else:\n",
    "    print(\"ERROR: Model files not found. Training may not have completed successfully.\")\n",
    "    if not os.path.exists(output_dir):\n",
    "        print(f\"ERROR: Output directory doesn't exist: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9f318b",
   "metadata": {},
   "source": [
    "## Step 9: Copy Model for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the trained model to the checkpoints directory for use with evaluation notebook\n",
    "import shutil\n",
    "\n",
    "# Create destination directory\n",
    "dest_dir = \"../checkpoints/token_level_nanogpt\"\n",
    "os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "# Copy model files\n",
    "source_dir = \"out-shakespeare-token\"\n",
    "if os.path.exists(source_dir):\n",
    "    # Copy checkpoint\n",
    "    if os.path.exists(f\"{source_dir}/ckpt.pt\"):\n",
    "        shutil.copy2(f\"{source_dir}/ckpt.pt\", f\"{dest_dir}/token_level_nanogpt.pt\")\n",
    "        print(f\"Copied model checkpoint to {dest_dir}/token_level_nanogpt.pt\")\n",
    "    \n",
    "    # Copy config\n",
    "    if os.path.exists(f\"{source_dir}/config.pkl\"):\n",
    "        shutil.copy2(f\"{source_dir}/config.pkl\", f\"{dest_dir}/token_level_meta.pkl\")\n",
    "        print(f\"Copied model config to {dest_dir}/token_level_meta.pkl\")\n",
    "    \n",
    "    print(f\"\\nFiles in checkpoint directory:\")\n",
    "    for file in os.listdir(dest_dir):\n",
    "        file_path = os.path.join(dest_dir, file)\n",
    "        size = os.path.getsize(file_path)\n",
    "        print(f\"  {file} ({size:,} bytes)\")\n",
    "    \n",
    "    print(f\"\\nTo use this model in the evaluation notebook, update the config:\")\n",
    "    print(f\"  model_path: '../checkpoints/token_level_nanogpt/token_level_nanogpt.pt'\")\n",
    "    print(f\"  meta_path: '../checkpoints/token_level_nanogpt/token_level_meta.pkl'\")\n",
    "    print(f\"  data_dir: 'nanoGPT/data/shakespeare'\")\n",
    "    \n",
    "else:\n",
    "    print(\"ERROR: Source directory not found. Training may not have completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe01868",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Training Complete!\n",
    "\n",
    "### Summary of What We Accomplished:\n",
    "\n",
    "1. âœ… **Prepared token-level data** using tiktoken GPT-2 BPE tokenization\n",
    "2. âœ… **Created proper configuration** for token-level training\n",
    "3. âœ… **Trained the model** with vocabulary size â‰ˆ 50,257 (compatible with data)\n",
    "4. âœ… **Generated text samples** to verify model quality\n",
    "5. âœ… **Verified compatibility** between model and data\n",
    "6. âœ… **Copied model files** to checkpoint directory for evaluation\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Use the evaluation notebook** with the new model:\n",
    "   - Update paths in the evaluation notebook config\n",
    "   - Run evaluation to get proper metrics (no more infinite perplexity!)\n",
    "\n",
    "2. **Compare results** between character-level and token-level models\n",
    "\n",
    "3. **Experiment further**:\n",
    "   - Try different hyperparameters\n",
    "   - Train for longer\n",
    "   - Fine-tune from pre-trained GPT-2\n",
    "\n",
    "### Key Files Created:\n",
    "- **Model**: `../checkpoints/token_level_nanogpt/token_level_nanogpt.pt`\n",
    "- **Metadata**: `../checkpoints/token_level_nanogpt/token_level_meta.pkl`\n",
    "- **Data**: `nanoGPT/data/shakespeare/train.bin` and `val.bin`\n",
    "- **Config**: `nanoGPT/config/train_shakespeare_token.py`\n",
    "\n",
    "The vocabulary mismatch issue is now **resolved**! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
