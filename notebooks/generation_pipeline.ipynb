{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3184a9c3",
   "metadata": {},
   "source": [
    "##### Text Generation Pipeline. Overview\n",
    "\n",
    "The notebook provides two generation pipelines:\n",
    "\n",
    "1. **Standard Model Generation**: Uses a deterministic transformer model to generate multiple text samples with nucleus sampling and temperature scaling.\n",
    "\n",
    "2. **Bayesian SGMCMC Generation**: Uses Stochastic Gradient MCMC methods (SGLD, SGHMC, or BAOA) to generate text with uncertainty quantification. This approach leverages multiple weight samples collected during training to capture model uncertainty.\n",
    "\n",
    "Features: <br>\n",
    "\n",
    "- **Standard Generation**: Fast inference with traditional sampling strategies (temperature, top-k)\n",
    "- **Bayesian Generation**: Uncertainty-aware generation using collected SGMCMC samples\n",
    "- **Result Persistence**: Automatically saves generation results with metadata to JSON files\n",
    "- **Configurable Parameters**: Control generation length, sampling strategies, and number of samples\n",
    "\n",
    "<b> Usage</b>: Simply run the cells to generate text with either model type. Results are saved to `checkpoints/generation_results/` for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72aeaf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1060497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import MODEL_PATH, META_PATH, BNN_MODEL_PATH\n",
    "from src.generation_utils import generate_text_standard, save_generation_result, generate_text_bayesian_sgmcmc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1593be42",
   "metadata": {},
   "source": [
    "For Standard Models (non-Bayesian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be2cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "texts = generate_text_standard(\n",
    "    model_path=MODEL_PATH,\n",
    "    start_prompt=\"to be, or not to be;\",\n",
    "    max_new_tokens=500,\n",
    "    temperature=0.8,\n",
    "    top_k=200,\n",
    "    num_samples=3\n",
    ")\n",
    "\n",
    "# Save results\n",
    "save_path = os.path.join(root_path, \"checkpoints/generation_results/generation_results_standard.json\")\n",
    "save_generation_result(\n",
    "    start_prompt=\"to be, or not to be;\",\n",
    "    texts=texts,\n",
    "    max_new_tokens=500,\n",
    "    temperature=0.8,\n",
    "    top_k=200,\n",
    "    num_samples=3,\n",
    "    model_path=MODEL_PATH,\n",
    "    save_path=save_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ceca7a",
   "metadata": {},
   "source": [
    "For Bayesian Models (SGMCMC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55e5a9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 collected samples from checkpoint\n",
      "number of parameters: 10.65M\n",
      "Using 10 SGMCMC samples for generation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Saved generation result with ID: 20251118_151801 to /Users/sofianikolenko/Downloads/Projects_25/ADL/adl-bnn-textgen/checkpoints/generation_results/generation_results_sgmcmc.json'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate text using Bayesian SGMCMC\n",
    "text, unc_info = generate_text_bayesian_sgmcmc(\n",
    "    model_path=BNN_MODEL_PATH,\n",
    "    start_prompt=\"to be, or not to be;\",\n",
    "    max_new_tokens=600,\n",
    "    temperature=0.8,\n",
    "    top_k=10,\n",
    "    num_samples=10 # Use 20 of the collected samples (~100 total available)\n",
    ")\n",
    "\n",
    "# Save results\n",
    "save_path = os.path.join(root_path, \"checkpoints/generation_results/generation_results_sgmcmc.json\")\n",
    "save_generation_result(\n",
    "    start_prompt=\"to be, or not to be;\",\n",
    "    texts=text, \n",
    "    max_new_tokens=600,\n",
    "    temperature=0.8,\n",
    "    top_k=10,\n",
    "    num_samples=10,\n",
    "    model_path=MODEL_PATH,\n",
    "    save_path=save_path,\n",
    "    unc_info=unc_info,\n",
    "    has_collected_samples=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5137a8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be, or not to be; by this is he as he\n",
      "hath some courage of a place and call it\n",
      "accused at all a foot-garden plack that we have made\n",
      "me tribunes to her fail of thy state order.\n",
      "\n",
      "ESCALUS:\n",
      "Here is a shepherd when he makes me her sons to avoid there\n",
      "both, and such as to the bosom of the world.\n",
      "\n",
      "ELBOW:\n",
      "Thou dost sand this better to complay from thy sorrow;\n",
      "how came to scope us thy desire through's friends for a fight,\n",
      "bloody that he was made the much other to execute\n",
      "and take me well as my father's breast\n",
      "common of thee a most thrice and stumbling.\n",
      "\n",
      "Clown:\n",
      "Thy blood ways in thy friends shall be come to thy beast; I\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
