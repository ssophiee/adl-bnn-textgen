{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d0da394",
   "metadata": {},
   "source": [
    "# NanoGPT Bayesian Neural Network Inference\n",
    "\n",
    "This notebook applies Bayesian Neural Network (BNN) inference to a trained NanoGPT model using the `posteriors` library. We'll load a pre-trained model from the checkpoint folder and perform variational inference to learn a posterior distribution over the model parameters.\n",
    "\n",
    "## Overview\n",
    "- Load trained NanoGPT model and tokenizer\n",
    "- Set up Bayesian inference with variational inference (VI)\n",
    "- Train posterior distribution over model parameters\n",
    "- Compare deterministic vs. Bayesian predictions\n",
    "- Generate text with uncertainty quantification\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d2e5e0",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1405903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0925 02:42:20.971000 5828 Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "c:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\bnn\\Lib\\site-packages\\posteriors\\utils.py:9: FutureWarning: The 'optree.integration' module is deprecated and will be removed in version 0.18.0. Please use 'optree.integrations' instead.\n",
      "  from optree.integration.torch import tree_ravel\n",
      "c:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\bnn\\Lib\\site-packages\\posteriors\\utils.py:9: FutureWarning: The 'optree.integration' module is deprecated and will be removed in version 0.18.0. Please use 'optree.integrations' instead.\n",
      "  from optree.integration.torch import tree_ravel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\notebooks\n",
      "PyTorch version: 2.8.0+cpu\n",
      "CUDA available: False\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import func\n",
    "\n",
    "# Import Bayesian libraries\n",
    "import torchopt\n",
    "import posteriors\n",
    "\n",
    "# Add paths for importing utilities and models\n",
    "current_dir = Path.cwd()\n",
    "sys.path.append(str(current_dir))\n",
    "sys.path.append(str(current_dir / \"baselines\"))\n",
    "\n",
    "# Import our utilities\n",
    "from utils import load_model, load_tokenizer, encode, decode\n",
    "\n",
    "print(f\"Current directory: {current_dir}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45516298",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "230c65be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration (DEBUG MODE):\n",
      "  model_path: ../checkpoints/baseline_nanogpt/baseline_nanogpt.pt\n",
      "  meta_path: ../checkpoints/baseline_nanogpt/nanogpt_meta.pkl\n",
      "  data_dir: nanoGPT/data/shakespeare_char\n",
      "  batch_size: 4\n",
      "  num_epochs: 2\n",
      "  learning_rate: 0.001\n",
      "  temperature: 1.0\n",
      "  prior_std: 1.0\n",
      "  max_new_tokens: 50\n",
      "  generation_temperature: 0.8\n",
      "  num_samples: 3\n",
      "  max_seq_length: 64\n",
      "  train_samples: 20\n",
      "\n",
      "Checking paths:\n",
      "model_path: ..\\checkpoints\\baseline_nanogpt\\baseline_nanogpt.pt\n",
      "meta_path: ..\\checkpoints\\baseline_nanogpt\\nanogpt_meta.pkl\n",
      "data_dir: nanoGPT\\data\\shakespeare_char\n"
     ]
    }
   ],
   "source": [
    "# Configuration for Bayesian NanoGPT\n",
    "CONFIG = {\n",
    "    # Model paths - choose one of the available checkpoints\n",
    "    'model_path': '../checkpoints/baseline_nanogpt/baseline_nanogpt.pt',\n",
    "    'meta_path': '../checkpoints/baseline_nanogpt/nanogpt_meta.pkl',\n",
    "    'data_dir': 'nanoGPT/data/shakespeare_char',\n",
    "    \n",
    "    # Alternative: Use token-level model if available\n",
    "    # 'model_path': '../checkpoints/token_level_nanogpt/token_level_nanogpt.pt',\n",
    "    # 'meta_path': '../checkpoints/token_level_nanogpt/token_level_meta.pkl',\n",
    "    # 'data_dir': 'nanoGPT/data/shakespeare',\n",
    "    \n",
    "    # Bayesian inference parameters (reduced for debugging)\n",
    "    'batch_size': 4,        # Reduced batch size\n",
    "    'num_epochs': 2,        # Reduced epochs\n",
    "    'learning_rate': 1e-3,  # Slightly higher learning rate\n",
    "    'temperature': 1.0,     # Higher temperature\n",
    "    'prior_std': 1.0,       # Prior standard deviation\n",
    "    \n",
    "    # Text generation parameters\n",
    "    'max_new_tokens': 50,   # Reduced for faster testing\n",
    "    'generation_temperature': 0.8,\n",
    "    'num_samples': 3,       # Fewer samples for testing\n",
    "    \n",
    "    # Data parameters (much reduced for debugging)\n",
    "    'max_seq_length': 64,   # Shorter sequences\n",
    "    'train_samples': 20,    # Much fewer samples\n",
    "}\n",
    "\n",
    "print(\"Configuration (DEBUG MODE):\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Check if paths exist\n",
    "print(\"\\nChecking paths:\")\n",
    "for path_key in ['model_path', 'meta_path', 'data_dir']:\n",
    "    path = Path(CONFIG[path_key])\n",
    "    if path.exists():\n",
    "        print(f\"{path_key}: {path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"{path_key} does not exist: {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7f516c",
   "metadata": {},
   "source": [
    "## 3. Load Pre-trained NanoGPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41603fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained NanoGPT model...\n",
      "Loading model from: ..\\checkpoints\\baseline_nanogpt\\baseline_nanogpt.pt\n",
      "Model arguments: {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'block_size': 256, 'bias': False, 'vocab_size': 65, 'dropout': 0.2}\n",
      "Model arguments: {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'block_size': 256, 'bias': False, 'vocab_size': 65, 'dropout': 0.2}\n",
      "number of parameters: 10.65M\n",
      "Model loaded successfully!\n",
      "Number of parameters: 10,745,088\n",
      "Model loaded successfully!\n",
      "Model parameters: 10,745,088\n",
      "Vocabulary size: 65\n",
      "Model architecture:\n",
      "  - Layers: 6\n",
      "  - Heads: 6\n",
      "  - Embedding dim: 384\n",
      "  - Block size: 256\n",
      "Number of parameter tensors: 39\n",
      "Parameter shapes:\n",
      "  transformer.wte.weight: torch.Size([65, 384])\n",
      "  transformer.wpe.weight: torch.Size([256, 384])\n",
      "  transformer.h.0.ln_1.weight: torch.Size([384])\n",
      "  transformer.h.0.attn.c_attn.weight: torch.Size([1152, 384])\n",
      "  transformer.h.0.attn.c_proj.weight: torch.Size([384, 384])\n",
      "  ... and 34 more parameter tensors\n",
      "number of parameters: 10.65M\n",
      "Model loaded successfully!\n",
      "Number of parameters: 10,745,088\n",
      "Model loaded successfully!\n",
      "Model parameters: 10,745,088\n",
      "Vocabulary size: 65\n",
      "Model architecture:\n",
      "  - Layers: 6\n",
      "  - Heads: 6\n",
      "  - Embedding dim: 384\n",
      "  - Block size: 256\n",
      "Number of parameter tensors: 39\n",
      "Parameter shapes:\n",
      "  transformer.wte.weight: torch.Size([65, 384])\n",
      "  transformer.wpe.weight: torch.Size([256, 384])\n",
      "  transformer.h.0.ln_1.weight: torch.Size([384])\n",
      "  transformer.h.0.attn.c_attn.weight: torch.Size([1152, 384])\n",
      "  transformer.h.0.attn.c_proj.weight: torch.Size([384, 384])\n",
      "  ... and 34 more parameter tensors\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained NanoGPT model and tokenizer\n",
    "print(\"Loading pre-trained NanoGPT model...\")\n",
    "\n",
    "try:\n",
    "    # Load model\n",
    "    model, checkpoint = load_model(Path(CONFIG['model_path']), device)\n",
    "    print(f\"Model loaded successfully!\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    stoi, itos = load_tokenizer(Path(CONFIG['meta_path']))\n",
    "    vocab_size = len(itos)\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    # Get model architecture info\n",
    "    print(f\"Model architecture:\")\n",
    "    print(f\"  - Layers: {model.config.n_layer}\")\n",
    "    print(f\"  - Heads: {model.config.n_head}\")\n",
    "    print(f\"  - Embedding dim: {model.config.n_embd}\")\n",
    "    print(f\"  - Block size: {model.config.block_size}\")\n",
    "    \n",
    "    # Extract model parameters for posteriors\n",
    "    params = dict(model.named_parameters())\n",
    "    print(f\"Number of parameter tensors: {len(params)}\")\n",
    "    \n",
    "    # Show parameter tensor shapes\n",
    "    print(\"Parameter shapes:\")\n",
    "    for name, param in list(params.items())[:5]:  # Show first 5\n",
    "        print(f\"  {name}: {param.shape}\")\n",
    "    if len(params) > 5:\n",
    "        print(f\"  ... and {len(params) - 5} more parameter tensors\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    model, params, stoi, itos = None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37367eb6",
   "metadata": {},
   "source": [
    "## 4. Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85856a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n",
      "Loaded data: 1,003,854 tokens\n",
      "Created 5 training batches\n",
      "Batch shape: torch.Size([4, 64])\n",
      "Target shape: torch.Size([4, 1])\n",
      "Total training samples: 20\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data for Bayesian inference\n",
    "print(\"Preparing training data...\")\n",
    "\n",
    "if model is not None:\n",
    "    # Load training data\n",
    "    train_data_path = Path(CONFIG['data_dir']) / 'train.bin'\n",
    "    \n",
    "    if train_data_path.exists():\n",
    "        # Load binary data\n",
    "        data = np.memmap(str(train_data_path), dtype=np.uint16, mode='r')\n",
    "        print(f\"Loaded data: {len(data):,} tokens\")\n",
    "        \n",
    "        # Create training batches for next-token prediction\n",
    "        def create_training_batches(data, batch_size, seq_length, num_samples):\n",
    "            \"\"\"Create training batches from the data for next-token prediction\"\"\"\n",
    "            batches = []\n",
    "            max_start = len(data) - seq_length - 1\n",
    "            \n",
    "            # Sample random starting positions\n",
    "            start_indices = np.random.choice(max_start, size=num_samples, replace=False)\n",
    "            \n",
    "            for i in range(0, len(start_indices), batch_size):\n",
    "                batch_starts = start_indices[i:i+batch_size]\n",
    "                x_batch = []\n",
    "                y_batch = []\n",
    "                \n",
    "                for start in batch_starts:\n",
    "                    # For next-token prediction, x is the sequence and y is the next token\n",
    "                    x_seq = data[start:start+seq_length].astype(np.int64)\n",
    "                    # y is just the last token (next token prediction)\n",
    "                    y_seq = data[start+seq_length:start+seq_length+1].astype(np.int64)\n",
    "                    x_batch.append(x_seq)\n",
    "                    y_batch.append(y_seq)\n",
    "                \n",
    "                x_tensor = torch.tensor(np.array(x_batch), device=device)\n",
    "                y_tensor = torch.tensor(np.array(y_batch), device=device)\n",
    "                batches.append((x_tensor, y_tensor))\n",
    "            \n",
    "            return batches\n",
    "        \n",
    "        # Create training batches\n",
    "        training_batches = create_training_batches(\n",
    "            data, \n",
    "            CONFIG['batch_size'], \n",
    "            CONFIG['max_seq_length'], \n",
    "            CONFIG['train_samples']\n",
    "        )\n",
    "        \n",
    "        print(f\"Created {len(training_batches)} training batches\")\n",
    "        print(f\"Batch shape: {training_batches[0][0].shape}\")\n",
    "        print(f\"Target shape: {training_batches[0][1].shape}\")\n",
    "        \n",
    "        # Calculate number of data points for posteriors\n",
    "        num_data = CONFIG['train_samples']\n",
    "        print(f\"Total training samples: {num_data}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Training data not found at {train_data_path}\")\n",
    "        training_batches = []\n",
    "        num_data = 0\n",
    "        \n",
    "else:\n",
    "    print(\"Model not loaded, skipping data preparation\")\n",
    "    training_batches = []\n",
    "    num_data = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c0a9a9",
   "metadata": {},
   "source": [
    "## 5. Define Log Posterior Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ba2721a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log posterior function defined for posteriors\n",
      "Testing log posterior function...\n",
      "Input shape: torch.Size([4, 64])\n",
      "Target shape: torch.Size([4, 1])\n",
      "num_data: 20\n",
      "Logits shape: torch.Size([4, 1, 65])\n",
      "Model block size: 256\n",
      "Model vocab size: 65\n",
      "Test successful!\n",
      "Log posterior value: -494057.5312\n",
      "Simple log posterior value: -494057.5938\n",
      "Loss value: 0.8578\n",
      "Ready for posteriors VI setup\n",
      "Test successful!\n",
      "Log posterior value: -494057.5312\n",
      "Simple log posterior value: -494057.5938\n",
      "Loss value: 0.8578\n",
      "Ready for posteriors VI setup\n"
     ]
    }
   ],
   "source": [
    "# Define the log posterior function for posteriors library\n",
    "def single_batch_loss(params, batch):\n",
    "    \"\"\"Compute loss for a single batch using functional_call\"\"\"\n",
    "    x, y = batch\n",
    "    \n",
    "    # Forward pass through the model using functional call\n",
    "    logits, _ = func.functional_call(model, params, (x,))\n",
    "    \n",
    "    # For next-token prediction, we expect:\n",
    "    # - logits shape: [batch_size, 1, vocab_size] (predicting next token)\n",
    "    # - targets shape: [batch_size, 1] (the actual next token)\n",
    "    \n",
    "    # Compute cross entropy loss\n",
    "    batch_size, seq_length, vocab_size = logits.shape\n",
    "    logits_flat = logits.view(-1, vocab_size)  # (batch_size * seq_length, vocab_size)\n",
    "    targets_flat = y.view(-1)  # (batch_size * seq_length,)\n",
    "    \n",
    "    loss = F.cross_entropy(logits_flat, targets_flat, reduction='mean')\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def log_posterior_fn(params, batch):\n",
    "    \"\"\"\n",
    "    Log posterior function compatible with posteriors library.\n",
    "    \n",
    "    Args:\n",
    "        params: Model parameters dictionary\n",
    "        batch: Tuple of (input_tokens, target_tokens)\n",
    "        \n",
    "    Returns:\n",
    "        log_posterior_value: Scalar tensor\n",
    "    \"\"\"\n",
    "    # Compute negative log likelihood\n",
    "    nll = single_batch_loss(params, batch)\n",
    "    \n",
    "    # Compute log prior more safely (avoiding potential tensor iteration issues)\n",
    "    log_prior = torch.tensor(0.0, device=device)\n",
    "    for param in params.values():\n",
    "        if param.requires_grad:\n",
    "            # Use normal distribution for prior - more robust computation\n",
    "            prior_dist = torch.distributions.Normal(0.0, CONFIG['prior_std'])\n",
    "            param_log_prob = prior_dist.log_prob(param).sum()\n",
    "            log_prior = log_prior + param_log_prob\n",
    "    \n",
    "    # Scale prior by number of data points (standard in Bayesian inference)\n",
    "    # Ensure num_data is a proper scalar\n",
    "    num_data_tensor = torch.tensor(float(num_data), device=device)\n",
    "    log_posterior = -nll + log_prior / num_data_tensor\n",
    "    \n",
    "    return log_posterior\n",
    "\n",
    "\n",
    "if model is not None and training_batches:\n",
    "    print(\"Log posterior function defined for posteriors\")\n",
    "    \n",
    "    # Test the log posterior function with a sample batch\n",
    "    print(\"Testing log posterior function...\")\n",
    "    test_batch = training_batches[0]\n",
    "    \n",
    "    try:\n",
    "        # Debug: Print shapes to verify they're correct\n",
    "        x_test, y_test = test_batch\n",
    "        print(f\"Input shape: {x_test.shape}\")\n",
    "        print(f\"Target shape: {y_test.shape}\")\n",
    "        print(f\"num_data: {num_data}\")\n",
    "        \n",
    "        # Test forward pass first\n",
    "        with torch.no_grad():\n",
    "            logits, _ = func.functional_call(model, params, (x_test,))\n",
    "            print(f\"Logits shape: {logits.shape}\")\n",
    "            \n",
    "            # Check model configuration\n",
    "            print(f\"Model block size: {model.config.block_size}\")\n",
    "            print(f\"Model vocab size: {model.config.vocab_size}\")\n",
    "        \n",
    "        # Test both versions\n",
    "        log_post_val = log_posterior_fn(params, test_batch)\n",
    "        simple_log_post_val = log_posterior_fn(params, test_batch)\n",
    "        loss_val = single_batch_loss(params, test_batch)\n",
    "        \n",
    "        print(f\"Test successful!\")\n",
    "        print(f\"Log posterior value: {log_post_val.item():.4f}\")\n",
    "        print(f\"Simple log posterior value: {simple_log_post_val.item():.4f}\")\n",
    "        print(f\"Loss value: {loss_val.item():.4f}\")\n",
    "        \n",
    "        # Test sampling functionality placeholder\n",
    "        print(\"Ready for posteriors VI setup\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"Cannot define log posterior - model or data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c98ab27",
   "metadata": {},
   "source": [
    "## 6. Setup Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "530e0629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up variational inference with posteriors...\n",
      "Using simplified log posterior function for debugging...\n",
      "Building VI transform with posteriors.vi.diag...\n",
      "VI transform created successfully!\n",
      "Initializing VI state...\n",
      "VI state initialized!\n",
      "State keys: Custom state object\n",
      "Ready for Bayesian training with posteriors!\n"
     ]
    }
   ],
   "source": [
    "# Setup variational inference using posteriors library\n",
    "if model is not None and training_batches:\n",
    "    print(\"Setting up variational inference with posteriors...\")\n",
    "    \n",
    "    try:\n",
    "        # Use the simplified log posterior function to avoid tensor iteration issues\n",
    "        print(\"Using simplified log posterior function for debugging...\")\n",
    "        \n",
    "        # Build the variational inference transform using posteriors.vi.diag\n",
    "        print(\"Building VI transform with posteriors.vi.diag...\")\n",
    "        \n",
    "        # Create optimizer\n",
    "        optimizer = torchopt.adam(lr=CONFIG['learning_rate'])\n",
    "        \n",
    "        # Build the VI transform with simplified log posterior\n",
    "        vi_transform = posteriors.vi.diag.build(\n",
    "            log_posterior=log_posterior_fn,  # Use simplified version\n",
    "            optimizer=optimizer,\n",
    "            temperature=CONFIG['temperature'],\n",
    "            n_samples=1  # Number of samples per VI update\n",
    "        )\n",
    "        \n",
    "        print(\"VI transform created successfully!\")\n",
    "        \n",
    "        # Initialize the variational state\n",
    "        print(\"Initializing VI state...\")\n",
    "        vi_state = vi_transform.init(params)\n",
    "        \n",
    "        print(\"VI state initialized!\")\n",
    "        print(f\"State keys: {list(vi_state._asdict().keys()) if hasattr(vi_state, '_asdict') else 'Custom state object'}\")\n",
    "        \n",
    "        print(\"Ready for Bayesian training with posteriors!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up posteriors VI: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        vi_transform = None\n",
    "        vi_state = None\n",
    "        \n",
    "else:\n",
    "    print(\"Cannot setup VI - model or data not available\")\n",
    "    vi_transform = None\n",
    "    vi_state = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eb2119",
   "metadata": {},
   "source": [
    "## 7. Run Bayesian Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed304a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Bayesian training with posteriors...\n",
      "==================================================\n",
      "\n",
      "Epoch 1/2\n",
      "Error in batch 1: iteration over a 0-d tensor\n",
      "Error in batch 1: iteration over a 0-d tensor\n",
      "Error in batch 2: iteration over a 0-d tensor\n",
      "Error in batch 2: iteration over a 0-d tensor\n",
      "Error in batch 3: iteration over a 0-d tensor\n",
      "Error in batch 3: iteration over a 0-d tensor\n",
      "Error in batch 4: iteration over a 0-d tensor\n",
      "Error in batch 4: iteration over a 0-d tensor\n",
      "Error in batch 5: iteration over a 0-d tensor\n",
      "Epoch 1 failed - no successful batches\n",
      " Training failed - no successful epochs\n",
      "Error in batch 5: iteration over a 0-d tensor\n",
      "Epoch 1 failed - no successful batches\n",
      " Training failed - no successful epochs\n"
     ]
    }
   ],
   "source": [
    "# Run Bayesian training using posteriors VI\n",
    "if vi_transform is not None and vi_state is not None:\n",
    "    print(\"Starting Bayesian training with posteriors...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Training metrics tracking\n",
    "    training_losses = []\n",
    "    log_posterior_values = []\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(CONFIG['num_epochs']):\n",
    "            epoch_losses = []\n",
    "            epoch_log_posts = []\n",
    "            print(f\"\\nEpoch {epoch + 1}/{CONFIG['num_epochs']}\")\n",
    "            \n",
    "            for batch_idx, batch in enumerate(training_batches):\n",
    "                try:\n",
    "                    # Update the variational state using posteriors\n",
    "                    vi_state = vi_transform.update(vi_state, batch)\n",
    "                    \n",
    "                    # Evaluate current performance\n",
    "                    with torch.no_grad():\n",
    "                        # Get current parameter estimates (posterior mean)\n",
    "                        current_loss = single_batch_loss(vi_state.params, batch)\n",
    "                        current_log_post = log_posterior_fn(vi_state.params, batch)\n",
    "                        \n",
    "                        epoch_losses.append(current_loss.item())\n",
    "                        epoch_log_posts.append(current_log_post.item())\n",
    "                    \n",
    "                    # Print progress every 2 batches (since we have fewer batches)\n",
    "                    if (batch_idx + 1) % 2 == 0 or batch_idx == 0:\n",
    "                        recent_loss = np.mean(epoch_losses[-2:]) if len(epoch_losses) >= 2 else epoch_losses[-1]\n",
    "                        recent_log_post = np.mean(epoch_log_posts[-2:]) if len(epoch_log_posts) >= 2 else epoch_log_posts[-1]\n",
    "                        print(f\"  Batch {batch_idx + 1}/{len(training_batches)}: Loss = {recent_loss:.4f}, Log Post = {recent_log_post:.4f}\")\n",
    "                        \n",
    "                except Exception as batch_error:\n",
    "                    print(f\"Error in batch {batch_idx + 1}: {batch_error}\")\n",
    "                    # Continue with next batch\n",
    "                    continue\n",
    "            \n",
    "            if epoch_losses:  # Only proceed if we have some losses\n",
    "                # Calculate epoch averages\n",
    "                avg_epoch_loss = np.mean(epoch_losses)\n",
    "                avg_log_post = np.mean(epoch_log_posts)\n",
    "                \n",
    "                training_losses.append(avg_epoch_loss)\n",
    "                log_posterior_values.append(avg_log_post)\n",
    "                \n",
    "                print(f\"Epoch {epoch + 1} completed:\")\n",
    "                print(f\"   Average Loss: {avg_epoch_loss:.4f}\")\n",
    "                print(f\"   Average Log Posterior: {avg_log_post:.4f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch + 1} failed - no successful batches\")\n",
    "                break\n",
    "        \n",
    "        if training_losses:\n",
    "            print(f\"\\nBayesian training with posteriors completed!\")\n",
    "            print(f\"Final loss: {training_losses[-1]:.4f}\")\n",
    "            print(f\"Final log posterior: {log_posterior_values[-1]:.4f}\")\n",
    "\n",
    "            # Plot training progress\n",
    "            try:\n",
    "                import matplotlib.pyplot as plt\n",
    "                \n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "                \n",
    "                # Plot loss\n",
    "                epochs = range(1, len(training_losses) + 1)\n",
    "                ax1.plot(epochs, training_losses, 'b-', linewidth=2, label='Loss')\n",
    "                ax1.set_title('Training Loss')\n",
    "                ax1.set_xlabel('Epoch')\n",
    "                ax1.set_ylabel('Loss')\n",
    "                ax1.grid(True, alpha=0.3)\n",
    "                ax1.legend()\n",
    "                \n",
    "                # Plot log posterior\n",
    "                ax2.plot(epochs, log_posterior_values, 'r-', linewidth=2, label='Log Posterior')\n",
    "                ax2.set_title('Log Posterior')\n",
    "                ax2.set_xlabel('Epoch')\n",
    "                ax2.set_ylabel('Log Posterior')\n",
    "                ax2.grid(True, alpha=0.3)\n",
    "                ax2.legend()\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "            except ImportError:\n",
    "                print(\"Matplotlib not available for plotting\")\n",
    "        else:\n",
    "            print(\" Training failed - no successful epochs\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error during posteriors training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"Cannot run training - posteriors VI setup failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a938a2e",
   "metadata": {},
   "source": [
    "# NEVER GOT THIS FAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c688eb21",
   "metadata": {},
   "source": [
    "## 8. Compare Deterministic vs Bayesian Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e44000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare deterministic vs Bayesian predictions using posteriors\n",
    "if vi_state is not None and model is not None:\n",
    "    print(\"Comparing Deterministic vs Bayesian Predictions (posteriors)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Evaluate on a test batch\n",
    "    test_batch = training_batches[0] if training_batches else None\n",
    "    \n",
    "    if test_batch is not None:\n",
    "        x_test, y_test = test_batch\n",
    "        \n",
    "        print(f\"Test batch shape: {x_test.shape}\")\n",
    "        \n",
    "        # 1. Deterministic prediction (original model)\n",
    "        print(\"\\nDeterministic Prediction:\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            deterministic_logits, _ = model(x_test)\n",
    "            deterministic_loss = F.cross_entropy(\n",
    "                deterministic_logits.view(-1, deterministic_logits.size(-1)),\n",
    "                y_test.view(-1)\n",
    "            )\n",
    "            print(f\"  Loss: {deterministic_loss.item():.4f}\")\n",
    "            \n",
    "            # Calculate perplexity\n",
    "            deterministic_ppl = torch.exp(deterministic_loss)\n",
    "            print(f\"  Perplexity: {deterministic_ppl.item():.4f}\")\n",
    "        \n",
    "        # 2. Bayesian prediction using posteriors (posterior mean)\n",
    "        print(\"\\nBayesian Prediction (Posterior Mean via posteriors):\")\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                bayesian_logits, _ = func.functional_call(model, vi_state.params, (x_test,))\n",
    "                bayesian_loss = F.cross_entropy(\n",
    "                    bayesian_logits.view(-1, bayesian_logits.size(-1)),\n",
    "                    y_test.view(-1)\n",
    "                )\n",
    "                print(f\"  Loss: {bayesian_loss.item():.4f}\")\n",
    "                \n",
    "                # Calculate perplexity\n",
    "                bayesian_ppl = torch.exp(bayesian_loss)\n",
    "                print(f\"  Perplexity: {bayesian_ppl.item():.4f}\")\n",
    "                \n",
    "                # Calculate improvement\n",
    "                improvement = deterministic_loss.item() - bayesian_loss.item()\n",
    "                print(f\"\\nLoss Improvement: {improvement:.4f}\")\n",
    "                \n",
    "                if improvement > 0:\n",
    "                    print(\"Bayesian model performs better!\")\n",
    "                else:\n",
    "                    print(\"Deterministic model performs better\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error in Bayesian prediction: {e}\")\n",
    "        \n",
    "        # 3. Sample multiple predictions from posterior using posteriors\n",
    "        print(\"\\nMultiple Posterior Samples (using posteriors.vi.diag.sample):\")\n",
    "        try:\n",
    "            posterior_samples = []\n",
    "            sample_losses = []\n",
    "            \n",
    "            # Generate multiple samples from the posterior using posteriors\n",
    "            for i in range(CONFIG['num_samples']):\n",
    "                # Use posteriors to sample from the variational distribution\n",
    "                sample_params = posteriors.vi.diag.sample(vi_state)\n",
    "                \n",
    "                # Evaluate sample\n",
    "                with torch.no_grad():\n",
    "                    sample_logits, _ = func.functional_call(model, sample_params, (x_test,))\n",
    "                    sample_loss = F.cross_entropy(\n",
    "                        sample_logits.view(-1, sample_logits.size(-1)),\n",
    "                        y_test.view(-1)\n",
    "                    )\n",
    "                    sample_losses.append(sample_loss.item())\n",
    "                    posterior_samples.append(sample_logits)\n",
    "            \n",
    "            # Calculate statistics\n",
    "            mean_loss = np.mean(sample_losses)\n",
    "            std_loss = np.std(sample_losses)\n",
    "            \n",
    "            print(f\"  Mean Loss: {mean_loss:.4f} ± {std_loss:.4f}\")\n",
    "            print(f\"  Min Loss: {min(sample_losses):.4f}\")\n",
    "            print(f\"  Max Loss: {max(sample_losses):.4f}\")\n",
    "            \n",
    "            # Calculate predictive uncertainty\n",
    "            if len(posterior_samples) > 1:\n",
    "                # Stack samples and calculate variance\n",
    "                stacked_logits = torch.stack(posterior_samples)  # (num_samples, batch, seq, vocab)\n",
    "                pred_probs = F.softmax(stacked_logits, dim=-1)\n",
    "                \n",
    "                # Calculate predictive entropy (uncertainty)\n",
    "                mean_probs = pred_probs.mean(dim=0)  # Average over samples\n",
    "                pred_entropy = -(mean_probs * torch.log(mean_probs + 1e-8)).sum(dim=-1)\n",
    "                \n",
    "                avg_uncertainty = pred_entropy.mean().item()\n",
    "                print(f\"  Average Predictive Uncertainty: {avg_uncertainty:.4f}\")\n",
    "                \n",
    "                # Show parameter uncertainty statistics\n",
    "                print(f\"\\nParameter Uncertainty (from posteriors):\")\n",
    "                total_params = 0\n",
    "                total_std = 0\n",
    "                \n",
    "                for name in vi_state.params:\n",
    "                    if hasattr(vi_state, 'log_scale'):\n",
    "                        # Extract standard deviation from log_scale\n",
    "                        param_std = torch.exp(vi_state.log_scale[name]).mean().item()\n",
    "                        param_count = vi_state.params[name].numel()\n",
    "                        \n",
    "                        total_params += param_count\n",
    "                        total_std += param_std * param_count\n",
    "                        \n",
    "                        print(f\"  {name}: std = {param_std:.6f}\")\n",
    "                \n",
    "                if total_params > 0:\n",
    "                    avg_param_std = total_std / total_params\n",
    "                    print(f\"  Average parameter std: {avg_param_std:.6f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in posterior sampling with posteriors: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "    else:\n",
    "        print(\"No test data available\")\n",
    "        \n",
    "else:\n",
    "    print(\"Cannot compare predictions - models not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389aad66",
   "metadata": {},
   "source": [
    "## 9. Text Generation with Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945b28e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with uncertainty quantification using posteriors\n",
    "if vi_state is not None and model is not None:\n",
    "    print(\"Generating Text with Bayesian Uncertainty (posteriors)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Define generation prompts\n",
    "    prompts = [\n",
    "        \"To be or not to be,\",\n",
    "        \"HAMLET:\",\n",
    "        \"Fair is foul and foul is\",\n",
    "        \"Tomorrow, and tomorrow,\"\n",
    "    ]\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nPrompt: '{prompt}'\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            # Encode prompt\n",
    "            prompt_tokens = encode(prompt, stoi)\n",
    "            x = torch.tensor(prompt_tokens, dtype=torch.long, device=device)[None, ...]\n",
    "            \n",
    "            # 1. Deterministic generation (original model)\n",
    "            print(\"Deterministic Generation:\")\n",
    "            with torch.no_grad():\n",
    "                deterministic_x = x.clone()\n",
    "                for _ in range(CONFIG['max_new_tokens']):\n",
    "                    # Crop if sequence gets too long\n",
    "                    x_cond = deterministic_x if deterministic_x.size(1) <= model.config.block_size else deterministic_x[:, -model.config.block_size:]\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    logits, _ = model(x_cond)\n",
    "                    logits = logits[:, -1, :] / CONFIG['generation_temperature']\n",
    "                    \n",
    "                    # Sample next token\n",
    "                    probs = F.softmax(logits, dim=-1)\n",
    "                    next_token = torch.multinomial(probs, num_samples=1)\n",
    "                    deterministic_x = torch.cat((deterministic_x, next_token), dim=1)\n",
    "                \n",
    "                # Decode generated text\n",
    "                deterministic_tokens = deterministic_x[0].tolist()\n",
    "                deterministic_text = decode(deterministic_tokens, itos)\n",
    "                generated_part = deterministic_text[len(prompt):].strip()[:200]  # Limit length\n",
    "                print(f\"  {generated_part}\")\n",
    "            \n",
    "            # 2. Bayesian generation with multiple samples using posteriors\n",
    "            print(f\"\\nBayesian Generation ({CONFIG['num_samples']} samples via posteriors):\")\n",
    "            bayesian_generations = []\n",
    "            \n",
    "            for sample_idx in range(CONFIG['num_samples']):\n",
    "                # Sample parameters from posterior using posteriors\n",
    "                try:\n",
    "                    sampled_params = posteriors.vi.diag.sample(vi_state)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error sampling from posteriors, using mean: {e}\")\n",
    "                    # Fallback to using mean parameters\n",
    "                    sampled_params = vi_state.params\n",
    "                \n",
    "                # Generate with sampled parameters\n",
    "                with torch.no_grad():\n",
    "                    bayesian_x = x.clone()\n",
    "                    for _ in range(CONFIG['max_new_tokens']):\n",
    "                        # Crop if sequence gets too long\n",
    "                        x_cond = bayesian_x if bayesian_x.size(1) <= model.config.block_size else bayesian_x[:, -model.config.block_size:]\n",
    "                        \n",
    "                        # Forward pass with sampled parameters\n",
    "                        logits, _ = func.functional_call(model, sampled_params, (x_cond,))\n",
    "                        logits = logits[:, -1, :] / CONFIG['generation_temperature']\n",
    "                        \n",
    "                        # Sample next token\n",
    "                        probs = F.softmax(logits, dim=-1)\n",
    "                        next_token = torch.multinomial(probs, num_samples=1)\n",
    "                        bayesian_x = torch.cat((bayesian_x, next_token), dim=1)\n",
    "                    \n",
    "                    # Decode generated text\n",
    "                    bayesian_tokens = bayesian_x[0].tolist()\n",
    "                    bayesian_text = decode(bayesian_tokens, itos)\n",
    "                    generated_part = bayesian_text[len(prompt):].strip()[:200]  # Limit length\n",
    "                    bayesian_generations.append(generated_part)\n",
    "                    print(f\"  Sample {sample_idx + 1}: {generated_part}\")\n",
    "            \n",
    "            # 3. Analyze diversity using posteriors samples\n",
    "            print(f\"\\nAnalysis (posteriors-based):\")\n",
    "            if len(bayesian_generations) > 1:\n",
    "                # Calculate diversity metrics\n",
    "                unique_starts = set()\n",
    "                for gen in bayesian_generations:\n",
    "                    words = gen.split()[:5]  # First 5 words\n",
    "                    if words:\n",
    "                        unique_starts.add(' '.join(words))\n",
    "                \n",
    "                diversity = len(unique_starts) / len(bayesian_generations)\n",
    "                print(f\"  Diversity (unique 5-word starts): {diversity:.2f}\")\n",
    "                \n",
    "                # Character-level diversity\n",
    "                avg_length = np.mean([len(gen) for gen in bayesian_generations])\n",
    "                std_length = np.std([len(gen) for gen in bayesian_generations])\n",
    "                print(f\"  Average length: {avg_length:.1f} ± {std_length:.1f} chars\")\n",
    "                \n",
    "                # Calculate generation uncertainty\n",
    "                if len(set(bayesian_generations)) > 1:\n",
    "                    unique_ratio = len(set(bayesian_generations)) / len(bayesian_generations)\n",
    "                    print(f\"  Uniqueness ratio: {unique_ratio:.2f}\")\n",
    "                else:\n",
    "                    print(f\"  All generations identical (low uncertainty)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating text for prompt '{prompt}': {e}\")\n",
    "            continue\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot generate text - Bayesian model not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc3f14",
   "metadata": {},
   "source": [
    "## 10. Uncertainty Quantification Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcd7247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze uncertainty in predictions using posteriors\n",
    "if vi_state is not None and model is not None:\n",
    "    print(\"Uncertainty Quantification Analysis (posteriors)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test sequence for uncertainty analysis\n",
    "    test_sequence = \"To be or not to be, that is the\"\n",
    "    test_tokens = encode(test_sequence, stoi)\n",
    "    x_test = torch.tensor(test_tokens, dtype=torch.long, device=device)[None, ...]\n",
    "    \n",
    "    print(f\"Test sequence: '{test_sequence}'\")\n",
    "    print(f\"Analyzing next token predictions using posteriors...\")\n",
    "    \n",
    "    # Collect predictions from multiple posterior samples\n",
    "    next_token_logits = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample_idx in range(20):  # More samples for better uncertainty estimation\n",
    "            try:\n",
    "                # Sample parameters from posterior using posteriors\n",
    "                sampled_params = posteriors.vi.diag.sample(vi_state)\n",
    "                \n",
    "                # Forward pass with sampled parameters\n",
    "                logits, _ = func.functional_call(model, sampled_params, (x_test,))\n",
    "                next_token_logit = logits[:, -1, :]  # Last token predictions\n",
    "                next_token_logits.append(next_token_logit.cpu())\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in sample {sample_idx}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if next_token_logits:\n",
    "        # Stack predictions\n",
    "        logits_stack = torch.stack(next_token_logits, dim=0)  # [n_samples, 1, vocab_size]\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        probs_stack = F.softmax(logits_stack, dim=-1)  # [n_samples, 1, vocab_size]\n",
    "        \n",
    "        # Calculate statistics\n",
    "        mean_probs = probs_stack.mean(dim=0)[0]  # [vocab_size]\n",
    "        std_probs = probs_stack.std(dim=0)[0]    # [vocab_size]\n",
    "        \n",
    "        # Find top predictions with uncertainty\n",
    "        top_k = 10\n",
    "        top_indices = mean_probs.argsort(descending=True)[:top_k]\n",
    "        \n",
    "        print(f\"\\nTop {top_k} next token predictions with uncertainty (posteriors):\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"{'Rank':<4} {'Token':<15} {'Mean Prob':<10} {'Std Prob':<10} {'CV':<8}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for rank, idx in enumerate(top_indices):\n",
    "            token_idx = idx.item()\n",
    "            if token_idx < len(itos):\n",
    "                token = itos[token_idx]\n",
    "                mean_p = mean_probs[token_idx].item()\n",
    "                std_p = std_probs[token_idx].item()\n",
    "                cv = std_p / mean_p if mean_p > 0 else float('inf')  # Coefficient of variation\n",
    "                \n",
    "                print(f\"{rank+1:<4} '{token}':<15} {mean_p:.6f} {std_p:.6f} {cv:.3f}\")\n",
    "        \n",
    "        # Overall uncertainty metrics\n",
    "        entropy = -(mean_probs * torch.log(mean_probs + 1e-10)).sum()\n",
    "        pred_uncertainty = std_probs.mean()\n",
    "        max_prob = mean_probs.max()\n",
    "        \n",
    "        print(f\"\\nUncertainty Metrics (posteriors-based):\")\n",
    "        print(f\"  Predictive Entropy: {entropy:.4f}\")\n",
    "        print(f\"  Average Std: {pred_uncertainty:.6f}\")\n",
    "        print(f\"  Max Probability: {max_prob:.6f}\")\n",
    "        print(f\"  Confidence: {1 - entropy/torch.log(torch.tensor(len(itos), dtype=torch.float)):.4f}\")\n",
    "        \n",
    "        # Extract parameter uncertainty from posteriors VI state\n",
    "        print(f\"\\nParameter Uncertainty (from posteriors VI):\")\n",
    "        try:\n",
    "            if hasattr(vi_state, 'log_scale'):\n",
    "                param_uncertainties = {}\n",
    "                for name, log_scale in vi_state.log_scale.items():\n",
    "                    param_std = torch.exp(log_scale).mean().item()\n",
    "                    param_uncertainties[name] = param_std\n",
    "                \n",
    "                # Show top 5 most uncertain parameters\n",
    "                sorted_params = sorted(param_uncertainties.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "                for name, std in sorted_params:\n",
    "                    print(f\"  {name}: {std:.6f}\")\n",
    "                    \n",
    "                avg_param_uncertainty = np.mean(list(param_uncertainties.values()))\n",
    "                print(f\"  Average parameter uncertainty: {avg_param_uncertainty:.6f}\")\n",
    "            else:\n",
    "                print(\"  Parameter uncertainties not directly accessible\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error extracting parameter uncertainties: {e}\")\n",
    "        \n",
    "        # Visualize uncertainty if matplotlib is available\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            \n",
    "            # Plot top predictions with uncertainty bars\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            \n",
    "            # Top predictions with error bars\n",
    "            top_5_indices = top_indices[:5]\n",
    "            top_5_tokens = [itos[idx.item()] if idx.item() < len(itos) else f\"[{idx.item()}]\" \n",
    "                           for idx in top_5_indices]\n",
    "            top_5_means = [mean_probs[idx].item() for idx in top_5_indices]\n",
    "            top_5_stds = [std_probs[idx].item() for idx in top_5_indices]\n",
    "            \n",
    "            ax1.bar(range(5), top_5_means, yerr=top_5_stds, capsize=5, alpha=0.7)\n",
    "            ax1.set_xlabel('Token Rank')\n",
    "            ax1.set_ylabel('Probability')\n",
    "            ax1.set_title('Top 5 Predictions with Uncertainty (posteriors)')\n",
    "            ax1.set_xticks(range(5))\n",
    "            ax1.set_xticklabels([f\"'{token}'\" for token in top_5_tokens], rotation=45)\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Uncertainty distribution\n",
    "            uncertainty_values = std_probs[mean_probs > 0.001]  # Only consider tokens with some probability\n",
    "            ax2.hist(uncertainty_values.numpy(), bins=30, alpha=0.7, edgecolor='black')\n",
    "            ax2.set_xlabel('Standard Deviation')\n",
    "            ax2.set_ylabel('Frequency')\n",
    "            ax2.set_title('Distribution of Prediction Uncertainties (posteriors)')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"Matplotlib not available for visualization\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No valid predictions collected for uncertainty analysis\")\n",
    "        \n",
    "else:\n",
    "    print(\"Cannot analyze uncertainty - Bayesian model not available\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
