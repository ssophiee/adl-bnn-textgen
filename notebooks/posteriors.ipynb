{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c71f773c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\bnn\\Lib\\site-packages\\posteriors\\utils.py:9: FutureWarning: The 'optree.integration' module is deprecated and will be removed in version 0.18.0. Please use 'optree.integrations' instead.\n",
      "  from optree.integration.torch import tree_ravel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Bayesian NanoGPT with Posteriors Library\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import posteriors\n",
    "import torchopt\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, Tuple, List\n",
    "import sys\n",
    "\n",
    "from utils import encode, decode\n",
    "\n",
    "\n",
    "# Set up paths and import config\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "import config\n",
    "from utils import load_model, load_tokenizer, load_shakespeare_dataset, generate_text\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(509)\n",
    "np.random.seed(509)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cee1ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration and paths from config.py...\n",
      "Base directory: c:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\n",
      "Model path: c:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\checkpoints\\baseline_nanogpt\\baseline_nanogpt.pt\n",
      "Meta path: c:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\checkpoints\\baseline_nanogpt\\nanogpt_meta.pkl\n",
      "Dataset path: c:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\baselines\\nanogpt\\dataset.txt\n",
      "Loading model from: c:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\checkpoints\\baseline_nanogpt\\baseline_nanogpt.pt\n",
      "Model arguments: {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'block_size': 256, 'bias': False, 'vocab_size': 65, 'dropout': 0.2}\n",
      "number of parameters: 10.65M\n",
      "Model loaded successfully!\n",
      "Number of parameters: 10,745,088\n",
      "Vocabulary size: 65\n",
      "Successfully loaded Shakespeare dataset: 1,115,394 characters\n",
      "Found 27660 meaningful lines in the dataset\n",
      "Created 20 test samples from the dataset\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Training iterations: 1000\n",
      "Best validation loss: 1.5196\n",
      "number of parameters: 10.65M\n",
      "Model loaded successfully!\n",
      "Number of parameters: 10,745,088\n",
      "Vocabulary size: 65\n",
      "Successfully loaded Shakespeare dataset: 1,115,394 characters\n",
      "Found 27660 meaningful lines in the dataset\n",
      "Created 20 test samples from the dataset\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Training iterations: 1000\n",
      "Best validation loss: 1.5196\n"
     ]
    }
   ],
   "source": [
    "# Load model, tokenizer, and dataset using config paths\n",
    "print(\"Loading configuration and paths from config.py...\")\n",
    "print(f\"Base directory: {config.BASE_DIR}\")\n",
    "print(f\"Model path: {config.MODEL_PATH}\")\n",
    "print(f\"Meta path: {config.META_PATH}\")\n",
    "print(f\"Dataset path: {config.DATASET_PATH}\")\n",
    "\n",
    "model, checkpoint = load_model(config.MODEL_PATH, device=device)\n",
    "model.eval()\n",
    "\n",
    "stoi, itos = load_tokenizer(config.META_PATH)\n",
    "vocab_size = len(itos)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "full_text, prompts, references = load_shakespeare_dataset(config.DATASET_PATH)\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "\n",
    "if 'iter_num' in checkpoint:\n",
    "    print(f\"Training iterations: {checkpoint['iter_num']}\")\n",
    "if 'best_val_loss' in checkpoint:\n",
    "    print(f\"Best validation loss: {checkpoint['best_val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265a8de9",
   "metadata": {},
   "source": [
    "# Bayesian Neural Network Text Generation with Posteriors\n",
    "\n",
    "This notebook demonstrates how to convert a pre-trained NanoGPT model into a Bayesian neural network using the **posteriors** library. We'll explore different posterior approximation methods and analyze uncertainty in text generation.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Model**: Pre-trained character-level NanoGPT on Shakespeare text\n",
    "- **Methods**: Laplace approximation, Variational Inference, SGMCMC\n",
    "- **Goal**: Quantify uncertainty in text generation and compare different Bayesian approaches\n",
    "\n",
    "Let's start by setting up the environment and loading our pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152666c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test text generation with the deterministic model\n",
    "print(\"=\"*60)\n",
    "print(\"DETERMINISTIC TEXT GENERATION (BASELINE)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_prompts = [\n",
    "    \"To be, or not to be\",\n",
    "    \"All the world's a stage\",\n",
    "    \"What light through yonder window breaks?\"\n",
    "]\n",
    "\n",
    "print(\"Generating text with deterministic model...\")\n",
    "deterministic_outputs = []\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\nPrompt {i+1}: '{prompt}'\")\n",
    "    generated = generate_text(\n",
    "        model, \n",
    "        prompt, \n",
    "        stoi, \n",
    "        itos, \n",
    "        max_new_tokens=30,\n",
    "        temperature=0.9,\n",
    "        top_k=40,\n",
    "        device=device\n",
    "    )\n",
    "    deterministic_outputs.append(generated)\n",
    "    print(f\"Generated: '{generated[len(prompt):].strip()}'\")\n",
    "\n",
    "print(f\"\\nCompleted deterministic text generation for {len(test_prompts)} prompts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d5953",
   "metadata": {},
   "source": [
    "# Laplace Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d91cf7dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     34\u001b[39m laplace_transform = posteriors.laplace.diag_fisher.build(\n\u001b[32m     35\u001b[39m     log_posterior, \n\u001b[32m     36\u001b[39m     per_sample=\u001b[32m20\u001b[39m  \u001b[38;5;66;03m# Number of samples for Fisher information estimation\u001b[39;00m\n\u001b[32m     37\u001b[39m )\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Initialize Laplace state\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m laplace_state = laplace_transform.init(\u001b[43mmodel_params\u001b[49m)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Create training batch from Shakespeare text\u001b[39;00m\n\u001b[32m     43\u001b[39m train_batch = create_shakespeare_batch(full_text[:\u001b[32m10000\u001b[39m], batch_size=\u001b[32m16\u001b[39m, block_size=\u001b[32m64\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model_params' is not defined"
     ]
    }
   ],
   "source": [
    "# Create sample batch for Laplace approximation\n",
    "def create_shakespeare_batch(text_sample, batch_size=20, block_size=32):\n",
    "    \"\"\"Create a batch from Shakespeare text for training\"\"\"\n",
    "    data = torch.tensor([stoi[c] for c in text_sample], dtype=torch.long)\n",
    "    \n",
    "    # Create random starting positions\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "# Define log posterior for the language model\n",
    "def log_posterior(params, batch):\n",
    "    x, y = batch\n",
    "    \n",
    "    # Forward pass through model with functional call\n",
    "    logits = torch.func.functional_call(model, params, (x,))\n",
    "    \n",
    "    # Calculate negative log likelihood\n",
    "    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "    \n",
    "    # Add prior (L2 regularization)\n",
    "    prior_precision = 1.0\n",
    "    log_prior = sum(posteriors.diag_normal_log_prob({name: param}, loc=0.0, scale=1.0/prior_precision) \n",
    "                   for name, param in params.items())\n",
    "    \n",
    "    # Return log posterior (negative loss + log prior)\n",
    "    log_post_val = -loss + log_prior / len(full_text)\n",
    "    \n",
    "    return log_post_val, logits\n",
    "\n",
    "# Create Laplace approximation transform\n",
    "laplace_transform = posteriors.laplace.diag_fisher.build(\n",
    "    log_posterior, \n",
    "    per_sample=20  # Number of samples for Fisher information estimation\n",
    ")\n",
    "\n",
    "# Initialize Laplace state\n",
    "laplace_state = laplace_transform.init(model_params)\n",
    "\n",
    "# Create training batch from Shakespeare text\n",
    "train_batch = create_shakespeare_batch(full_text[:10000], batch_size=16, block_size=64)\n",
    "\n",
    "# Update Laplace approximation\n",
    "laplace_state, aux = laplace_transform.update(laplace_state, train_batch)\n",
    "\n",
    "print(\"Laplace approximation initialized and updated successfully!\")\n",
    "print(f\"Batch shape: {train_batch[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77f74f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 39 tensors\n",
      "Test batch: torch.Size([2, 16]) -> torch.Size([2, 16])\n",
      "Log posterior test: -1000.0000\n",
      "Running Laplace update...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "level.has_value() && level <= current_level INTERNAL ASSERT FAILED at \"C:\\\\actions-runner\\\\_work\\\\pytorch\\\\pytorch\\\\pytorch\\\\aten\\\\src\\\\ATen\\\\functorch\\\\ADInterpreters.cpp\":46, please report a bug to PyTorch. escaped?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mworking_log_posterior\u001b[39m\u001b[34m(params, batch)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Restore immediately\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\bnn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\bnn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\baselines\\nanogpt\\model.py:177\u001b[39m, in \u001b[36mGPT.forward\u001b[39m\u001b[34m(self, idx, targets)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# forward the GPT model itself\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m tok_emb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwte\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# token embeddings of shape (b, t, n_embd)\u001b[39;00m\n\u001b[32m    178\u001b[39m pos_emb = \u001b[38;5;28mself\u001b[39m.transformer.wpe(pos) \u001b[38;5;66;03m# position embeddings of shape (t, n_embd)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\bnn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\bnn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\bnn\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:192\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\bnn\\Lib\\site-packages\\torch\\nn\\functional.py:2546\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2546\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: level.has_value() && level <= current_level INTERNAL ASSERT FAILED at \"C:\\\\actions-runner\\\\_work\\\\pytorch\\\\pytorch\\\\pytorch\\\\aten\\\\src\\\\ATen\\\\functorch\\\\ADInterpreters.cpp\":46, please report a bug to PyTorch. escaped?",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# Run update\u001b[39;00m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning Laplace update...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     laplace_state, aux = \u001b[43mlaplace_transform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlaplace_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Laplace approximation successful!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\bnn\\Lib\\site-packages\\posteriors\\laplace\\diag_fisher.py:125\u001b[39m, in \u001b[36mupdate\u001b[39m\u001b[34m(state, batch, log_posterior, per_sample, inplace)\u001b[39m\n\u001b[32m    122\u001b[39m     log_posterior = per_samplify(log_posterior)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad(), CatchAuxError():\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     jac, aux = \u001b[43mjacrev\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_posterior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     batch_diag_score_sq = tree_map(\u001b[38;5;28;01mlambda\u001b[39;00m j: j.square().sum(\u001b[32m0\u001b[39m), jac)\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate_func\u001b[39m(x, y):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\bnn\\Lib\\site-packages\\torch\\_functorch\\eager_transforms.py:570\u001b[39m, in \u001b[36mjacrev.<locals>.wrapper_fn\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    567\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper_fn\u001b[39m(*args):\n\u001b[32m    569\u001b[39m     error_if_complex(\u001b[33m\"\u001b[39m\u001b[33mjacrev\u001b[39m\u001b[33m\"\u001b[39m, args, is_input=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m     vjp_out = \u001b[43m_vjp_with_argnums\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m=\u001b[49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    571\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[32m    572\u001b[39m         output, vjp_fn, aux = vjp_out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\bnn\\Lib\\site-packages\\torch\\_functorch\\vmap.py:48\u001b[39m, in \u001b[36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(f)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfn\u001b[39m(*args, **kwargs):\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.graph.disable_saved_tensors_hooks(message):\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\bnn\\Lib\\site-packages\\torch\\_functorch\\eager_transforms.py:358\u001b[39m, in \u001b[36m_vjp_with_argnums\u001b[39m\u001b[34m(func, argnums, has_aux, *primals)\u001b[39m\n\u001b[32m    356\u001b[39m     diff_primals = _slice_argnums(primals, argnums, as_tuple=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    357\u001b[39m     tree_map_(partial(_create_differentiable, level=level), diff_primals)\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m primals_out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprimals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(primals_out, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(primals_out) == \u001b[32m2\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mworking_log_posterior\u001b[39m\u001b[34m(params, batch)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model.named_parameters():\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m original:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m = original[name]\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.tensor(-\u001b[32m1000.0\u001b[39m, device=device), \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: level.has_value() && level <= current_level INTERNAL ASSERT FAILED at \"C:\\\\actions-runner\\\\_work\\\\pytorch\\\\pytorch\\\\pytorch\\\\aten\\\\src\\\\ATen\\\\functorch\\\\ADInterpreters.cpp\":46, please report a bug to PyTorch. escaped?"
     ]
    }
   ],
   "source": [
    "# WORKING LAPLACE APPROXIMATION - FIXED VERSION\n",
    "import time\n",
    "\n",
    "def create_small_batch(text_sample, batch_size=2, block_size=16):\n",
    "    \"\"\"Create a small batch for testing\"\"\"\n",
    "    # Safe encoding\n",
    "    data = []\n",
    "    for c in text_sample[:1000]:  # Use only first 1000 chars\n",
    "        if c in stoi:\n",
    "            data.append(stoi[c])\n",
    "        else:\n",
    "            data.append(stoi.get(' ', 0))\n",
    "    \n",
    "    data = torch.tensor(data, dtype=torch.long)\n",
    "    \n",
    "    if len(data) < block_size + 1:\n",
    "        return None, None\n",
    "    \n",
    "    # Create sequences\n",
    "    max_start = len(data) - block_size - 1\n",
    "    ix = torch.randint(0, max_start, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "def working_log_posterior(params, batch):\n",
    "    \"\"\"Working log posterior function\"\"\"\n",
    "    x, y = batch\n",
    "    \n",
    "    # Temporarily set parameters\n",
    "    original = {}\n",
    "    try:\n",
    "        for name, param in model.named_parameters():\n",
    "            original[name] = param.data.clone()\n",
    "            param.data = params[name]\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(x)\n",
    "        \n",
    "        # Restore immediately\n",
    "        for name, param in model.named_parameters():\n",
    "            param.data = original[name]\n",
    "        \n",
    "        # Handle tuple output\n",
    "        if isinstance(outputs, tuple):\n",
    "            logits = outputs[0]\n",
    "        else:\n",
    "            logits = outputs\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "        \n",
    "        # Simple prior\n",
    "        prior = sum(0.5 * torch.sum(p**2) * 1e-6 for p in params.values())\n",
    "        \n",
    "        return -loss + prior, logits\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Always restore\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in original:\n",
    "                param.data = original[name]\n",
    "        return torch.tensor(-1000.0, device=device), None\n",
    "\n",
    "# Get parameters\n",
    "model_params = dict(model.named_parameters())\n",
    "print(f\"Model parameters: {len(model_params)} tensors\")\n",
    "\n",
    "# Create test batch\n",
    "test_batch = create_small_batch(full_text)\n",
    "if test_batch[0] is not None:\n",
    "    print(f\"Test batch: {test_batch[0].shape} -> {test_batch[1].shape}\")\n",
    "    \n",
    "    # Test log posterior\n",
    "    log_val, logits = working_log_posterior(model_params, test_batch)\n",
    "    print(f\"Log posterior test: {log_val.item():.4f}\")\n",
    "    \n",
    "    # Create Laplace transform\n",
    "    laplace_transform = posteriors.laplace.diag_fisher.build(working_log_posterior, 1.0)\n",
    "    laplace_state = laplace_transform.init(model_params)\n",
    "    \n",
    "    # Run update\n",
    "    print(\"Running Laplace update...\")\n",
    "    laplace_state, aux = laplace_transform.update(laplace_state, test_batch)\n",
    "    print(\"✅ Laplace approximation successful!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Failed to create batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e411660e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# SIMPLE MANUAL BAYESIAN APPROACH (AVOIDING FUNCTORCH ISSUES)\n",
    "import time\n",
    "\n",
    "class SimpleBayesianNanoGPT:\n",
    "    \"\"\"Simple Bayesian wrapper for NanoGPT that works around functorch issues\"\"\"\n",
    "    \n",
    "    def __init__(self, base_model):\n",
    "        self.model = base_model\n",
    "        self.base_params = dict(base_model.named_parameters())\n",
    "        self.param_uncertainties = {}\n",
    "        \n",
    "        # Initialize uncertainties as small values\n",
    "        for name, param in self.base_params.items():\n",
    "            self.param_uncertainties[name] = torch.ones_like(param) * 0.001\n",
    "    \n",
    "    def estimate_uncertainties(self, text_sample, num_batches=5):\n",
    "        \"\"\"Estimate parameter uncertainties using simple gradient variance\"\"\"\n",
    "        print(\"Estimating parameter uncertainties...\")\n",
    "        \n",
    "        # Collect gradients from multiple batches\n",
    "        all_gradients = {name: [] for name in self.base_params.keys()}\n",
    "        \n",
    "        # Prepare small batches\n",
    "        data = [stoi.get(c, 0) for c in text_sample[:2000]]\n",
    "        data = torch.tensor(data, dtype=torch.long, device=device)\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            # Create small batch\n",
    "            if len(data) < 32:\n",
    "                continue\n",
    "                \n",
    "            start_idx = torch.randint(0, len(data) - 16, (1,)).item()\n",
    "            x = data[start_idx:start_idx+16].unsqueeze(0)\n",
    "            y = data[start_idx+1:start_idx+17].unsqueeze(0)\n",
    "            \n",
    "            # Forward pass with gradients\n",
    "            self.model.train()\n",
    "            self.model.zero_grad()\n",
    "            \n",
    "            outputs = self.model(x)\n",
    "            if isinstance(outputs, tuple):\n",
    "                logits = outputs[0]\n",
    "            else:\n",
    "                logits = outputs\n",
    "            \n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            loss.backward()\n",
    "            \n",
    "            # Collect gradients\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    all_gradients[name].append(param.grad.clone())\n",
    "            \n",
    "            print(f\"  Batch {i+1}/{num_batches} processed\")\n",
    "        \n",
    "        # Compute gradient variances as uncertainty estimates\n",
    "        for name in self.base_params.keys():\n",
    "            if len(all_gradients[name]) > 1:\n",
    "                grad_stack = torch.stack(all_gradients[name])\n",
    "                self.param_uncertainties[name] = torch.var(grad_stack, dim=0).clamp(min=1e-6)\n",
    "            else:\n",
    "                self.param_uncertainties[name] = torch.ones_like(self.base_params[name]) * 0.001\n",
    "        \n",
    "        # Clear gradients\n",
    "        self.model.zero_grad()\n",
    "        self.model.eval()\n",
    "        print(\"✅ Uncertainty estimation completed!\")\n",
    "    \n",
    "    def sample_parameters(self, scale=0.01):\n",
    "        \"\"\"Sample parameters from approximate posterior\"\"\"\n",
    "        sampled_params = {}\n",
    "        for name, base_param in self.base_params.items():\n",
    "            noise_std = torch.sqrt(self.param_uncertainties[name]) * scale\n",
    "            noise = torch.randn_like(base_param) * noise_std\n",
    "            sampled_params[name] = base_param + noise\n",
    "        return sampled_params\n",
    "    \n",
    "    def generate_with_uncertainty(self, prompt, num_samples=3, max_tokens=30):\n",
    "        \"\"\"Generate text with uncertainty quantification\"\"\"\n",
    "        print(f\"Generating {num_samples} samples for: '{prompt}'\")\n",
    "        \n",
    "        results = []\n",
    "        for i in range(num_samples):\n",
    "            # Sample parameters\n",
    "            sampled_params = self.sample_parameters(scale=0.005)  # Small noise\n",
    "            \n",
    "            # Temporarily set sampled parameters\n",
    "            original_params = {}\n",
    "            try:\n",
    "                for name, param in self.model.named_parameters():\n",
    "                    original_params[name] = param.data.clone()\n",
    "                    param.data = sampled_params[name]\n",
    "                \n",
    "                # Generate text\n",
    "                encoded = [stoi.get(c, 0) for c in prompt]\n",
    "                x = torch.tensor(encoded, dtype=torch.long, device=device)[None, ...]\n",
    "                \n",
    "                self.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for _ in range(max_tokens):\n",
    "                        outputs = self.model(x)\n",
    "                        if isinstance(outputs, tuple):\n",
    "                            logits = outputs[0]\n",
    "                        else:\n",
    "                            logits = outputs\n",
    "                        \n",
    "                        # Sample next token\n",
    "                        probs = F.softmax(logits[0, -1] / 0.8, dim=-1)\n",
    "                        next_token = torch.multinomial(probs, 1)\n",
    "                        x = torch.cat([x, next_token.unsqueeze(0)], dim=1)\n",
    "                \n",
    "                # Decode result\n",
    "                generated_tokens = x[0].tolist()\n",
    "                generated_text = ''.join([itos.get(i, '?') for i in generated_tokens])\n",
    "                results.append(generated_text)\n",
    "                \n",
    "                # Restore original parameters\n",
    "                for name, param in self.model.named_parameters():\n",
    "                    param.data = original_params[name]\n",
    "                \n",
    "                print(f\"  Sample {i+1}: '{generated_text[len(prompt):].strip()}'\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Always restore parameters\n",
    "                for name, param in self.model.named_parameters():\n",
    "                    if name in original_params:\n",
    "                        param.data = original_params[name]\n",
    "                print(f\"  Sample {i+1}: Error - {e}\")\n",
    "                results.append(f\"Error: {e}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Create Bayesian wrapper\n",
    "bayesian_nanogpt = SimpleBayesianNanoGPT(model)\n",
    "\n",
    "# Estimate uncertainties\n",
    "bayesian_nanogpt.estimate_uncertainties(full_text)\n",
    "\n",
    "# Test Bayesian generation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SIMPLE BAYESIAN TEXT GENERATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_prompts = [\"To be\", \"Romeo\", \"The king\"]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    results = bayesian_nanogpt.generate_with_uncertainty(prompt, num_samples=3, max_tokens=20)\n",
    "\n",
    "print(\"\\n✅ Simple Bayesian text generation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ace12bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
