{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c71f773c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "PyTorch version: 2.8.0+cpu\n",
      "Posteriors available: 0.1.1\n"
     ]
    }
   ],
   "source": [
    "# Bayesian NanoGPT with Posteriors Library\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import posteriors\n",
    "import torchopt\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, Tuple, List\n",
    "import sys\n",
    "\n",
    "# Set up paths and import config\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "import config\n",
    "from utils import load_model, load_tokenizer, load_shakespeare_dataset, generate_text\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Posteriors available: {posteriors.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(509)\n",
    "np.random.seed(509)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8cee1ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration and paths from config.py...\n",
      "Base directory: C:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\n",
      "Model path: C:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\checkpoints\\baseline_nanogpt\\baseline_nanogpt.pt\n",
      "Meta path: C:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\checkpoints\\baseline_nanogpt\\nanogpt_meta.pkl\n",
      "Dataset path: C:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\baselines\\nanogpt\\dataset.txt\n",
      "\n",
      "==================================================\n",
      "LOADING NANOGPT MODEL\n",
      "==================================================\n",
      "Loading model from: C:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\checkpoints\\baseline_nanogpt\\baseline_nanogpt.pt\n",
      "Model arguments: {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'block_size': 256, 'bias': False, 'vocab_size': 65, 'dropout': 0.2}\n",
      "number of parameters: 10.65M\n",
      "Model loaded successfully!\n",
      "Number of parameters: 10,745,088\n",
      "\n",
      "==================================================\n",
      "LOADING TOKENIZER\n",
      "==================================================\n",
      "Vocabulary size: 65\n",
      "\n",
      "==================================================\n",
      "LOADING SHAKESPEARE DATASET\n",
      "==================================================\n",
      "Successfully loaded Shakespeare dataset: 1,115,394 characters\n",
      "Found 27660 meaningful lines in the dataset\n",
      "Created 20 test samples from the dataset\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Total text length: 1,115,394 characters\n",
      "Number of test prompts: 20\n",
      "Number of references: 20\n",
      "\n",
      "==================================================\n",
      "MODEL INFORMATION\n",
      "==================================================\n",
      "Model architecture: GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65, 384)\n",
      "    (wpe): Embedding(256, 384)\n",
      "    (drop): Dropout(p=0.2, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-5): 6 x Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=384, out_features=1152, bias=False)\n",
      "          (c_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=False)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=False)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=384, out_features=65, bias=False)\n",
      ")\n",
      "Number of parameters: 10,745,088\n",
      "Block size: 256\n",
      "Number of layers: 6\n",
      "Number of heads: 6\n",
      "Embedding dimension: 384\n",
      "Training iterations: 1000\n",
      "Best validation loss: 1.5196\n"
     ]
    }
   ],
   "source": [
    "# Load model, tokenizer, and dataset using config paths\n",
    "print(\"Loading configuration and paths from config.py...\")\n",
    "print(f\"Base directory: {config.BASE_DIR}\")\n",
    "print(f\"Model path: {config.MODEL_PATH}\")\n",
    "print(f\"Meta path: {config.META_PATH}\")\n",
    "print(f\"Dataset path: {config.DATASET_PATH}\")\n",
    "\n",
    "# Load the pre-trained NanoGPT model\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LOADING NANOGPT MODEL\")\n",
    "print(\"=\"*50)\n",
    "model, checkpoint = load_model(config.MODEL_PATH, device=device)\n",
    "model.eval()\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LOADING TOKENIZER\")\n",
    "print(\"=\"*50)\n",
    "stoi, itos = load_tokenizer(config.META_PATH)\n",
    "vocab_size = len(itos)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Load Shakespeare dataset\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LOADING SHAKESPEARE DATASET\")\n",
    "print(\"=\"*50)\n",
    "full_text, prompts, references = load_shakespeare_dataset(config.DATASET_PATH)\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Total text length: {len(full_text):,} characters\")\n",
    "print(f\"Number of test prompts: {len(prompts)}\")\n",
    "print(f\"Number of references: {len(references)}\")\n",
    "\n",
    "# Print model information\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL INFORMATION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Model architecture: {model}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Block size: {model.config.block_size}\")\n",
    "print(f\"Number of layers: {model.config.n_layer}\")\n",
    "print(f\"Number of heads: {model.config.n_head}\")\n",
    "print(f\"Embedding dimension: {model.config.n_embd}\")\n",
    "\n",
    "if 'iter_num' in checkpoint:\n",
    "    print(f\"Training iterations: {checkpoint['iter_num']}\")\n",
    "if 'best_val_loss' in checkpoint:\n",
    "    print(f\"Best validation loss: {checkpoint['best_val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265a8de9",
   "metadata": {},
   "source": [
    "# Bayesian Neural Network Text Generation with Posteriors\n",
    "\n",
    "This notebook demonstrates how to convert a pre-trained NanoGPT model into a Bayesian neural network using the **posteriors** library. We'll explore different posterior approximation methods and analyze uncertainty in text generation.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Model**: Pre-trained character-level NanoGPT on Shakespeare text\n",
    "- **Methods**: Laplace approximation, Variational Inference, SGMCMC\n",
    "- **Goal**: Quantify uncertainty in text generation and compare different Bayesian approaches\n",
    "\n",
    "Let's start by setting up the environment and loading our pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c982dfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_PROMPT = \"To be or not to be\"\n",
    "\n",
    "start_ids = encode(START_PROMPT)\n",
    "x = torch.tensor(start_ids, dtype=torch.long, device=DEVICE)[None, ...]\n",
    "\n",
    "# Generating text samples\n",
    "with torch.no_grad():\n",
    "    y = model.generate(x, max_new_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE, top_k=TOP_K)\n",
    "    print(decode(y[0].tolist()))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "152666c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DETERMINISTIC TEXT GENERATION (BASELINE)\n",
      "============================================================\n",
      "Generating text with deterministic model...\n",
      "\n",
      "Prompt 1: 'To be, or not to be'\n",
      "Generated: 'past.\n",
      "\n",
      "NERRSAN:\n",
      "The groan:\n",
      "No'\n",
      "\n",
      "Prompt 2: 'All the world's a stage'\n",
      "Generated: 'past.\n",
      "\n",
      "NERRSAN:\n",
      "The groan:\n",
      "No'\n",
      "\n",
      "Prompt 2: 'All the world's a stage'\n",
      "Generated: ';\n",
      "And, that my gentleman have'\n",
      "\n",
      "Prompt 3: 'What light through yonder window breaks?'\n",
      "Generated: ';\n",
      "And, that my gentleman have'\n",
      "\n",
      "Prompt 3: 'What light through yonder window breaks?'\n",
      "Generated: 'When have I truep it?\n",
      "\n",
      "CLIFFO'\n",
      "\n",
      "Completed deterministic text generation for 3 prompts.\n",
      "Generated: 'When have I truep it?\n",
      "\n",
      "CLIFFO'\n",
      "\n",
      "Completed deterministic text generation for 3 prompts.\n"
     ]
    }
   ],
   "source": [
    "# Test text generation with the deterministic model\n",
    "print(\"=\"*60)\n",
    "print(\"DETERMINISTIC TEXT GENERATION (BASELINE)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_prompts = [\n",
    "    \"To be, or not to be\",\n",
    "    \"All the world's a stage\",\n",
    "    \"What light through yonder window breaks?\"\n",
    "]\n",
    "\n",
    "print(\"Generating text with deterministic model...\")\n",
    "deterministic_outputs = []\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\nPrompt {i+1}: '{prompt}'\")\n",
    "    generated = generate_text(\n",
    "        model, \n",
    "        prompt, \n",
    "        stoi, \n",
    "        itos, \n",
    "        max_new_tokens=30,\n",
    "        temperature=0.9,\n",
    "        top_k=40,\n",
    "        device=device\n",
    "    )\n",
    "    deterministic_outputs.append(generated)\n",
    "    print(f\"Generated: '{generated[len(prompt):].strip()}'\")\n",
    "\n",
    "print(f\"\\nCompleted deterministic text generation for {len(test_prompts)} prompts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9668af",
   "metadata": {},
   "source": [
    "## Setting up Bayesian Inference\n",
    "\n",
    "Now we'll convert our deterministic NanoGPT model into a Bayesian neural network using the posteriors library. We'll implement three different approaches:\n",
    "\n",
    "1. **Laplace Approximation**: Quick Gaussian approximation around the MAP estimate\n",
    "2. **Variational Inference**: Learn a parameterized posterior distribution\n",
    "3. **SGMCMC**: Sample from the true posterior using stochastic gradient MCMC\n",
    "\n",
    "Each method has different computational costs and approximation quality trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27e4879d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed log posterior function defined!\n",
      "Testing fixed log posterior function...\n",
      "  Log posterior value: -713.5050\n",
      "  Output shape: torch.Size([4, 1, 65])\n",
      "  ‚úÖ Log posterior function working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Set up the log posterior function for posteriors library\n",
    "def prepare_data_for_posteriors(dataset_text: str, tokenizer, block_size: int = 256, \n",
    "                               batch_size: int = 32, num_batches: int = 10):\n",
    "    \"\"\"\n",
    "    Prepare training data batches for posterior inference.\n",
    "    \n",
    "    Args:\n",
    "        dataset_text: Full Shakespeare text\n",
    "        tokenizer: (stoi, itos) tuple\n",
    "        block_size: Context length\n",
    "        batch_size: Batch size\n",
    "        num_batches: Number of batches to create\n",
    "        \n",
    "    Returns:\n",
    "        List of (input, target) batches\n",
    "    \"\"\"\n",
    "    stoi, itos = tokenizer\n",
    "    \n",
    "    # Encode the full text\n",
    "    encoded_text = [stoi.get(c, 0) for c in dataset_text]  # Use 0 for unknown chars\n",
    "    \n",
    "    batches = []\n",
    "    for _ in range(num_batches):\n",
    "        # Random starting positions\n",
    "        start_indices = torch.randint(0, len(encoded_text) - block_size - 1, (batch_size,))\n",
    "        \n",
    "        batch_x = torch.stack([\n",
    "            torch.tensor(encoded_text[start:start + block_size], dtype=torch.long)\n",
    "            for start in start_indices\n",
    "        ])\n",
    "        \n",
    "        batch_y = torch.stack([\n",
    "            torch.tensor(encoded_text[start + 1:start + block_size + 1], dtype=torch.long)\n",
    "            for start in start_indices\n",
    "        ])\n",
    "        \n",
    "        batches.append((batch_x.to(device), batch_y.to(device)))\n",
    "    \n",
    "    return batches\n",
    "\n",
    "# Prepare data batches\n",
    "print(\"Preparing data batches for Bayesian inference...\")\n",
    "data_batches = prepare_data_for_posteriors(\n",
    "    full_text, \n",
    "    (stoi, itos), \n",
    "    block_size=model.config.block_size,\n",
    "    batch_size=16,  # Smaller batch size for memory efficiency\n",
    "    num_batches=20\n",
    ")\n",
    "\n",
    "print(f\"Created {len(data_batches)} batches for posterior inference\")\n",
    "print(f\"Batch shape: {data_batches[0][0].shape} -> {data_batches[0][1].shape}\")\n",
    "\n",
    "# Define log posterior function for posteriors library\n",
    "def log_posterior(params, batch):\n",
    "    \"\"\"\n",
    "    Compute log posterior for a batch of data.\n",
    "    \n",
    "    Args:\n",
    "        params: Model parameters (dict)\n",
    "        batch: (input, target) batch\n",
    "        \n",
    "    Returns:\n",
    "        log_posterior_value: Scalar tensor\n",
    "        model_output: For auxiliary information\n",
    "    \"\"\"\n",
    "    inputs, targets = batch\n",
    "    \n",
    "    # Use functional API to compute model output with given parameters\n",
    "    outputs, _ = torch.func.functional_call(model, params, (inputs,))\n",
    "    \n",
    "    # Compute log likelihood (negative cross entropy)\n",
    "    log_likelihood = -F.cross_entropy(\n",
    "        outputs.view(-1, outputs.size(-1)), \n",
    "        targets.view(-1),\n",
    "        reduction='mean'\n",
    "    )\n",
    "    \n",
    "    # Add log prior (simple Gaussian prior)\n",
    "    log_prior = posteriors.diag_normal_log_prob(params, sigma=1.0) / len(data_batches)\n",
    "    \n",
    "    log_post = log_likelihood + log_prior\n",
    "    \n",
    "    return log_post, outputs\n",
    "\n",
    "print(\"Log posterior function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a11136",
   "metadata": {},
   "source": [
    "## Laplace Approximation for Bayesian Neural Networks\n",
    "\n",
    "### What is Laplace Approximation?\n",
    "\n",
    "The **Laplace approximation** is a method for approximating complex posterior distributions with a Gaussian distribution. It's particularly useful for Bayesian neural networks because:\n",
    "\n",
    "1. **Computationally Efficient**: Only requires computing the Hessian at the MAP (Maximum A Posteriori) estimate\n",
    "2. **Post-hoc Method**: Can be applied to any pre-trained neural network\n",
    "3. **Analytical Uncertainty**: Provides closed-form uncertainty estimates\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "Given a neural network with parameters Œ∏ and data D, the posterior distribution is:\n",
    "\n",
    "```\n",
    "p(Œ∏|D) ‚àù p(D|Œ∏) √ó p(Œ∏)\n",
    "```\n",
    "\n",
    "The Laplace approximation approximates this posterior as a Gaussian centered at the MAP estimate Œ∏*:\n",
    "\n",
    "```\n",
    "p(Œ∏|D) ‚âà N(Œ∏*, Œ£)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **Œ∏*** = argmax p(Œ∏|D) (the MAP estimate, i.e., our pre-trained weights)\n",
    "- **Œ£** = H‚Åª¬π (inverse Hessian of the negative log posterior at Œ∏*)\n",
    "\n",
    "### Hessian Approximations\n",
    "\n",
    "Computing the full Hessian is expensive, so we use approximations:\n",
    "\n",
    "1. **Fisher Information Matrix**: Uses first-order gradients only\n",
    "   - `H ‚âà E[‚àá log p(D|Œ∏) ‚àá log p(D|Œ∏)·µÄ]`\n",
    "   - More stable and faster to compute\n",
    "\n",
    "2. **Diagonal Approximation**: Assumes parameter independence\n",
    "   - Only computes diagonal elements of the Hessian\n",
    "   - Much more memory efficient\n",
    "\n",
    "### Implementation with Posteriors\n",
    "\n",
    "The `posteriors` library provides several Laplace variants:\n",
    "- `posteriors.laplace.diag_fisher`: Diagonal Fisher information matrix\n",
    "- `posteriors.laplace.dense_fisher`: Full Fisher information matrix  \n",
    "- `posteriors.laplace.diag_ggn`: Diagonal Gauss-Newton approximation\n",
    "\n",
    "We'll use the **diagonal Fisher** approach for computational efficiency while still capturing parameter uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a858a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LAPLACE APPROXIMATION - DIAGONAL FISHER\n",
      "============================================================\n",
      "Number of parameter tensors: 39\n",
      "Total parameters: 10,745,088\n",
      "Available Laplace methods:\n",
      "  - dense_fisher\n",
      "  - dense_ggn\n",
      "  - dense_hessian\n",
      "  - diag_fisher\n",
      "  - diag_ggn\n",
      "\n",
      "Initializing Laplace approximation...\n",
      "Using posteriors.laplace.diag_fisher directly\n",
      "‚úÖ Laplace transform initialized!\n",
      "Testing parameter initialization...\n",
      "‚úÖ State initialized with init method\n"
     ]
    }
   ],
   "source": [
    "# Implement Laplace Approximation using Diagonal Fisher Information\n",
    "print(\"=\"*60)\n",
    "print(\"LAPLACE APPROXIMATION - DIAGONAL FISHER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get model parameters as a dictionary\n",
    "model_params = dict(model.named_parameters())\n",
    "print(f\"Number of parameter tensors: {len(model_params)}\")\n",
    "total_params = sum(p.numel() for p in model_params.values())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "# Initialize Laplace transform with diagonal Fisher information\n",
    "print(\"\\nInitializing Laplace approximation...\")\n",
    "laplace_transform = posteriors.laplace.diag_fisher.build(\n",
    "    log_posterior,\n",
    "    len(data_batches)  # Number of data points for proper scaling\n",
    ")\n",
    "\n",
    "# Initialize the Laplace state\n",
    "print(\"Initializing Laplace state...\")\n",
    "laplace_state = laplace_transform.init(model_params)\n",
    "\n",
    "print(f\"Laplace state initialized with keys: {list(laplace_state._fields)}\")\n",
    "print(f\"State params shape info:\")\n",
    "for name, param in laplace_state.params.items():\n",
    "    if hasattr(param, 'shape'):\n",
    "        print(f\"  {name}: {param.shape}\")\n",
    "\n",
    "# Fit the Laplace approximation by computing Fisher information\n",
    "print(\"\\nFitting Laplace approximation (computing Fisher information)...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "fit_start_time = torch.cuda.Event(enable_timing=True) if device.type == 'cuda' else None\n",
    "fit_end_time = torch.cuda.Event(enable_timing=True) if device.type == 'cuda' else None\n",
    "\n",
    "if fit_start_time:\n",
    "    fit_start_time.record()\n",
    "\n",
    "# Process batches to compute Fisher information\n",
    "for i, batch in enumerate(data_batches):\n",
    "    print(f\"Processing batch {i+1}/{len(data_batches)}\", end='\\r')\n",
    "    laplace_state, aux = laplace_transform.update(laplace_state, batch)\n",
    "\n",
    "if fit_end_time:\n",
    "    fit_end_time.record()\n",
    "    torch.cuda.synchronize()\n",
    "    fit_time = fit_start_time.elapsed_time(fit_end_time) / 1000.0  # Convert to seconds\n",
    "    print(f\"\\nLaplace fitting completed in {fit_time:.2f} seconds\")\n",
    "else:\n",
    "    print(f\"\\nLaplace fitting completed!\")\n",
    "\n",
    "# Examine the fitted Laplace approximation\n",
    "print(f\"\\nFinal Laplace state:\")\n",
    "print(f\"  Fitted parameters available: {hasattr(laplace_state, 'params')}\")\n",
    "print(f\"  Fisher information available: {hasattr(laplace_state, 'fisher')}\")\n",
    "\n",
    "if hasattr(laplace_state, 'fisher'):\n",
    "    print(f\"Fisher information computed for {len(laplace_state.fisher)} parameter groups\")\n",
    "    \n",
    "    # Print some statistics about the Fisher information\n",
    "    fisher_norms = {}\n",
    "    for name, fisher_diag in laplace_state.fisher.items():\n",
    "        if hasattr(fisher_diag, 'norm'):\n",
    "            fisher_norms[name] = fisher_diag.norm().item()\n",
    "        \n",
    "    if fisher_norms:\n",
    "        print(f\"Fisher diagonal norms:\")\n",
    "        for name, norm in list(fisher_norms.items())[:5]:  # Show first 5\n",
    "            print(f\"  {name}: {norm:.6f}\")\n",
    "        if len(fisher_norms) > 5:\n",
    "            print(f\"  ... and {len(fisher_norms) - 5} more layers\")\n",
    "\n",
    "print(\"\\nLaplace approximation fitted successfully! üéâ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21ab2d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Laplace approximation with correct API...\n",
      "Fitting Laplace approximation...\n",
      "Processing batch with 4 samples...\n",
      "‚ùå Error with posteriors API: 'NoneType' object has no attribute 'params'\n",
      "\n",
      "==================================================\n",
      "FALLBACK: Manual Diagonal Fisher Implementation\n",
      "==================================================\n",
      "Computing diagonal Fisher information manually...\n",
      "‚úÖ Manual Laplace approximation created!\n",
      "  Parameters: 39 tensors\n",
      "  Fisher diagonal: 39 tensors\n",
      "  Fisher statistics (first 3 layers):\n",
      "    transformer.wte.weight: mean=0.000027, max=0.000501\n",
      "    transformer.wpe.weight: mean=0.000013, max=0.000798\n",
      "    transformer.h.0.ln_1.weight: mean=0.008387, max=0.010461\n",
      "\n",
      "‚úÖ Laplace approximation setup complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hayk_\\AppData\\Local\\Temp\\ipykernel_23412\\2215269773.py\", line 27, in <module>\n",
      "    laplace_state = laplace_transform.update(laplace_state, batch_log_posterior, test_batch)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\bnn\\Lib\\site-packages\\posteriors\\laplace\\diag_fisher.py\", line 125, in update\n",
      "    jac, aux = jacrev(log_posterior, has_aux=True)(state.params, batch)\n",
      "                                                   ^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'params'\n"
     ]
    }
   ],
   "source": [
    "# Function to sample from the Laplace posterior\n",
    "def sample_laplace_weights(laplace_state, num_samples=5):\n",
    "    \"\"\"\n",
    "    Sample parameter sets from the Laplace posterior.\n",
    "    \n",
    "    Args:\n",
    "        laplace_state: Fitted Laplace state\n",
    "        num_samples: Number of parameter samples to draw\n",
    "        \n",
    "    Returns:\n",
    "        List of parameter dictionaries sampled from posterior\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    print(f\"Sampling {num_samples} parameter sets from Laplace posterior...\")\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Sample from the Gaussian posterior\n",
    "        sampled_params = {}\n",
    "        \n",
    "        for name, mean_param in laplace_state.params.items():\n",
    "            if name in laplace_state.fisher:\n",
    "                # Get diagonal Fisher information (precision)\n",
    "                fisher_diag = laplace_state.fisher[name]\n",
    "                \n",
    "                # Convert Fisher information to standard deviation\n",
    "                # Fisher is precision (inverse variance), so std = 1/sqrt(Fisher)\n",
    "                std = 1.0 / torch.sqrt(fisher_diag + 1e-8)  # Add small epsilon for numerical stability\n",
    "                \n",
    "                # Sample from Gaussian: N(mean, std¬≤)\n",
    "                noise = torch.randn_like(mean_param) * std\n",
    "                sampled_param = mean_param + noise\n",
    "            else:\n",
    "                # If no Fisher information, just use the MAP estimate\n",
    "                sampled_param = mean_param.clone()\n",
    "            \n",
    "            sampled_params[name] = sampled_param\n",
    "        \n",
    "        samples.append(sampled_params)\n",
    "        print(f\"  Sample {i+1}/{num_samples} generated\")\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Sample from the Laplace posterior\n",
    "print(\"=\"*60)\n",
    "print(\"SAMPLING FROM LAPLACE POSTERIOR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "num_posterior_samples = 8\n",
    "laplace_samples = sample_laplace_weights(laplace_state, num_posterior_samples)\n",
    "\n",
    "print(f\"\\nSuccessfully generated {len(laplace_samples)} parameter samples!\")\n",
    "\n",
    "# Verify sample structure\n",
    "sample_keys = list(laplace_samples[0].keys())\n",
    "print(f\"Each sample contains {len(sample_keys)} parameter tensors\")\n",
    "print(f\"Sample parameter names: {sample_keys[:5]}{'...' if len(sample_keys) > 5 else ''}\")\n",
    "\n",
    "# Check parameter differences between samples\n",
    "print(f\"\\nParameter variation analysis:\")\n",
    "if len(laplace_samples) > 1:\n",
    "    total_variation = 0\n",
    "    param_count = 0\n",
    "    \n",
    "    for name in sample_keys[:3]:  # Check first 3 layers\n",
    "        diff = (laplace_samples[1][name] - laplace_samples[0][name]).norm().item()\n",
    "        param_size = laplace_samples[0][name].numel()\n",
    "        relative_diff = diff / torch.norm(laplace_samples[0][name]).item()\n",
    "        \n",
    "        print(f\"  {name}: ||ŒîŒ∏|| = {diff:.6f}, relative = {relative_diff:.6f}\")\n",
    "        total_variation += diff\n",
    "        param_count += param_size\n",
    "    \n",
    "    print(f\"  Average parameter variation: {total_variation/len(sample_keys[:3]):.6f}\")\n",
    "\n",
    "print(\"\\nLaplace posterior sampling complete! üé≤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462bb394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Bayesian Text Generation with Laplace Samples\n",
    "print(\"=\"*60)\n",
    "print(\"BAYESIAN TEXT GENERATION EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def generate_text_with_sampled_params(model, params_dict, prompt, stoi, itos, \n",
    "                                    max_new_tokens=30, temperature=0.8, top_k=40, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate text using a specific parameter sample.\n",
    "    \n",
    "    Args:\n",
    "        model: Base model architecture\n",
    "        params_dict: Sampled parameters\n",
    "        prompt: Input text prompt\n",
    "        stoi, itos: Tokenizer\n",
    "        max_new_tokens: Length of generation\n",
    "        temperature: Sampling temperature\n",
    "        top_k: Top-k sampling\n",
    "        device: Device for computation\n",
    "        \n",
    "    Returns:\n",
    "        Generated text string\n",
    "    \"\"\"\n",
    "    # Encode prompt\n",
    "    encoded_prompt = [stoi.get(c, 0) for c in prompt]\n",
    "    x = torch.tensor(encoded_prompt, dtype=torch.long, device=device)[None, ...]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Use functional call with sampled parameters\n",
    "            logits, _ = torch.func.functional_call(model, params_dict, (x,))\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Apply top-k filtering\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            \n",
    "            # Sample next token\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            x = torch.cat((x, next_token), dim=1)\n",
    "    \n",
    "    # Decode generated sequence\n",
    "    generated_tokens = x[0].tolist()\n",
    "    return ''.join([itos.get(i, '?') for i in generated_tokens])\n",
    "\n",
    "# Test Bayesian generation with multiple samples\n",
    "test_prompt = \"To be, or not to be\"\n",
    "print(f\"Generating text with Bayesian NanoGPT...\")\n",
    "print(f\"Prompt: '{test_prompt}'\")\n",
    "print(f\"Using {len(laplace_samples)} posterior samples\\n\")\n",
    "\n",
    "bayesian_outputs = []\n",
    "generation_times = []\n",
    "\n",
    "for i, params_sample in enumerate(laplace_samples):\n",
    "    print(f\"Sample {i+1}/{len(laplace_samples)}:\")\n",
    "    \n",
    "    start_time = torch.cuda.Event(enable_timing=True) if device.type == 'cuda' else None\n",
    "    end_time = torch.cuda.Event(enable_timing=True) if device.type == 'cuda' else None\n",
    "    \n",
    "    if start_time:\n",
    "        start_time.record()\n",
    "    \n",
    "    generated = generate_text_with_sampled_params(\n",
    "        model, params_sample, test_prompt, stoi, itos,\n",
    "        max_new_tokens=25, temperature=0.8, top_k=40, device=device\n",
    "    )\n",
    "    \n",
    "    if end_time:\n",
    "        end_time.record()\n",
    "        torch.cuda.synchronize()\n",
    "        gen_time = start_time.elapsed_time(end_time)\n",
    "        generation_times.append(gen_time)\n",
    "    \n",
    "    bayesian_outputs.append(generated)\n",
    "    generated_part = generated[len(test_prompt):].strip()\n",
    "    print(f\"  Generated: '{generated_part}'\")\n",
    "\n",
    "if generation_times:\n",
    "    avg_time = sum(generation_times) / len(generation_times)\n",
    "    print(f\"\\nAverage generation time: {avg_time:.2f}ms\")\n",
    "\n",
    "print(f\"\\nBayesian text generation complete!\")\n",
    "print(f\"Generated {len(bayesian_outputs)} diverse outputs from posterior samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7646abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncertainty Quantification Analysis\n",
    "print(\"=\"*60)\n",
    "print(\"UNCERTAINTY QUANTIFICATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def compute_predictive_uncertainty(model, laplace_samples, test_inputs, max_seq_len=20):\n",
    "    \"\"\"\n",
    "    Compute predictive uncertainty for multiple test inputs.\n",
    "    \n",
    "    Args:\n",
    "        model: Base model\n",
    "        laplace_samples: List of parameter samples\n",
    "        test_inputs: List of encoded input sequences\n",
    "        max_seq_len: Maximum sequence length to analyze\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with uncertainty metrics\n",
    "    \"\"\"\n",
    "    uncertainties = []\n",
    "    \n",
    "    for input_seq in test_inputs:\n",
    "        x = torch.tensor(input_seq, dtype=torch.long, device=device)[None, :]\n",
    "        \n",
    "        # Collect predictions from all samples\n",
    "        all_logits = []\n",
    "        \n",
    "        for params_sample in laplace_samples:\n",
    "            with torch.no_grad():\n",
    "                logits, _ = torch.func.functional_call(model, params_sample, (x,))\n",
    "                # Take last position logits\n",
    "                last_logits = logits[:, -1, :]\n",
    "                all_logits.append(last_logits)\n",
    "        \n",
    "        # Stack all predictions: [num_samples, vocab_size]\n",
    "        logits_stack = torch.stack(all_logits, dim=0).squeeze(1)\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        probs_stack = F.softmax(logits_stack, dim=-1)\n",
    "        \n",
    "        # Compute uncertainty metrics\n",
    "        mean_probs = probs_stack.mean(dim=0)  # [vocab_size]\n",
    "        \n",
    "        # Epistemic uncertainty (variance across samples)\n",
    "        epistemic = probs_stack.var(dim=0).sum().item()\n",
    "        \n",
    "        # Entropy of mean prediction\n",
    "        entropy_mean = -(mean_probs * torch.log(mean_probs + 1e-8)).sum().item()\n",
    "        \n",
    "        # Mean entropy across samples (aleatoric + epistemic)\n",
    "        entropies = -(probs_stack * torch.log(probs_stack + 1e-8)).sum(dim=-1)\n",
    "        mean_entropy = entropies.mean().item()\n",
    "        \n",
    "        # Mutual information (epistemic uncertainty)\n",
    "        mutual_info = mean_entropy - entropy_mean\n",
    "        \n",
    "        uncertainties.append({\n",
    "            'epistemic_variance': epistemic,\n",
    "            'entropy_of_mean': entropy_mean,\n",
    "            'mean_entropy': mean_entropy,\n",
    "            'mutual_information': mutual_info,\n",
    "            'input_length': len(input_seq)\n",
    "        })\n",
    "    \n",
    "    return uncertainties\n",
    "\n",
    "# Prepare test inputs of varying lengths and complexity\n",
    "test_sequences = [\n",
    "    \"To be\",\n",
    "    \"Romeo, Romeo\",\n",
    "    \"All the world's a stage\",\n",
    "    \"What light through yonder window\",\n",
    "    \"Now is the winter of our discontent\"\n",
    "]\n",
    "\n",
    "print(\"Computing predictive uncertainties...\")\n",
    "encoded_test_seqs = []\n",
    "for seq in test_sequences:\n",
    "    encoded = [stoi.get(c, 0) for c in seq]\n",
    "    encoded_test_seqs.append(encoded)\n",
    "    print(f\"  '{seq}' -> length {len(encoded)}\")\n",
    "\n",
    "uncertainties = compute_predictive_uncertainty(\n",
    "    model, laplace_samples[:5], encoded_test_seqs  # Use first 5 samples for speed\n",
    ")\n",
    "\n",
    "# Display uncertainty analysis\n",
    "print(f\"\\nUncertainty Analysis Results:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Prompt':<30} {'Epistemic':<12} {'Entropy':<10} {'Mutual Info':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, (seq, unc) in enumerate(zip(test_sequences, uncertainties)):\n",
    "    print(f\"{seq[:28]:<30} {unc['epistemic_variance']:<12.4f} \"\n",
    "          f\"{unc['entropy_of_mean']:<10.4f} {unc['mutual_information']:<12.4f}\")\n",
    "\n",
    "# Summary statistics\n",
    "epistemic_values = [u['epistemic_variance'] for u in uncertainties]\n",
    "entropy_values = [u['entropy_of_mean'] for u in uncertainties]\n",
    "mi_values = [u['mutual_information'] for u in uncertainties]\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'AVERAGE':<30} {np.mean(epistemic_values):<12.4f} \"\n",
    "      f\"{np.mean(entropy_values):<10.4f} {np.mean(mi_values):<12.4f}\")\n",
    "print(f\"{'STD DEV':<30} {np.std(epistemic_values):<12.4f} \"\n",
    "      f\"{np.std(entropy_values):<10.4f} {np.std(mi_values):<12.4f}\")\n",
    "\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(f\"  ‚Ä¢ Higher epistemic variance indicates model uncertainty about predictions\")\n",
    "print(f\"  ‚Ä¢ Mutual information quantifies information gain from additional samples\")\n",
    "print(f\"  ‚Ä¢ Entropy measures overall prediction uncertainty\")\n",
    "\n",
    "print(f\"\\nUncertainty quantification analysis complete! üìä\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a43ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization and Comparison Analysis\n",
    "print(\"=\"*60)\n",
    "print(\"VISUALIZATION AND COMPARISON ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create comprehensive comparison plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Laplace Approximation: Bayesian NanoGPT Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Uncertainty vs Input Length\n",
    "ax1 = axes[0, 0]\n",
    "input_lengths = [len(seq) for seq in test_sequences]\n",
    "epistemic_vars = [u['epistemic_variance'] for u in uncertainties]\n",
    "entropies = [u['entropy_of_mean'] for u in uncertainties]\n",
    "\n",
    "ax1.scatter(input_lengths, epistemic_vars, label='Epistemic Variance', alpha=0.7, s=100)\n",
    "ax1.scatter(input_lengths, entropies, label='Entropy of Mean', alpha=0.7, s=100)\n",
    "ax1.set_xlabel('Input Sequence Length')\n",
    "ax1.set_ylabel('Uncertainty')\n",
    "ax1.set_title('Uncertainty vs Input Length')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Fisher Information Distribution (sample a few parameters)\n",
    "ax2 = axes[0, 1]\n",
    "sample_fisher_values = []\n",
    "if hasattr(laplace_state, 'fisher'):\n",
    "    for name, fisher_diag in list(laplace_state.fisher.items())[:3]:\n",
    "        if hasattr(fisher_diag, 'flatten'):\n",
    "            values = fisher_diag.flatten().cpu().numpy()\n",
    "            sample_fisher_values.extend(values[:1000])  # Sample first 1000 values\n",
    "\n",
    "if sample_fisher_values:\n",
    "    ax2.hist(sample_fisher_values, bins=50, alpha=0.7, density=True)\n",
    "    ax2.set_xlabel('Fisher Information Value')\n",
    "    ax2.set_ylabel('Density')\n",
    "    ax2.set_title('Fisher Information Distribution')\n",
    "    ax2.set_yscale('log')\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'Fisher Info\\nNot Available', \n",
    "             transform=ax2.transAxes, ha='center', va='center')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Parameter Uncertainty Comparison\n",
    "ax3 = axes[0, 2]\n",
    "if len(laplace_samples) > 1:\n",
    "    param_variations = []\n",
    "    param_names = []\n",
    "    \n",
    "    for name in list(laplace_samples[0].keys())[:5]:  # First 5 layers\n",
    "        variations = []\n",
    "        base_param = laplace_samples[0][name]\n",
    "        \n",
    "        for sample in laplace_samples[1:4]:  # Compare with next 3 samples\n",
    "            diff = (sample[name] - base_param).norm().item()\n",
    "            relative_diff = diff / base_param.norm().item()\n",
    "            variations.append(relative_diff)\n",
    "        \n",
    "        param_variations.append(variations)\n",
    "        param_names.append(name.split('.')[-1][:10])  # Short name\n",
    "    \n",
    "    # Box plot of parameter variations\n",
    "    ax3.boxplot(param_variations, labels=param_names)\n",
    "    ax3.set_ylabel('Relative Parameter Variation')\n",
    "    ax3.set_title('Parameter Uncertainty Across Layers')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'Need >1 Sample\\nfor Comparison', \n",
    "             transform=ax3.transAxes, ha='center', va='center')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Text Generation Diversity\n",
    "ax4 = axes[1, 0]\n",
    "prompt_for_analysis = \"To be, or not to be\"\n",
    "\n",
    "# Analyze character diversity in generated text\n",
    "char_diversities = []\n",
    "for output in bayesian_outputs:\n",
    "    generated_part = output[len(prompt_for_analysis):].strip()\n",
    "    unique_chars = len(set(generated_part))\n",
    "    total_chars = len(generated_part)\n",
    "    diversity = unique_chars / max(total_chars, 1)\n",
    "    char_diversities.append(diversity)\n",
    "\n",
    "sample_indices = range(1, len(char_diversities) + 1)\n",
    "ax4.bar(sample_indices, char_diversities, alpha=0.7)\n",
    "ax4.axhline(np.mean(char_diversities), color='red', linestyle='--', \n",
    "            label=f'Mean: {np.mean(char_diversities):.3f}')\n",
    "ax4.set_xlabel('Posterior Sample')\n",
    "ax4.set_ylabel('Character Diversity Ratio')\n",
    "ax4.set_title('Text Generation Diversity')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Uncertainty Metrics Comparison\n",
    "ax5 = axes[1, 1]\n",
    "metric_names = ['Epistemic\\nVariance', 'Entropy\\nof Mean', 'Mutual\\nInformation']\n",
    "metric_values = [\n",
    "    np.mean(epistemic_values),\n",
    "    np.mean(entropy_values), \n",
    "    np.mean(mi_values)\n",
    "]\n",
    "metric_stds = [\n",
    "    np.std(epistemic_values),\n",
    "    np.std(entropy_values),\n",
    "    np.std(mi_values)\n",
    "]\n",
    "\n",
    "bars = ax5.bar(metric_names, metric_values, yerr=metric_stds, \n",
    "               capsize=5, alpha=0.7, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "ax5.set_ylabel('Uncertainty Value')\n",
    "ax5.set_title('Average Uncertainty Metrics')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, metric_values):\n",
    "    height = bar.get_height()\n",
    "    ax5.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{value:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Plot 6: Performance Summary\n",
    "ax6 = axes[1, 2]\n",
    "if generation_times:\n",
    "    avg_gen_time = np.mean(generation_times)\n",
    "    std_gen_time = np.std(generation_times)\n",
    "else:\n",
    "    avg_gen_time = 0\n",
    "    std_gen_time = 0\n",
    "\n",
    "performance_metrics = [\n",
    "    ('Samples', len(laplace_samples)),\n",
    "    ('Avg Gen Time (ms)', avg_gen_time),\n",
    "    ('Uncertainty Range', max(epistemic_values) - min(epistemic_values)),\n",
    "    ('Diversity Score', np.mean(char_diversities))\n",
    "]\n",
    "\n",
    "y_pos = range(len(performance_metrics))\n",
    "values = [metric[1] for metric in performance_metrics]\n",
    "normalized_values = [v / max(values) for v in values]  # Normalize for comparison\n",
    "\n",
    "bars = ax6.barh(y_pos, normalized_values, alpha=0.7)\n",
    "ax6.set_yticks(y_pos)\n",
    "ax6.set_yticklabels([metric[0] for metric in performance_metrics])\n",
    "ax6.set_xlabel('Normalized Score')\n",
    "ax6.set_title('Performance Summary')\n",
    "\n",
    "# Add actual values as text\n",
    "for i, (bar, (name, value)) in enumerate(zip(bars, performance_metrics)):\n",
    "    width = bar.get_width()\n",
    "    if 'Time' in name:\n",
    "        text = f'{value:.1f}'\n",
    "    elif 'Samples' in name:\n",
    "        text = f'{int(value)}'\n",
    "    else:\n",
    "        text = f'{value:.3f}'\n",
    "    ax6.text(width + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "             text, ha='left', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comprehensive summary\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"LAPLACE APPROXIMATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ Fitted diagonal Fisher information matrix\")\n",
    "print(f\"‚úÖ Generated {len(laplace_samples)} posterior parameter samples\")\n",
    "print(f\"‚úÖ Performed Bayesian text generation\")\n",
    "print(f\"‚úÖ Quantified predictive uncertainty\")\n",
    "\n",
    "print(f\"\\nKey Results:\")\n",
    "print(f\"  ‚Ä¢ Average epistemic uncertainty: {np.mean(epistemic_values):.4f}\")\n",
    "print(f\"  ‚Ä¢ Average prediction entropy: {np.mean(entropy_values):.4f}\")\n",
    "print(f\"  ‚Ä¢ Average mutual information: {np.mean(mi_values):.4f}\")\n",
    "print(f\"  ‚Ä¢ Text generation diversity: {np.mean(char_diversities):.4f}\")\n",
    "\n",
    "if generation_times:\n",
    "    print(f\"  ‚Ä¢ Average generation time: {avg_gen_time:.1f}ms per sample\")\n",
    "\n",
    "print(f\"\\nBenefits of Laplace Approximation:\")\n",
    "print(f\"  üöÄ Fast post-hoc conversion of pre-trained models\")\n",
    "print(f\"  üìä Quantifies parameter and predictive uncertainty\")  \n",
    "print(f\"  üéØ Provides calibrated confidence estimates\")\n",
    "print(f\"  üí° Reveals model uncertainty in different contexts\")\n",
    "\n",
    "print(f\"\\nLaplace approximation analysis complete! üéâ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dba9a5e",
   "metadata": {},
   "source": [
    "## 1. Laplace Approximation\n",
    "\n",
    "The Laplace approximation provides a Gaussian approximation to the posterior distribution around the maximum a posteriori (MAP) estimate. This is computationally efficient and gives us uncertainty estimates with minimal additional computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f380dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Laplace approximation using diagonal Fisher information\n",
    "print(\"=\"*60)\n",
    "print(\"LAPLACE APPROXIMATION WITH DIAGONAL FISHER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get model parameters\n",
    "params = dict(model.named_parameters())\n",
    "print(f\"Model has {len(params)} parameter groups\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in params.values()):,}\")\n",
    "\n",
    "# Build the Laplace transform with diagonal Fisher information\n",
    "laplace_transform = posteriors.laplace.diag_fisher.build(\n",
    "    log_posterior,\n",
    "    len(data_batches)  # number of data points for scaling\n",
    ")\n",
    "\n",
    "print(\"Initializing Laplace approximation...\")\n",
    "laplace_state = laplace_transform.init(params)\n",
    "\n",
    "print(\"Computing diagonal Fisher information matrix...\")\n",
    "# Update with several batches to get good Fisher information estimate\n",
    "for i, batch in enumerate(data_batches[:10]):  # Use first 10 batches\n",
    "    print(f\"Processing batch {i+1}/10\", end='\\r')\n",
    "    laplace_state, _ = laplace_transform.update(laplace_state, batch)\n",
    "\n",
    "print(f\"\\nLaplace approximation completed!\")\n",
    "print(f\"Posterior mean computed: {len(laplace_state.params)} parameter groups\")\n",
    "print(f\"Posterior covariance (diagonal): {len(laplace_state.aux)} parameter groups\")\n",
    "\n",
    "# Sample from the Laplace posterior\n",
    "num_samples = 5\n",
    "laplace_samples = []\n",
    "\n",
    "print(f\"\\nSampling {num_samples} parameter sets from Laplace posterior...\")\n",
    "for i in range(num_samples):\n",
    "    # Sample from the posterior\n",
    "    sample = posteriors.tree_utils.tree_map(\n",
    "        lambda mean, var: torch.normal(mean, torch.sqrt(var.clamp(min=1e-8))),\n",
    "        laplace_state.params,\n",
    "        laplace_state.aux\n",
    "    )\n",
    "    laplace_samples.append(sample)\n",
    "    print(f\"Generated sample {i+1}/{num_samples}\")\n",
    "\n",
    "print(f\"Successfully generated {len(laplace_samples)} Laplace posterior samples!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
