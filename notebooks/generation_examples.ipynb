{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e27f40",
   "metadata": {},
   "source": [
    "### Text Generation with Pretrained NanoGPT\n",
    "\n",
    "This notebook shows how to generate text using a pretrained NanoGPT model.\n",
    "\n",
    "By default:\n",
    "\n",
    "1. The pretrained model and metadata are loaded from the `baselines/nanogpt/shakespeare-char` folder.\n",
    "2. Generation starts from a configurable prompt (e.g., \"To be, or not to be...\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34abec1",
   "metadata": {},
   "source": [
    "Loading libraries and setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0adcd592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "859b981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "from baselines.nanogpt.model import GPT, GPTConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3142eeba",
   "metadata": {},
   "source": [
    "Configuring the model for sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6fc1846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#? Maybe we can have a config.py file to avoid hardcoding paths\n",
    "# MODEL_PATH = root_path + \"/baselines/nanogpt/shakespeare-char/baseline_model.pt\"\n",
    "# META_PATH = root_path + \"/baselines/nanogpt/shakespeare-char/meta.pkl\"\n",
    "from config import MODEL_PATH, META_PATH\n",
    "\n",
    "\n",
    "\n",
    "START_PROMPT = \"to be, or not to be -that is the question:\\n\"\n",
    "NUM_SAMPLES = 1\n",
    "MAX_NEW_TOKENS = 500\n",
    "TEMPERATURE = 0.8\n",
    "TOP_K = 200\n",
    "SEED = 42\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DTYPE = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "COMPILE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffc44625",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "device_type = 'cuda' if DEVICE == 'cuda' else 'cpu'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[DTYPE]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1accb587",
   "metadata": {},
   "source": [
    "Loading the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bf10d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 10.65M\n"
     ]
    }
   ],
   "source": [
    "# Loading model\n",
    "checkpoint = torch.load(MODEL_PATH, map_location='cpu')\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "\n",
    "# Remove unwanted prefixes if any\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k, v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval().to(DEVICE)\n",
    "\n",
    "if COMPILE:\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# Loading tokenizer\n",
    "with open(META_PATH, 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "\n",
    "stoi, itos = meta['stoi'], meta['itos']\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72670582",
   "metadata": {},
   "source": [
    "Example: Shakespeare text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a7e9c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_PROMPT = \"to be, or\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63f54db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample 1 ---\n",
      "to be, or the king-confess;\n",
      "And therefore a maid be speak in the certain\n",
      "To mine solemn the war king many of Hermio?\n",
      "\n",
      "JOHN OF GAUNT:\n",
      "A prisoner day, that same hath been\n",
      "That dargerous nighter, of it is now to-morrow\n",
      "The force of the ladies of Rome, and of which say\n",
      "That she were to Could have so other of might\n",
      "To see the base. What pleasure is the bear\n",
      "More?\n",
      "\n",
      "POLIXENES:\n",
      "Tull I call to his daughter.\n",
      "\n",
      "MENENIUS:\n",
      "Well, I must prithee to thee,\n",
      "If it is in me country's sadness.\n",
      "\n",
      "COMINIUS:\n",
      "Come, you have idle t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Encoding prompt \n",
    "if START_PROMPT.startswith(\"FILE:\"):\n",
    "    with open(START_PROMPT[5:], 'r', encoding='utf-8') as f:\n",
    "        START_PROMPT = f.read()\n",
    "\n",
    "start_ids = encode(START_PROMPT)\n",
    "x = torch.tensor(start_ids, dtype=torch.long, device=DEVICE)[None, ...]\n",
    "\n",
    "# Generating text samples\n",
    "with torch.no_grad():\n",
    "    for i in range(NUM_SAMPLES):\n",
    "        y = model.generate(x, max_new_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE, top_k=TOP_K)\n",
    "        print(f\"--- Sample {i+1} ---\")\n",
    "        print(decode(y[0].tolist()))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aa5b81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
