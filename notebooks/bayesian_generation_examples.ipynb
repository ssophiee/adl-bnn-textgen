{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcbadaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sofianikolenko/Downloads/Projects_25/ADL/adl-bnn-textgen/.venv/lib/python3.14/site-packages/posteriors/utils.py:9: FutureWarning: The 'optree.integration' module is deprecated and will be removed in version 0.18.0. Please use 'optree.integrations' instead.\n",
      "  from optree.integration.torch import tree_ravel\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Text Generation from Bayesian Neural Network (BNN) using Posteriors Library\n",
    "\n",
    "This script shows how to properly generate diverse text by sampling \n",
    "different parameter sets from the learned posterior distribution.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(root_path)\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from contextlib import nullcontext\n",
    "from torch import func\n",
    "import posteriors\n",
    "\n",
    "# Load your model architecture\n",
    "from baselines.nanogpt.model import GPT, GPTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f492c619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import MODEL_PATH, META_PATH, BNN_MODEL_PATH\n",
    "START_PROMPT = \"I keep on burning deadlines,\"\n",
    "NUM_SAMPLES = 1  # Generate 5 different completions\n",
    "MAX_NEW_TOKENS = 300\n",
    "TEMPERATURE = 0.8\n",
    "TOP_K = 200\n",
    "SEED = 42\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "SAMPLER_TYPE = 'vi' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53e55f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 10.65M\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(BNN_MODEL_PATH, map_location='cpu', weights_only=False)\n",
    "\n",
    "# Initialize model\n",
    "model_args = checkpoint.get('model_args', {\n",
    "    'n_layer': 6, 'n_head': 6, 'n_embd': 384, \n",
    "    'block_size': 256, 'bias': False, 'vocab_size': 65, 'dropout': 0.0\n",
    "})\n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf)\n",
    "model.eval()  \n",
    "model.to(DEVICE)\n",
    "\n",
    "# Load tokenizer\n",
    "with open(META_PATH, 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta['stoi'], meta['itos']\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87d7719",
   "metadata": {},
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfec1c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_text_bayesian(model, state, start_prompt, encode_fn, decode_fn, \n",
    "                          max_new_tokens=500, temperature=1.0, top_k=None,\n",
    "                          num_samples=1, use_uncertainty=True):\n",
    "    \"\"\"\n",
    "    Generate text using Bayesian model with uncertainty\n",
    "    \n",
    "    Args:\n",
    "        model: The base model\n",
    "        state: Bayesian sampler state (VIDiagState, etc.)\n",
    "        start_prompt: Starting text string\n",
    "        encode_fn: Function to encode text to tokens\n",
    "        decode_fn: Function to decode tokens to text\n",
    "        max_new_tokens: Number of tokens to generate\n",
    "        temperature: Sampling temperature (higher = more random)\n",
    "        top_k: If set, only sample from top k tokens\n",
    "        num_samples: Number of posterior samples to use (1 = posterior mean)\n",
    "        use_uncertainty: If True, sample from posterior; if False, use mean params\n",
    "    \n",
    "    Returns:\n",
    "        generated_text: String of generated text\n",
    "        uncertainty_info: Dict with token-level uncertainties (if num_samples > 1)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode starting prompt\n",
    "    context = torch.tensor(encode_fn(start_prompt), dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
    "    \n",
    "    # Get parameters to use\n",
    "    if use_uncertainty and num_samples > 1:\n",
    "        # Sample multiple parameter sets from posterior\n",
    "        param_samples = []\n",
    "        for _ in range(num_samples):\n",
    "            if hasattr(state, 'log_sd_diag'):  # VI, EKF, Laplace\n",
    "                sample_params = posteriors.vi.diag.sample(state)\n",
    "            else:\n",
    "                sample_params = state.params\n",
    "            param_samples.append(sample_params)\n",
    "    else:\n",
    "        # Use posterior mean\n",
    "        param_samples = [state.params]\n",
    "    \n",
    "    generated_tokens = []\n",
    "    token_uncertainties = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get predictions from all parameter samples\n",
    "            all_logits = []\n",
    "            \n",
    "            for params in param_samples:\n",
    "                # Forward pass with current parameters\n",
    "                logits, _ = func.functional_call(model, params, (context,))\n",
    "                # Get logits for last token\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "                all_logits.append(logits)\n",
    "            \n",
    "            # Average logits across samples\n",
    "            avg_logits = torch.stack(all_logits).mean(dim=0)\n",
    "            \n",
    "            # Calculate uncertainty (entropy of averaged predictions)\n",
    "            if len(all_logits) > 1:\n",
    "                probs = torch.stack([F.softmax(l, dim=-1) for l in all_logits])\n",
    "                mean_probs = probs.mean(dim=0)\n",
    "                entropy = -(mean_probs * torch.log(mean_probs + 1e-8)).sum(dim=-1)\n",
    "                token_uncertainties.append(entropy.item())\n",
    "            \n",
    "            # Apply top-k filtering if specified\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(avg_logits, min(top_k, avg_logits.size(-1)))\n",
    "                avg_logits[avg_logits < v[:, [-1]]] = -float('Inf')\n",
    "            \n",
    "            # Sample next token\n",
    "            probs = F.softmax(avg_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            generated_tokens.append(next_token.item())\n",
    "            \n",
    "            # Append to context\n",
    "            context = torch.cat([context, next_token], dim=1)\n",
    "            \n",
    "            # Optionally crop context to max sequence length\n",
    "            if context.size(1) > model.config.block_size:\n",
    "                context = context[:, -model.config.block_size:]\n",
    "    \n",
    "    # Decode generated tokens\n",
    "    generated_text = decode_fn(generated_tokens)\n",
    "    full_text = start_prompt + generated_text\n",
    "    \n",
    "    uncertainty_info = {\n",
    "        'token_uncertainties': token_uncertainties,\n",
    "        'avg_uncertainty': np.mean(token_uncertainties) if token_uncertainties else 0.0,\n",
    "        'max_uncertainty': np.max(token_uncertainties) if token_uncertainties else 0.0\n",
    "    }\n",
    "    \n",
    "    return full_text, uncertainty_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec2facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_PROMPT = \"to be, or\"\n",
    "from posteriors.vi.diag import VIDiagState\n",
    "params = {k: v.to(DEVICE) for k, v in checkpoint['sampler_state_params'].items()}\n",
    "\n",
    "state = VIDiagState(\n",
    "    params=params, \n",
    "    log_sd_diag=checkpoint['log_sd_diag'],\n",
    "    opt_state=checkpoint['opt_state'] \n",
    ")  \n",
    "\n",
    "text_v2, unc_info = generate_text_bayesian(\n",
    "    model, state, START_PROMPT, encode, decode,\n",
    "    max_new_tokens=600,\n",
    "    temperature=0.4, \n",
    "    top_k=10,        \n",
    "    num_samples=5,\n",
    "    use_uncertainty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0c51e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Bayesian Generation with Uncertainty ===\n",
      "\n",
      "Average uncertainty: 0.5176\n",
      "Max uncertainty: 2.4535\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Bayesian Generation with Uncertainty ===\")\n",
    "print(f\"\\nAverage uncertainty: {unc_info['avg_uncertainty']:.4f}\")\n",
    "print(f\"Max uncertainty: {unc_info['max_uncertainty']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "895c0ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be, the story Capulets and Saint George by the princes\n",
      "Of all the world at hands me: the heavens are by the\n",
      "That may strange her made me to the gates of a man the\n",
      "And bite makes a fall of any thing and the guest content be abroad;\n",
      "And yet I'll be ready to me; and therefore I do beseech you,\n",
      "Who doth not proclaim your son of lend and cry your lordship.\n",
      "\n",
      "LUCIO:\n",
      "No, by your honour, I know your honest sir; I pray your son\n",
      "Where is not advantage of such a hands.\n",
      "\n",
      "LUCIO:\n",
      "You shall not be so. You have been a man of the world and the state of your\n",
      "proclaim of a strange any thing, you say your son your son\n"
     ]
    }
   ],
   "source": [
    "print(text_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f05cf78",
   "metadata": {},
   "source": [
    "SGLD/SGHMC/BAOA checkpoint loading example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "906b507a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 collected samples from checkpoint\n"
     ]
    }
   ],
   "source": [
    "from src.generation_utils import load_checkpoint_for_generation, generate_text_bayesian_sgmcmc, save_generation_result\n",
    "\n",
    "\n",
    "checkpoint_data = load_checkpoint_for_generation(BNN_MODEL_PATH, device=DEVICE)\n",
    "\n",
    "# # Generate text using collected SGMCMC samples\n",
    "START_PROMPT = \"to be, or not to be;\"\n",
    "collected_samples = checkpoint_data['collected_samples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3f30138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 20 SGMCMC samples for generation\n"
     ]
    }
   ],
   "source": [
    "text, unc_info = generate_text_bayesian_sgmcmc(\n",
    "    model, collected_samples, START_PROMPT, encode, decode,\n",
    "    max_new_tokens=600,\n",
    "    temperature=0.3,\n",
    "    top_k=10,\n",
    "    num_samples=20  # Use 20 of the collected samples (~100 total available)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9d6121d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generation result with ID: sgmcmc_example_3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sgmcmc_example_3'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_generation_result(START_PROMPT, text, unc_info,\n",
    "    max_new_tokens=600,\n",
    "    temperature=0.3,\n",
    "    top_k=10,\n",
    "    num_samples=20,\n",
    "    collected_samples=collected_samples,\n",
    "    sample_id=\"sgmcmc_example_3\",\n",
    "    save_path=\"/Users/sofianikolenko/Downloads/Projects_25/ADL/adl-bnn-textgen/checkpoints/generation_results/generation_results_sgmcmc.json\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f28e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
