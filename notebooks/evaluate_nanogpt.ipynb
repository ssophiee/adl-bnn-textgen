{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e684c217",
   "metadata": {},
   "source": [
    "# NanoGPT Model Evaluation\n",
    "\n",
    "This notebook loads a trained NanoGPT model and evaluates it on train.bin and val.bin datasets using multiple metrics:\n",
    "- **Perplexity**: Measures how well the model predicts the next token\n",
    "- **BLEU Score**: Measures similarity between generated and reference text\n",
    "- **ROUGE Scores**: Measures overlap of n-grams between generated and reference text\n",
    "\n",
    "## Usage\n",
    "1. Configure the paths and parameters in the configuration cell\n",
    "2. Run all cells to perform the evaluation\n",
    "3. View results in the final summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8e75dc",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7abcd78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Configure logging to both file and console\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create formatters\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "file_handler = logging.FileHandler('nanogpt_evaluation.log', mode='a')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "\n",
    "# Add the required paths for importing\n",
    "current_dir = Path.cwd()\n",
    "sys.path.append(str(current_dir / \"baselines/nanogpt/shakespeare-char/models\"))\n",
    "sys.path.append(str(current_dir / \"notebooks\"))\n",
    "\n",
    "print(f\"Current directory: {current_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61fe1329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5634e17d",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set your model paths and evaluation parameters here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f51745b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  data_dir: nanoGPT/data/shakespeare_char\n",
      "  model_path: ../checkpoints/baseline_nanogpt/baseline_nanogpt.pt\n",
      "  meta_path: ../checkpoints/baseline_nanogpt/meta.pkl\n",
      "  batch_size: 32\n",
      "  max_eval_samples: 100\n",
      "  device: auto\n",
      "  splits: ['val', 'train']\n",
      "  num_text_samples: 10\n",
      "  prompt_length: 20\n",
      "  generation_length: 30\n",
      "  max_tokens: None\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # 'data_dir': \"/Users/sofianikolenko/Downloads\",\n",
    "    # 'model_path': parent_dir / 'checkpoints/baseline/models/baseline_model_2k.pt',\n",
    "    # 'meta_path': parent_dir / 'checkpoints/baseline/models/meta.pkl',\n",
    "    # 'data_dir': 'nanoGPT/data/shakespeare',\n",
    "    # 'model_path': '../checkpoints/baseline_token_level_nano/token_level_1500_iter.pt',\n",
    "    # 'meta_path': '../checkpoints/baseline_nanogpt/nanogpt_meta.pkl',\n",
    "    # 'data_dir': \"/Users/sofianikolenko/Downloads\",\n",
    "    # 'model_path': parent_dir / 'baselines/nanogpt/shakespeare-char/models/baseline_model_2k.pt',\n",
    "    # 'meta_path': parent_dir / 'baselines/nanogpt/shakespeare-char/models/meta.pkl',\n",
    "    'data_dir': 'nanoGPT/data/shakespeare_char',\n",
    "    'model_path': '../checkpoints/baseline_nanogpt/baseline_nanogpt.pt',\n",
    "    'meta_path': '../checkpoints/baseline_nanogpt/meta.pkl',\n",
    "\n",
    "    'batch_size': 32,\n",
    "    'max_eval_samples': 100,\n",
    "    'device': 'auto',  # 'auto', 'cpu', or 'cuda'\n",
    "    'splits': ['val', 'train'],  # Dataset splits to evaluate\n",
    "    'num_text_samples': 10,  # Number of text samples for BLEU/ROUGE\n",
    "    'prompt_length': 20,  # Length of prompt for text generation\n",
    "    'generation_length': 30,  # Length of generated text,\n",
    "    \"max_tokens\": None # for fast debug, None = all\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Check if paths exist\n",
    "for path_key in ['data_dir', 'model_path', 'meta_path']:\n",
    "    path = Path(CONFIG[path_key])\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Required path not found: {path}\")\n",
    "\n",
    "logger.debug(f\"Configuration: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa5259",
   "metadata": {},
   "source": [
    "<!-- ## 3. Alternative Utility Functions\n",
    "\n",
    "These functions provide fallback implementations if the utils module is not available: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66c2c364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved ROOT_DIR: c:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\n",
      "sys.path contains src? False\n",
      "MODEL_PATH env: ../checkpoints/baseline_nanogpt/baseline_nanogpt.pt\n",
      "META_PATH env: ../checkpoints/baseline_nanogpt/meta.pkl\n",
      "DATA_DIR env: nanoGPT/data/shakespeare_char\n",
      "Imported nanogpt_utils successfully.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Project Root Resolution ---\n",
    "# Notebook lives in <repo>/notebooks; project root is parent of current working directory.\n",
    "ROOT_DIR = Path.cwd().parent\n",
    "if not (ROOT_DIR / 'baselines' / 'nanogpt' / 'model.py').exists():\n",
    "    # Fallback: walk up until we find baselines/nanogpt/model.py\n",
    "    for p in Path.cwd().parents:\n",
    "        if (p / 'baselines' / 'nanogpt' / 'model.py').exists():\n",
    "            ROOT_DIR = p\n",
    "            break\n",
    "\n",
    "# Set BASE_DIR so config.py uses the proper root (config.py defaults to cwd if BASE_DIR unset)\n",
    "os.environ['BASE_DIR'] = str(ROOT_DIR)\n",
    "os.environ['MODEL_PATH'] = CONFIG['model_path']\n",
    "os.environ['META_PATH'] = CONFIG['meta_path']\n",
    "os.environ['DATA_DIR'] = CONFIG['data_dir']\n",
    "\n",
    "# Ensure src and baselines are on sys.path for imports\n",
    "src_path = ROOT_DIR / 'src'\n",
    "baselines_path = ROOT_DIR / 'baselines'\n",
    "for p in (src_path, baselines_path):\n",
    "    sp = str(p)\n",
    "    if sp not in sys.path:\n",
    "        sys.path.append(sp)\n",
    "\n",
    "print(f\"Resolved ROOT_DIR: {ROOT_DIR}\")\n",
    "print(f\"sys.path contains src? {'src' in sp.lower() if 'sp' in locals() else 'N/A'}\")\n",
    "print(f\"MODEL_PATH env: {os.environ['MODEL_PATH']}\")\n",
    "print(f\"META_PATH env: {os.environ['META_PATH']}\")\n",
    "print(f\"DATA_DIR env: {os.environ['DATA_DIR']}\")\n",
    "\n",
    "try:\n",
    "    from nanogpt_utils import load_model, load_tokenizer, encode, decode\n",
    "    print(\"Imported nanogpt_utils successfully.\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(\"Failed to import nanogpt_utils. sys.path:\")\n",
    "    for p in sys.path:\n",
    "        if 'baselines' in p or 'src' in p:\n",
    "            print('  ', p)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3765de",
   "metadata": {},
   "source": [
    "## 4. NanoGPT Evaluator Class\n",
    "\n",
    "This class handles model loading and evaluation with multiple metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678f9ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NanoGPTEvaluator class defined\n"
     ]
    }
   ],
   "source": [
    "# Replaced inline class definition with import from reusable module\n",
    "from evaluation.nanogpt_evaluator import NanoGPTEvaluator, evaluate_splits\n",
    "print(\"Imported NanoGPTEvaluator from evaluation.nanogpt_evaluator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab6659b",
   "metadata": {},
   "source": [
    "## 5. Initialize the Evaluator\n",
    "\n",
    "Load the model and initialize the evaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f77bf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing NanoGPT Evaluator...\n",
      "==================================================\n",
      "Model: ../checkpoints/baseline_nanogpt/baseline_nanogpt.pt\n",
      "Data: nanoGPT/data/shakespeare_char | ['input.txt', 'meta.pkl', 'prepare.py', 'readme.md', 'train.bin', 'val.bin']\n",
      "Meta: ../checkpoints/baseline_nanogpt/meta.pkl\n",
      "Batch size: 32\n",
      "Max eval samples: 100\n",
      "Splits: ['val', 'train']\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model arguments: {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'block_size': 256, 'bias': False, 'vocab_size': 65, 'dropout': 0.2}\n",
      "number of parameters: 10.65M\n",
      "Model loaded successfully!\n",
      "Number of parameters: 10,745,088\n",
      "number of parameters: 10.65M\n",
      "Model loaded successfully!\n",
      "Number of parameters: 10,745,088\n",
      "HuggingFace evaluation metrics loaded successfully\n",
      "\n",
      "Evaluator initialized successfully!\n",
      "HuggingFace evaluation metrics loaded successfully\n",
      "\n",
      "Evaluator initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize evaluator\n",
    "print(\"Initializing NanoGPT Evaluator...\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model: {CONFIG['model_path']}\")\n",
    "print(f\"Data: {CONFIG['data_dir']} | {os.listdir(CONFIG['data_dir'])}\")\n",
    "print(f\"Meta: {CONFIG['meta_path']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Max eval samples: {CONFIG['max_eval_samples']}\")\n",
    "print(f\"Splits: {CONFIG['splits']}\")\n",
    "\n",
    "try:\n",
    "    evaluator = NanoGPTEvaluator(\n",
    "        CONFIG['model_path'], \n",
    "        CONFIG['meta_path'], \n",
    "        CONFIG['device'],\n",
    "    )\n",
    "    print(\"\\nEvaluator initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing evaluator: {e}\")\n",
    "    evaluator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c039b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1d4ad1a",
   "metadata": {},
   "source": [
    "## 6. Run Evaluation\n",
    "\n",
    "Evaluate the model on the specified dataset splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3df396e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating val split...\n",
      "\n",
      "==================================================\n",
      "Evaluating on val set\n",
      "==================================================\n",
      "Loaded val data: 111,540 tokens\n",
      "\n",
      "1. Calculating Perplexity...\n",
      "Calculating perplexity with 32 batch size...\n",
      "Computing perplexity for 96 text samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de866544c9054976afa9f2594fc35a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 139.7433\n",
      "Perplexity: 139.7433 (took 35.06s)\n",
      "\n",
      "2. Generating samples for BLEU/ROUGE evaluation...\n",
      "Generating 5 samples for BLEU/ROUGE evaluation...\n",
      "Generating 5 text samples...\n",
      "Successfully generated 5 sample pairs\n",
      "\n",
      "3. Calculating BLEU score...\n",
      "  BLEU details: {'bleu': 0.1466420090326979, 'precisions': [0.5369127516778524, 0.20833333333333334, 0.07913669064748201, 0.05223880597014925], 'brevity_penalty': 1.0, 'length_ratio': 1.0067567567567568, 'translation_length': 149, 'reference_length': 148}\n",
      "BLEU Score: 0.1466\n",
      "\n",
      "4. Calculating ROUGE scores...\n",
      "Successfully generated 5 sample pairs\n",
      "\n",
      "3. Calculating BLEU score...\n",
      "  BLEU details: {'bleu': 0.1466420090326979, 'precisions': [0.5369127516778524, 0.20833333333333334, 0.07913669064748201, 0.05223880597014925], 'brevity_penalty': 1.0, 'length_ratio': 1.0067567567567568, 'translation_length': 149, 'reference_length': 148}\n",
      "BLEU Score: 0.1466\n",
      "\n",
      "4. Calculating ROUGE scores...\n",
      "  ROUGE details: {'rouge1': np.float64(0.5389401909214884), 'rouge2': np.float64(0.2087373606429868), 'rougeL': np.float64(0.31676602376777707), 'rougeLsum': np.float64(0.3727432216905901)}\n",
      "ROUGE-1: 0.5389\n",
      "ROUGE-2: 0.2087\n",
      "ROUGE-L: 0.3168\n",
      "\n",
      "5. Example generations:\n",
      "\n",
      "Example 1:\n",
      "Reference: less else her son.\n",
      "\n",
      "PETRUCHIO:...\n",
      "Generated: h a metter solemn\n",
      "is shape cha...\n",
      "\n",
      "Example 2:\n",
      "Reference: curstest shrew.\n",
      "Give me thy ha...\n",
      "Generated: law.\n",
      "\n",
      "POMPEY:\n",
      "A brothers and t...\n",
      "\n",
      "Example 3:\n",
      "Reference: ve:\n",
      "If you like me, she shall...\n",
      "Generated: ve vains your complat\n",
      "To slew...\n",
      "\n",
      "Evaluation completed in 3.11s\n",
      "val evaluation completed\n",
      "\n",
      "Evaluating train split...\n",
      "\n",
      "==================================================\n",
      "Evaluating on train set\n",
      "==================================================\n",
      "Loaded train data: 1,003,854 tokens\n",
      "\n",
      "1. Calculating Perplexity...\n",
      "Calculating perplexity with 32 batch size...\n",
      "Computing perplexity for 96 text samples...\n",
      "  ROUGE details: {'rouge1': np.float64(0.5389401909214884), 'rouge2': np.float64(0.2087373606429868), 'rougeL': np.float64(0.31676602376777707), 'rougeLsum': np.float64(0.3727432216905901)}\n",
      "ROUGE-1: 0.5389\n",
      "ROUGE-2: 0.2087\n",
      "ROUGE-L: 0.3168\n",
      "\n",
      "5. Example generations:\n",
      "\n",
      "Example 1:\n",
      "Reference: less else her son.\n",
      "\n",
      "PETRUCHIO:...\n",
      "Generated: h a metter solemn\n",
      "is shape cha...\n",
      "\n",
      "Example 2:\n",
      "Reference: curstest shrew.\n",
      "Give me thy ha...\n",
      "Generated: law.\n",
      "\n",
      "POMPEY:\n",
      "A brothers and t...\n",
      "\n",
      "Example 3:\n",
      "Reference: ve:\n",
      "If you like me, she shall...\n",
      "Generated: ve vains your complat\n",
      "To slew...\n",
      "\n",
      "Evaluation completed in 3.11s\n",
      "val evaluation completed\n",
      "\n",
      "Evaluating train split...\n",
      "\n",
      "==================================================\n",
      "Evaluating on train set\n",
      "==================================================\n",
      "Loaded train data: 1,003,854 tokens\n",
      "\n",
      "1. Calculating Perplexity...\n",
      "Calculating perplexity with 32 batch size...\n",
      "Computing perplexity for 96 text samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887ad47e060b4ddaa11aff83d11354d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 182.1334\n",
      "Perplexity: 182.1334 (took 30.14s)\n",
      "\n",
      "2. Generating samples for BLEU/ROUGE evaluation...\n",
      "Generating 5 samples for BLEU/ROUGE evaluation...\n",
      "Generating 5 text samples...\n",
      "Successfully generated 5 sample pairs\n",
      "\n",
      "3. Calculating BLEU score...\n",
      "  BLEU details: {'bleu': 0.0, 'precisions': [0.5763888888888888, 0.1223021582733813, 0.022388059701492536, 0.0], 'brevity_penalty': 0.9862071167439163, 'length_ratio': 0.9863013698630136, 'translation_length': 144, 'reference_length': 146}\n",
      "BLEU Score: 0.0000\n",
      "\n",
      "4. Calculating ROUGE scores...\n",
      "Successfully generated 5 sample pairs\n",
      "\n",
      "3. Calculating BLEU score...\n",
      "  BLEU details: {'bleu': 0.0, 'precisions': [0.5763888888888888, 0.1223021582733813, 0.022388059701492536, 0.0], 'brevity_penalty': 0.9862071167439163, 'length_ratio': 0.9863013698630136, 'translation_length': 144, 'reference_length': 146}\n",
      "BLEU Score: 0.0000\n",
      "\n",
      "4. Calculating ROUGE scores...\n",
      "  ROUGE details: {'rouge1': np.float64(0.572763088316935), 'rouge2': np.float64(0.1224068094304029), 'rougeL': np.float64(0.3589678462667481), 'rougeLsum': np.float64(0.3674417011950827)}\n",
      "ROUGE-1: 0.5728\n",
      "ROUGE-2: 0.1224\n",
      "ROUGE-L: 0.3590\n",
      "\n",
      "5. Example generations:\n",
      "\n",
      "Example 1:\n",
      "Reference: themselves, to your recomfort...\n",
      "Generated: my king dearly, lords by the...\n",
      "\n",
      "Example 2:\n",
      "Reference: let me use my sword, I'll make...\n",
      "Generated: with such a death county haste...\n",
      "\n",
      "Example 3:\n",
      "Reference: GLOUCESTER:\n",
      "Not I:\n",
      "No, God f...\n",
      "Generated: AUTOLYCUS:\n",
      "O, I am found foo...\n",
      "\n",
      "Evaluation completed in 3.01s\n",
      "train evaluation completed\n",
      "  ROUGE details: {'rouge1': np.float64(0.572763088316935), 'rouge2': np.float64(0.1224068094304029), 'rougeL': np.float64(0.3589678462667481), 'rougeLsum': np.float64(0.3674417011950827)}\n",
      "ROUGE-1: 0.5728\n",
      "ROUGE-2: 0.1224\n",
      "ROUGE-L: 0.3590\n",
      "\n",
      "5. Example generations:\n",
      "\n",
      "Example 1:\n",
      "Reference: themselves, to your recomfort...\n",
      "Generated: my king dearly, lords by the...\n",
      "\n",
      "Example 2:\n",
      "Reference: let me use my sword, I'll make...\n",
      "Generated: with such a death county haste...\n",
      "\n",
      "Example 3:\n",
      "Reference: GLOUCESTER:\n",
      "Not I:\n",
      "No, God f...\n",
      "Generated: AUTOLYCUS:\n",
      "O, I am found foo...\n",
      "\n",
      "Evaluation completed in 3.01s\n",
      "train evaluation completed\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation concisely using the imported module\n",
    "if 'evaluator' in globals() and evaluator is not None:\n",
    "    all_results = evaluate_splits(evaluator, CONFIG)\n",
    "else:\n",
    "    print(\"Cannot run evaluation - evaluator not initialized\")\n",
    "    all_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f355c",
   "metadata": {},
   "source": [
    "## 7. Results Summary\n",
    "\n",
    "Display a comprehensive summary of all evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abf50c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION SUMMARY\n",
      "============================================================\n",
      "Split Total Tokens Perplexity   BLEU ROUGE-1 ROUGE-2 ROUGE-L\n",
      "  VAL      111,540     0.0000 0.0775  0.4879  0.1386  0.3138\n",
      "TRAIN    1,003,854     0.0000 0.0602  0.5093  0.1217  0.3126\n",
      "\n",
      "Evaluation completed successfully!\n",
      "\n",
      "Results stored in 'evaluation_results' variable for further analysis\n"
     ]
    }
   ],
   "source": [
    "# Print comprehensive summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if all_results:\n",
    "    # Create a summary table\n",
    "    import pandas as pd\n",
    "    \n",
    "    summary_data = []\n",
    "    for split, results in all_results.items():\n",
    "        summary_data.append({\n",
    "            'Split': split.upper(),\n",
    "            'Total Tokens': f\"{results.get('total_tokens', 0):,}\",\n",
    "            'Perplexity': f\"{results.get('perplexity', 0):.4f}\",\n",
    "            'BLEU': f\"{results.get('bleu', 0):.4f}\",\n",
    "            'ROUGE-1': f\"{results.get('rouge1', 0):.4f}\",\n",
    "            'ROUGE-2': f\"{results.get('rouge2', 0):.4f}\",\n",
    "            'ROUGE-L': f\"{results.get('rougeL', 0):.4f}\"\n",
    "        })\n",
    "    \n",
    "    try:\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        print(df.to_string(index=False))\n",
    "    except:\n",
    "        # Fallback if pandas is not available\n",
    "        for split, results in all_results.items():\n",
    "            print(f\"\\n{split.upper()} SET:\")\n",
    "            print(f\"  Total tokens: {results.get('total_tokens', 0):,}\")\n",
    "            print(f\"  Perplexity:   {results.get('perplexity', 0):.4f}\")\n",
    "            print(f\"  BLEU:         {results.get('bleu', 0):.4f}\")\n",
    "            print(f\"  ROUGE-1:      {results.get('rouge1', 0):.4f}\")\n",
    "            print(f\"  ROUGE-2:      {results.get('rouge2', 0):.4f}\")\n",
    "            print(f\"  ROUGE-L:      {results.get('rougeL', 0):.4f}\")\n",
    "    \n",
    "    print(f\"\\nEvaluation completed successfully!\")\n",
    "    \n",
    "    # Store results for further analysis\n",
    "    evaluation_results = all_results\n",
    "    print(f\"\\nResults stored in 'evaluation_results' variable for further analysis\")\n",
    "else:\n",
    "    print(\"No evaluation results to display\")\n",
    "    evaluation_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bb8bb1",
   "metadata": {},
   "source": [
    "## 8. Additional Analysis (Optional)\n",
    "\n",
    "You can use this cell for additional analysis of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb8cf385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional Analysis:\n",
      "==============================\n",
      "\n",
      "Perplexity Comparison:\n",
      "  Training:   0.0000\n",
      "  Validation: 0.0000\n",
      "\n",
      "Text Generation Quality (val):\n",
      "  BLEU 0.0775: Low text similarity\n",
      "  ROUGE-1 0.4879: Good word overlap\n",
      "\n",
      "Text Generation Quality (train):\n",
      "  BLEU 0.0602: Low text similarity\n",
      "  ROUGE-1 0.5093: Good word overlap\n"
     ]
    }
   ],
   "source": [
    "# Additional analysis cell - customize as needed\n",
    "\n",
    "if evaluation_results:\n",
    "    print(\"Additional Analysis:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Compare train vs validation performance\n",
    "    if 'train' in evaluation_results and 'val' in evaluation_results:\n",
    "        train_ppl = evaluation_results['train'].get('perplexity', 0)\n",
    "        val_ppl = evaluation_results['val'].get('perplexity', 0)\n",
    "        \n",
    "        print(f\"\\nPerplexity Comparison:\")\n",
    "        print(f\"  Training:   {train_ppl:.4f}\")\n",
    "        print(f\"  Validation: {val_ppl:.4f}\")\n",
    "        \n",
    "        if train_ppl > 0 and val_ppl > 0:\n",
    "            ratio = val_ppl / train_ppl\n",
    "            print(f\"  Val/Train ratio: {ratio:.4f}\")\n",
    "            \n",
    "            if ratio > 1.5:\n",
    "                print(\"  High validation perplexity suggests overfitting\")\n",
    "            elif ratio < 1.1:\n",
    "                print(f\"  Good generalization - low overfitting\")\n",
    "            else:\n",
    "                print(f\"  Moderate generalization gap\")\n",
    "    \n",
    "    # Text generation quality assessment\n",
    "    for split in evaluation_results:\n",
    "        results = evaluation_results[split]\n",
    "        bleu = results.get('bleu', 0)\n",
    "        rouge1 = results.get('rouge1', 0)\n",
    "        \n",
    "        print(f\"\\nText Generation Quality ({split}):\")\n",
    "        if bleu > 0.3:\n",
    "            print(f\"  BLEU {bleu:.4f}: Good text similarity\")\n",
    "        elif bleu > 0.1:\n",
    "            print(f\"  BLEU {bleu:.4f}: Moderate text similarity\")\n",
    "        else:\n",
    "            print(f\"  BLEU {bleu:.4f}: Low text similarity\")\n",
    "        \n",
    "        if rouge1 > 0.3:\n",
    "            print(f\"  ROUGE-1 {rouge1:.4f}: Good word overlap\")\n",
    "        elif rouge1 > 0.15:\n",
    "            print(f\"  ROUGE-1 {rouge1:.4f}: Moderate word overlap\")\n",
    "        else:\n",
    "            print(f\"  ROUGE-1 {rouge1:.4f}: Low word overlap\")\n",
    "else:\n",
    "    print(\"No results available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3551d6",
   "metadata": {},
   "source": [
    "## 9. Export Results (Optional)\n",
    "\n",
    "Save the evaluation results to a file for later analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac77e559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results exported to: evaluation_results_20250919_144940.json\n"
     ]
    }
   ],
   "source": [
    "# Export results to JSON file\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "if evaluation_results:\n",
    "    # Add metadata\n",
    "    export_data = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'config': CONFIG,\n",
    "        'results': evaluation_results,\n",
    "        'model_info': {\n",
    "            'model_path': CONFIG['model_path'],\n",
    "            'meta_path': CONFIG['meta_path'],\n",
    "            'vocab_size': evaluator.vocab_size if evaluator else None,\n",
    "            'device': evaluator.device if evaluator else None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    output_file = f\"evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    \n",
    "    try:\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(export_data, f, indent=2)\n",
    "        print(f\"Results exported to: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting results: {e}\")\n",
    "else:\n",
    "    print(\"No results to export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e6e3eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
