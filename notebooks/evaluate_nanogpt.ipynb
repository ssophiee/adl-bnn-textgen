{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e684c217",
   "metadata": {},
   "source": [
    "# NanoGPT Model Evaluation\n",
    "\n",
    "This notebook loads a trained NanoGPT model and evaluates it on train.bin and val.bin datasets using multiple metrics:\n",
    "- **Perplexity**: Measures how well the model predicts the next token\n",
    "- **BLEU Score**: Measures similarity between generated and reference text\n",
    "- **ROUGE Scores**: Measures overlap of n-grams between generated and reference text\n",
    "\n",
    "## Usage\n",
    "1. Configure the paths and parameters in the configuration cell\n",
    "2. Run all cells to perform the evaluation\n",
    "3. View results in the final summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8e75dc",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7abcd78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/sofianikolenko/Downloads/Projects_25/ADL/adl-bnn-textgen/notebooks\n"
     ]
    }
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-12 02:56:06 - INFO - Current directory: c:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Configure logging to both file and console\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create formatters\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "file_handler = logging.FileHandler('nanogpt_evaluation.log', mode='a')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "\n",
    "# Add the required paths for importing\n",
    "current_dir = Path.cwd()\n",
    "sys.path.append(str(current_dir / \"baselines/nanogpt/shakespeare-char/models\"))\n",
    "sys.path.append(str(current_dir / \"notebooks\"))\n",
    "\n",
    "print(f\"Current directory: {current_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61fe1329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sofianikolenko/Downloads/Projects_25/ADL/adl-bnn-textgen/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5634e17d",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set your model paths and evaluation parameters here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f51745b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  data_dir: /Users/sofianikolenko/Downloads\n",
      "  model_path: /Users/sofianikolenko/Downloads/Projects_25/ADL/adl-bnn-textgen/checkpoints/baseline/models/baseline_model_2k.pt\n",
      "  meta_path: /Users/sofianikolenko/Downloads/Projects_25/ADL/adl-bnn-textgen/checkpoints/baseline/models/meta.pkl\n",
      "  batch_size: 32\n",
      "  data_dir: nanoGPT/data/shakespeare_char\n",
      "  model_path: ../checkpoints/baseline_nanogpt/baseline_nanogpt.pt\n",
      "  meta_path: ../checkpoints/baseline_nanogpt/nanogpt_meta.pkl\n",
      "  batch_size: 16\n",
      "  max_eval_samples: 1000\n",
      "  device: auto\n",
      "  splits: ['val', 'train']\n",
      "  num_text_samples: 30\n",
      "  prompt_length: 20\n",
      "  generation_length: 30\n",
      "Path exists data_dir: /Users/sofianikolenko/Downloads\n",
      "Path exists model_path: /Users/sofianikolenko/Downloads/Projects_25/ADL/adl-bnn-textgen/checkpoints/baseline/models/baseline_model_2k.pt\n",
      "Path exists meta_path: /Users/sofianikolenko/Downloads/Projects_25/ADL/adl-bnn-textgen/checkpoints/baseline/models/meta.pkl\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'data_dir': \"/Users/sofianikolenko/Downloads\",\n",
    "    'model_path': parent_dir / 'checkpoints/baseline/models/baseline_model_2k.pt',\n",
    "    'meta_path': parent_dir / 'checkpoints/baseline/models/meta.pkl',\n",
    "    # 'data_dir': 'nanoGPT/data/shakespeare',\n",
    "    # 'model_path': '../checkpoints/baseline_token_level_nano/token_level_1500_iter.pt',\n",
    "    # 'meta_path': '../checkpoints/baseline_nanogpt/nanogpt_meta.pkl',\n",
    "    # 'data_dir': \"/Users/sofianikolenko/Downloads\",\n",
    "    # 'model_path': parent_dir / 'baselines/nanogpt/shakespeare-char/models/baseline_model_2k.pt',\n",
    "    # 'meta_path': parent_dir / 'baselines/nanogpt/shakespeare-char/models/meta.pkl',\n",
    "    'data_dir': 'nanoGPT/data/shakespeare_char',\n",
    "    'model_path': '../checkpoints/baseline_nanogpt/baseline_nanogpt.pt',\n",
    "    'meta_path': '../checkpoints/baseline_nanogpt/nanogpt_meta.pkl',\n",
    "\n",
    "    'batch_size': 32,\n",
    "    'max_eval_samples': 1000,\n",
    "    'batch_size': 16,\n",
    "    'max_eval_samples': 1_000,\n",
    "    'device': 'auto',  # 'auto', 'cpu', or 'cuda'\n",
    "    'splits': ['val', 'train'],  # Dataset splits to evaluate\n",
    "    'num_text_samples': 30,  # Number of text samples for BLEU/ROUGE\n",
    "    'prompt_length': 20,  # Length of prompt for text generation\n",
    "    'generation_length': 30,  # Length of generated text,\n",
    "    \"max_tokens\": None # for fast debug, None = all\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Check if paths exist\n",
    "for path_key in ['data_dir', 'model_path', 'meta_path']:\n",
    "    path = Path(CONFIG[path_key])\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Required path not found: {path}\")\n",
    "\n",
    "logger.debug(f\"Configuration: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa5259",
   "metadata": {},
   "source": [
    "<!-- ## 3. Alternative Utility Functions\n",
    "\n",
    "These functions provide fallback implementations if the utils module is not available: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66c2c364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nanogpt_utils import load_model, load_tokenizer, decode"
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from nanogpt_utils import load_model, load_tokenizer, encode, decode\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3765de",
   "metadata": {},
   "source": [
    "## 4. NanoGPT Evaluator Class\n",
    "\n",
    "This class handles model loading and evaluation with multiple metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "execution_count": 38,
   "id": "678f9ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NanoGPTEvaluator class defined\n"
     ]
    }
   ],
   "source": [
    "class NanoGPTEvaluator:\n",
    "    \"\"\"Evaluator for NanoGPT models with multiple metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, meta_path: str, device: str = 'auto'):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the model checkpoint\n",
    "            meta_path: Path to the meta.pkl file containing tokenizer info\n",
    "            device: Device to use ('cpu', 'cuda', or 'auto')\n",
    "        \"\"\"\n",
    "        # Set device\n",
    "        if device == 'auto':\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "\n",
    "        self.model, self.checkpoint = load_model(Path(model_path), self.device)\n",
    "        self.stoi, self.itos = load_tokenizer(Path(meta_path))\n",
    "        \n",
    "        self.vocab_size = len(self.itos)\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Initialize metrics if available\n",
    "        self.metrics = {}\n",
    "        \n",
    "        # Load evaluation metrics from HuggingFace evaluate\n",
    "        self.bleu_metric = evaluate.load(\"bleu\")\n",
    "        self.rouge_metric = evaluate.load(\"rouge\")\n",
    "        self.perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "        print(\"HuggingFace evaluation metrics loaded successfully\")\n",
    "        \n",
    "            \n",
    "            \n",
    "print(\"NanoGPTEvaluator class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "execution_count": 47,
   "id": "f531a251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading methods added to NanoGPTEvaluator\n"
     ]
    }
   ],
   "source": [
    "# Add data loading methods to the evaluator\n",
    "def load_data(self, data_dir: str, split: str = 'val', max_tokens: int = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load train.bin or val.bin data\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing the data files\n",
    "        split: 'train' or 'val'\n",
    "        max_tokens: Optional limit on number of tokens to load (returns first x tokens)\n",
    "        \n",
    "    Returns:\n",
    "        Numpy array of token indices\n",
    "    \"\"\"\n",
    "    filename = f\"{split}.bin\"\n",
    "    filepath = os.path.join(data_dir, filename)\n",
    "    # filepath = \"/Users/sofianikolenko/Downloads/val.bin\"\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Data file not found: {filepath}\")\n",
    "    \n",
    "    if max_tokens is not None:\n",
    "        # Load only the first max_tokens tokens\n",
    "        data = np.memmap(filepath, dtype=np.uint16, mode='r', shape=(max_tokens,))\n",
    "    else:\n",
    "        data = np.memmap(filepath, dtype=np.uint16, mode='r')\n",
    "    print(f\"Loaded {split} data: {len(data):,} tokens\")\n",
    "    return data\n",
    "\n",
    "def get_batch(self, data: np.ndarray, batch_size: int, block_size: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Get a random batch of data for evaluation\n",
    "    \n",
    "    Args:\n",
    "        data: Token data array\n",
    "        batch_size: Number of sequences in the batch\n",
    "        block_size: Length of each sequence\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (input_tokens, target_tokens)\n",
    "    \"\"\"\n",
    "    if len(data) <= block_size:\n",
    "        # If data is smaller than block_size, just use what we have\n",
    "        ix = [0] * batch_size\n",
    "        max_len = len(data) - 1\n",
    "        x = torch.stack([torch.from_numpy(data[0:max_len].astype(np.int64)) for _ in range(batch_size)])\n",
    "        y = torch.stack([torch.from_numpy(data[1:max_len+1].astype(np.int64)) for _ in range(batch_size)])\n",
    "    else:\n",
    "        ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "        x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "        y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    \n",
    "    x, y = x.to(self.device), y.to(self.device)\n",
    "    return x, y\n",
    "\n",
    "# Add methods to the class\n",
    "NanoGPTEvaluator.load_data = load_data\n",
    "NanoGPTEvaluator.get_batch = get_batch\n",
    "\n",
    "print(\"Data loading methods added to NanoGPTEvaluator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab1e0740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity calculation method added\n"
     ]
    }
   ],
   "source": [
    "# Add perplexity calculation method\n",
    "@torch.no_grad()\n",
    "def calculate_perplexity(self, data: np.ndarray, batch_size: int = 16, max_batches: int = 100) -> float:\n",
    "    \"\"\"\n",
    "    Calculate perplexity on the given dataset\n",
    "    \n",
    "    Args:\n",
    "        data: Token data array\n",
    "        batch_size: Batch size for evaluation\n",
    "        max_batches: Maximum number of batches to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        Perplexity score\n",
    "    \"\"\"\n",
    "    print(f\"Calculating perplexity with {batch_size} batch size...\")\n",
    "    \n",
    "    # Use evaluate library for perplexity calculation\n",
    "    if self.perplexity_metric is not None:\n",
    "        try:\n",
    "            # Prepare text for evaluate library\n",
    "            text_samples = []\n",
    "            block_size = min(self.model.config.block_size, 512)  # Limit block size for memory\n",
    "            num_samples = min(max_batches * batch_size, len(data) // block_size)\n",
    "            \n",
    "            print(f\"Preparing {num_samples} text samples for perplexity calculation...\")\n",
    "            \n",
    "            for i in range(0, num_samples * block_size, block_size):\n",
    "                if i + block_size < len(data):\n",
    "                    tokens = data[i:i+block_size].astype(np.int64)\n",
    "                    text = decode(tokens.tolist(), self.itos)\n",
    "                    # Clean up text and ensure it's not empty\n",
    "                    text = text.strip()\n",
    "                    if len(text) > 0:\n",
    "                        text_samples.append(text)\n",
    "            \n",
    "            if text_samples:\n",
    "                print(f\"Computing perplexity for {len(text_samples)} text samples...\")\n",
    "                # Use evaluate library with proper model specification\n",
    "                result = self.perplexity_metric.compute(\n",
    "                    predictions=text_samples, \n",
    "                    model_id=\"gpt2\",  # Use GPT-2 as reference model\n",
    "                    add_start_token=False\n",
    "                )\n",
    "                return result['mean_perplexity']\n",
    "            else:\n",
    "                print(\"No valid text samples generated\")\n",
    "                return float('inf')\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with evaluate library perplexity: {e}\")\n",
    "            print(\"Could not calculate perplexity using evaluate library\")\n",
    "            return float('inf')\n",
    "    else:\n",
    "        print(\"Perplexity metric not available\")\n",
    "        return float('inf')\n",
    "\n",
    "# Add method to the class\n",
    "NanoGPTEvaluator.calculate_perplexity = calculate_perplexity\n",
    "\n",
    "print(\"Perplexity calculation method added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "execution_count": 48,
   "id": "b1596f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generation method added\n"
     ]
    }
   ],
   "source": [
    "# Add text generation and metric calculation methods\n",
    "def generate_samples_for_metrics(self, data: np.ndarray, num_samples: int = 50, \n",
    "                               prompt_length: int = 20, generation_length: int = 30) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Generate text samples for BLEU/ROUGE evaluation\n",
    "    \n",
    "    Args:\n",
    "        data: Token data array\n",
    "        num_samples: Number of samples to generate\n",
    "        prompt_length: Length of prompt in tokens\n",
    "        generation_length: Length of generated text in tokens\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (references, predictions)\n",
    "    \"\"\"\n",
    "    print(f\"Generating {num_samples} samples for BLEU/ROUGE evaluation...\")\n",
    "    \n",
    "    references = []\n",
    "    predictions = []\n",
    "    \n",
    "    # Limit samples based on data size\n",
    "    max_possible_samples = max(1, (len(data) - prompt_length - generation_length) // 100)\n",
    "    num_samples = min(num_samples, max_possible_samples)\n",
    "    \n",
    "    print(f\"Generating {num_samples} text samples...\")\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        try:\n",
    "            # Select a random starting position\n",
    "            if len(data) > prompt_length + generation_length + 10:\n",
    "                start_idx = np.random.randint(0, len(data) - prompt_length - generation_length - 10)\n",
    "            else:\n",
    "                start_idx = 0\n",
    "            \n",
    "            # Extract prompt and reference\n",
    "            prompt_tokens = data[start_idx:start_idx + prompt_length].astype(np.int64)\n",
    "            reference_tokens = data[start_idx + prompt_length:start_idx + prompt_length + generation_length].astype(np.int64)\n",
    "            \n",
    "            # Decode reference\n",
    "            reference_text = decode(reference_tokens.tolist(), self.itos)\n",
    "            \n",
    "            # Generate prediction\n",
    "            x = torch.tensor(prompt_tokens, dtype=torch.long, device=self.device)[None, ...]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                generated_tokens = []\n",
    "                for _ in range(generation_length):\n",
    "                    # Crop if sequence gets too long\n",
    "                    x_cond = x if x.size(1) <= self.model.config.block_size else x[:, -self.model.config.block_size:]\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    logits, _ = self.model(x_cond)\n",
    "                    logits = logits[:, -1, :] / 0.8  # temperature\n",
    "                    \n",
    "                    # Sample next token\n",
    "                    probs = F.softmax(logits, dim=-1)\n",
    "                    next_token = torch.multinomial(probs, num_samples=1)\n",
    "                    generated_tokens.append(next_token.item())\n",
    "                    \n",
    "                    # Append to sequence\n",
    "                    x = torch.cat((x, next_token), dim=1)\n",
    "            \n",
    "            # Decode prediction\n",
    "            prediction_text = decode(generated_tokens, self.itos)\n",
    "            \n",
    "            # Clean up texts\n",
    "            reference_text = reference_text.strip()\n",
    "            prediction_text = prediction_text.strip()\n",
    "            \n",
    "            if len(reference_text) > 0 and len(prediction_text) > 0:\n",
    "                references.append(reference_text)\n",
    "                predictions.append(prediction_text)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  Generated {i + 1}/{num_samples} samples\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating sample {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Successfully generated {len(references)} sample pairs\")\n",
    "    return references, predictions\n",
    "\n",
    "# Add method to the class\n",
    "NanoGPTEvaluator.generate_samples_for_metrics = generate_samples_for_metrics\n",
    "\n",
    "print(\"Text generation method added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec4cebbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom tokenizer method added to NanoGPTEvaluator\n"
     ]
    }
   ],
   "source": [
    "# Add custom tokenizer method for evaluation metrics\n",
    "def get_tokenizer(self):\n",
    "    \"\"\"\n",
    "    Get a tokenizer function that matches the model's vocabulary\n",
    "    \n",
    "    This works for both character-level and token-level models:\n",
    "    - Character-level: stoi maps each character to an index\n",
    "    - Token-level: stoi maps tokens/words to indices\n",
    "    \n",
    "    Returns:\n",
    "        Tokenizer function that splits text according to model's vocabulary\n",
    "    \"\"\"\n",
    "    def custom_tokenizer(text):\n",
    "        \"\"\"Tokenizer that matches the model's vocabulary\"\"\"\n",
    "        tokens = []\n",
    "        for char in text:\n",
    "            if char in self.stoi:\n",
    "                tokens.append(char)\n",
    "        return tokens\n",
    "    \n",
    "    return custom_tokenizer\n",
    "\n",
    "# Add method to the class\n",
    "NanoGPTEvaluator.get_tokenizer = get_tokenizer\n",
    "\n",
    "print(\"Custom tokenizer method added to NanoGPTEvaluator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "44f9fe6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed perplexity calculation method - using only HuggingFace evaluate library\n"
     ]
    }
   ],
   "source": [
    "# Fix the perplexity calculation method\n",
    "def calculate_perplexity(self, data: np.ndarray, batch_size: int = 16, max_batches: int = 100) -> float:\n",
    "    \"\"\"\n",
    "    Calculate perplexity using HuggingFace evaluate library only\n",
    "    \n",
    "    Args:\n",
    "        data: Token data array\n",
    "        batch_size: Batch size for evaluation\n",
    "        max_batches: Maximum number of batches to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        Perplexity value or None if calculation fails\n",
    "    \"\"\"\n",
    "    print(f\"Calculating perplexity with {batch_size} batch size...\")\n",
    "    \n",
    "    if self.perplexity_metric is not None:\n",
    "        try:\n",
    "            # Prepare data for HuggingFace perplexity metric\n",
    "            # Convert tokens to text for the metric\n",
    "            \n",
    "            # Calculate number of batches\n",
    "            seq_len = 256  # Standard sequence length\n",
    "            max_start = len(data) - seq_len\n",
    "            \n",
    "            if max_start <= 0:\n",
    "                print(\"Dataset too small for perplexity calculation\")\n",
    "                return None\n",
    "                \n",
    "            # Limit the number of batches\n",
    "            num_batches = min(max_batches, max_start // batch_size)\n",
    "            \n",
    "            # Collect text samples for perplexity calculation\n",
    "            texts = []\n",
    "            for i in range(num_batches * batch_size):\n",
    "                start_idx = i * (max_start // (num_batches * batch_size))\n",
    "                if start_idx + seq_len <= len(data):\n",
    "                    tokens = data[start_idx:start_idx + seq_len].astype(np.int64)\n",
    "                    text = decode(tokens.tolist(), self.itos)\n",
    "                    if len(text.strip()) > 0:\n",
    "                        texts.append(text)\n",
    "            \n",
    "            if len(texts) == 0:\n",
    "                print(\"No valid texts extracted for perplexity calculation\")\n",
    "                return None\n",
    "                \n",
    "            print(f\"Computing perplexity for {len(texts)} text samples...\")\n",
    "            \n",
    "            # Use HuggingFace evaluate perplexity metric\n",
    "            result = self.perplexity_metric.compute(\n",
    "                predictions=texts,\n",
    "                model_id=\"gpt2\"  # Use a standard reference model\n",
    "            )\n",
    "            perplexity_value = result.get('mean_perplexity', None)\n",
    "            if perplexity_value is not None:\n",
    "                print(f\"Perplexity: {perplexity_value:.4f}\")\n",
    "                return float(perplexity_value)\n",
    "            else:\n",
    "                print(\"Perplexity calculation returned None\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with evaluate library perplexity: {e} | {traceback.format_exc()}\")\n",
    "            print(\"Perplexity calculation failed - returning None\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Perplexity metric not available\")\n",
    "        return None\n",
    "\n",
    "# Update the method in the class\n",
    "NanoGPTEvaluator.calculate_perplexity = calculate_perplexity\n",
    "\n",
    "print(\"Fixed perplexity calculation method - using only HuggingFace evaluate library\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e9217fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU and ROUGE score calculation methods added\n"
     ]
    }
   ],
   "source": [
    "# Add BLEU and ROUGE score calculation methods\n",
    "def calculate_bleu_score(self, references: List[str], predictions: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate BLEU score using HuggingFace evaluate library with custom tokenization\n",
    "    \n",
    "    Args:\n",
    "        references: List of reference texts\n",
    "        predictions: List of predicted texts\n",
    "        \n",
    "    Returns:\n",
    "        BLEU score (0-1 range)\n",
    "    \"\"\"\n",
    "    if self.bleu_metric is None:\n",
    "        raise Exception(\"BLEU metric not available\")\n",
    "    \n",
    "    try:\n",
    "        # Get tokenizer that matches the model's vocabulary\n",
    "        custom_tokenizer = self.get_tokenizer()\n",
    "        \n",
    "        # HuggingFace BLEU expects references as list of lists\n",
    "        formatted_references = [[ref] for ref in references]\n",
    "        \n",
    "        # Use custom tokenizer for BLEU calculation\n",
    "        result = self.bleu_metric.compute(\n",
    "            predictions=predictions,\n",
    "            references=formatted_references,\n",
    "            tokenizer=custom_tokenizer\n",
    "        )\n",
    "        \n",
    "        bleu_score = result.get('bleu', 0.0)\n",
    "        print(f\"  BLEU details: {result}\")\n",
    "        return float(bleu_score)\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error calculating BLEU score: {e} | {traceback.format_exc()}\")\n",
    "\n",
    "def calculate_rouge_score(self, references: List[str], predictions: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate ROUGE scores using HuggingFace evaluate library with custom tokenization\n",
    "    \n",
    "    Args:\n",
    "        references: List of reference texts\n",
    "        predictions: List of predicted texts\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with ROUGE-1, ROUGE-2, and ROUGE-L scores\n",
    "    \"\"\"\n",
    "    if self.rouge_metric is None:\n",
    "        raise ValueError(\"ROUGE metric not available\")\n",
    "    \n",
    "    try:\n",
    "        # Get tokenizer that matches the model's vocabulary\n",
    "        custom_tokenizer = self.get_tokenizer()\n",
    "        \n",
    "        # Use custom tokenizer for ROUGE calculation\n",
    "        result = self.rouge_metric.compute(\n",
    "            predictions=predictions,\n",
    "            references=references,\n",
    "            tokenizer=custom_tokenizer\n",
    "        )\n",
    "        \n",
    "        # Extract F1 scores for each ROUGE variant\n",
    "        rouge_scores = {\n",
    "            'rouge1': float(result.get('rouge1', 0.0)),\n",
    "            'rouge2': float(result.get('rouge2', 0.0)),\n",
    "            'rougeL': float(result.get('rougeL', 0.0))\n",
    "        }\n",
    "        \n",
    "        print(f\"  ROUGE details: {result}\")\n",
    "        return rouge_scores\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error calculating ROUGE scores: {e} | {traceback.format_exc()}\"\n",
    "        \n",
    "        raise Exception(error_msg)\n",
    "\n",
    "# Add methods to the class\n",
    "NanoGPTEvaluator.calculate_bleu_score = calculate_bleu_score\n",
    "NanoGPTEvaluator.calculate_rouge_score = calculate_rouge_score\n",
    "\n",
    "print(\"BLEU and ROUGE score calculation methods added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b239b32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed main evaluation method - properly handles None perplexity values\n"
     ]
    }
   ],
   "source": [
    "def evaluate_dataset(self, data_dir: str, split: str = 'val', batch_size: int = 16, \n",
    "                    max_eval_samples: int = 1000, num_text_samples: int = 50,\n",
    "                    prompt_length: int = 20, generation_length: int = 30, max_tokens = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset split with proper None handling\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing train.bin and val.bin\n",
    "        split: 'train' or 'val'\n",
    "        batch_size: Batch size for evaluation\n",
    "        max_eval_samples: Maximum number of samples for evaluation\n",
    "        num_text_samples: Number of text samples for BLEU/ROUGE\n",
    "        prompt_length: Length of prompt for text generation\n",
    "        generation_length: Length of generated text\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Evaluating on {split} set\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Load data\n",
    "    data = self.load_data(data_dir, split, max_tokens)\n",
    "    \n",
    "    results = {'split': split, 'total_tokens': len(data)}\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    print(\"\\n1. Calculating Perplexity...\")\n",
    "    start_time = time.time()\n",
    "    perplexity = self.calculate_perplexity(data, batch_size, max_batches=min(100, max_eval_samples//batch_size))\n",
    "    results['perplexity'] = perplexity if perplexity is not None else 0.0\n",
    "    \n",
    "    # Handle None perplexity properly\n",
    "    if perplexity is not None:\n",
    "        print(f\"Perplexity: {perplexity:.4f} (took {time.time() - start_time:.2f}s)\")\n",
    "    else:\n",
    "        print(f\"Perplexity: Failed to calculate (took {time.time() - start_time:.2f}s)\")\n",
    "    \n",
    "    # Generate samples and calculate BLEU/ROUGE\n",
    "    if len(data) > 100:  # Only if we have enough data\n",
    "        print(\"\\n2. Generating samples for BLEU/ROUGE evaluation...\")\n",
    "        start_time = time.time()\n",
    "        num_samples = min(num_text_samples, max_eval_samples//20, len(data)//100)  # Reasonable number of samples\n",
    "        references, predictions = self.generate_samples_for_metrics(\n",
    "            data, num_samples, prompt_length, generation_length\n",
    "        )\n",
    "        \n",
    "        if references and predictions:\n",
    "            print(\"\\n3. Calculating BLEU score...\")\n",
    "            bleu_score = self.calculate_bleu_score(references, predictions)\n",
    "            results['bleu'] = bleu_score\n",
    "            print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "            \n",
    "            print(\"\\n4. Calculating ROUGE scores...\")\n",
    "            rouge_scores = self.calculate_rouge_score(references, predictions)\n",
    "            results.update(rouge_scores)\n",
    "            print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "            print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "            print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "            \n",
    "            # Show some example generations\n",
    "            print(\"\\n5. Example generations:\")\n",
    "            for i in range(min(3, len(references))):\n",
    "                print(f\"\\nExample {i+1}:\")\n",
    "                print(f\"Reference: {references[i][:100]}...\")\n",
    "                print(f\"Generated: {predictions[i][:100]}...\")\n",
    "        else:\n",
    "            raise Exception(\"Could not generate samples for BLEU/ROUGE evaluation\")\n",
    "    else:\n",
    "        raise Exception(\"Dataset too small for text generation evaluation\")\n",
    "    \n",
    "    print(f\"\\nEvaluation completed in {time.time() - start_time:.2f}s\")\n",
    "    return results\n",
    "\n",
    "# Replace the method in the class\n",
    "NanoGPTEvaluator.evaluate_dataset = evaluate_dataset\n",
    "\n",
    "print(\"Fixed main evaluation method - properly handles None perplexity values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab6659b",
   "metadata": {},
   "source": [
    "## 5. Initialize the Evaluator\n",
    "\n",
    "Load the model and initialize the evaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7f77bf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing NanoGPT Evaluator...\n",
      "==================================================\n",
      "Model: /Users/sofianikolenko/Downloads/Projects_25/ADL/adl-bnn-textgen/checkpoints/baseline/models/baseline_model_2k.pt\n",
      "Data: /Users/sofianikolenko/Downloads\n",
      "Meta: /Users/sofianikolenko/Downloads/Projects_25/ADL/adl-bnn-textgen/checkpoints/baseline/models/meta.pkl\n",
      "Batch size: 32\n",
      "Max eval samples: 1000\n",
      "Splits: ['val', 'train']\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model arguments: {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'block_size': 256, 'bias': False, 'vocab_size': 65, 'dropout': 0.2}\n",
      "number of parameters: 10.65M\n",
      "Model loaded successfully!\n",
      "Number of parameters: 10,745,088\n",
      "HuggingFace evaluation metrics loaded successfully\n",
      "\n",
      "Evaluator initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize evaluator\n",
    "print(\"Initializing NanoGPT Evaluator...\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model: {CONFIG['model_path']}\")\n",
    "print(f\"Data: {CONFIG['data_dir']} | {os.listdir(CONFIG['data_dir'])}\")\n",
    "print(f\"Meta: {CONFIG['meta_path']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Max eval samples: {CONFIG['max_eval_samples']}\")\n",
    "print(f\"Splits: {CONFIG['splits']}\")\n",
    "\n",
    "try:\n",
    "    evaluator = NanoGPTEvaluator(\n",
    "        CONFIG['model_path'], \n",
    "        CONFIG['meta_path'], \n",
    "        CONFIG['device'],\n",
    "    )\n",
    "    print(\"\\nEvaluator initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing evaluator: {e}\")\n",
    "    evaluator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c039b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1d4ad1a",
   "metadata": {},
   "source": [
    "## 6. Run Evaluation\n",
    "\n",
    "Evaluate the model on the specified dataset splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3df396e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating val split...\n",
      "\n",
      "==================================================\n",
      "Evaluating on val set\n",
      "==================================================\n",
      "Loaded val data: 111,540 tokens\n",
      "\n",
      "1. Calculating Perplexity...\n",
      "Calculating perplexity with 32 batch size...\n",
      "Computing perplexity for 992 text samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [09:16<00:00,  8.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 148.9178\n",
      "Perplexity: 148.9178 (took 559.82s)\n",
      "\n",
      "2. Generating samples for BLEU/ROUGE evaluation...\n",
      "Generating 50 samples for BLEU/ROUGE evaluation...\n",
      "Generating 50 text samples...\n",
      "  Generated 10/50 samples\n",
      "  Generated 20/50 samples\n",
      "  Generated 30/50 samples\n",
      "  Generated 40/50 samples\n",
      "  Generated 50/50 samples\n",
      "Successfully generated 50 sample pairs\n",
      "\n",
      "3. Calculating BLEU score...\n",
      "Error evaluating val split: 'NanoGPTEvaluator' object has no attribute 'calculate_bleu_score'\n",
      "\n",
      "Evaluating train split...\n",
      "\n",
      "==================================================\n",
      "Evaluating on train set\n",
      "==================================================\n",
      "Loaded train data: 1,003,854 tokens\n",
      "\n",
      "1. Calculating Perplexity...\n",
      "Calculating perplexity with 32 batch size...\n",
      "Computing perplexity for 992 text samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [07:03<00:00,  6.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 167.2480\n",
      "Perplexity: 167.2480 (took 426.61s)\n",
      "\n",
      "2. Generating samples for BLEU/ROUGE evaluation...\n",
      "Generating 50 samples for BLEU/ROUGE evaluation...\n",
      "Generating 50 text samples...\n",
      "  Generated 10/50 samples\n",
      "  Generated 20/50 samples\n",
      "  Generated 30/50 samples\n",
      "  Generated 40/50 samples\n",
      "  Generated 50/50 samples\n",
      "Successfully generated 50 sample pairs\n",
      "\n",
      "3. Calculating BLEU score...\n",
      "Error evaluating train split: 'NanoGPTEvaluator' object has no attribute 'calculate_bleu_score'\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation on all specified splits\n",
    "if evaluator is not None:\n",
    "    all_results = {}\n",
    "    \n",
    "    for split in CONFIG['splits']:#[CONFIG['splits'][0]]:\n",
    "        print(f\"\\nEvaluating {split} split...\")\n",
    "        try:\n",
    "            results = evaluator.evaluate_dataset(\n",
    "                CONFIG['data_dir'], \n",
    "                split, \n",
    "                CONFIG['batch_size'], \n",
    "                CONFIG['max_eval_samples'],\n",
    "                CONFIG['num_text_samples'],\n",
    "                CONFIG['prompt_length'],\n",
    "                CONFIG['generation_length'],\n",
    "                max_tokens=CONFIG['max_tokens']\n",
    "            )\n",
    "            all_results[split] = results\n",
    "            print(f\"{split} evaluation completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {split} split: {e}\")\n",
    "            continue\n",
    "else:\n",
    "    print(\"Cannot run evaluation - evaluator not initialized\")\n",
    "    all_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f355c",
   "metadata": {},
   "source": [
    "## 7. Results Summary\n",
    "\n",
    "Display a comprehensive summary of all evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abf50c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION SUMMARY\n",
      "============================================================\n",
      "Split Total Tokens Perplexity   BLEU ROUGE-1 ROUGE-2 ROUGE-L\n",
      "  VAL      111,540     0.0000 0.0775  0.4879  0.1386  0.3138\n",
      "TRAIN    1,003,854     0.0000 0.0602  0.5093  0.1217  0.3126\n",
      "\n",
      "Evaluation completed successfully!\n",
      "\n",
      "Results stored in 'evaluation_results' variable for further analysis\n"
     ]
    }
   ],
   "source": [
    "# Print comprehensive summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if all_results:\n",
    "    # Create a summary table\n",
    "    import pandas as pd\n",
    "    \n",
    "    summary_data = []\n",
    "    for split, results in all_results.items():\n",
    "        summary_data.append({\n",
    "            'Split': split.upper(),\n",
    "            'Total Tokens': f\"{results.get('total_tokens', 0):,}\",\n",
    "            'Perplexity': f\"{results.get('perplexity', 0):.4f}\",\n",
    "            'BLEU': f\"{results.get('bleu', 0):.4f}\",\n",
    "            'ROUGE-1': f\"{results.get('rouge1', 0):.4f}\",\n",
    "            'ROUGE-2': f\"{results.get('rouge2', 0):.4f}\",\n",
    "            'ROUGE-L': f\"{results.get('rougeL', 0):.4f}\"\n",
    "        })\n",
    "    \n",
    "    try:\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        print(df.to_string(index=False))\n",
    "    except:\n",
    "        # Fallback if pandas is not available\n",
    "        for split, results in all_results.items():\n",
    "            print(f\"\\n{split.upper()} SET:\")\n",
    "            print(f\"  Total tokens: {results.get('total_tokens', 0):,}\")\n",
    "            print(f\"  Perplexity:   {results.get('perplexity', 0):.4f}\")\n",
    "            print(f\"  BLEU:         {results.get('bleu', 0):.4f}\")\n",
    "            print(f\"  ROUGE-1:      {results.get('rouge1', 0):.4f}\")\n",
    "            print(f\"  ROUGE-2:      {results.get('rouge2', 0):.4f}\")\n",
    "            print(f\"  ROUGE-L:      {results.get('rougeL', 0):.4f}\")\n",
    "    \n",
    "    print(f\"\\nEvaluation completed successfully!\")\n",
    "    \n",
    "    # Store results for further analysis\n",
    "    evaluation_results = all_results\n",
    "    print(f\"\\nResults stored in 'evaluation_results' variable for further analysis\")\n",
    "else:\n",
    "    print(\"No evaluation results to display\")\n",
    "    evaluation_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bb8bb1",
   "metadata": {},
   "source": [
    "## 8. Additional Analysis (Optional)\n",
    "\n",
    "You can use this cell for additional analysis of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb8cf385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional Analysis:\n",
      "==============================\n",
      "\n",
      "Perplexity Comparison:\n",
      "  Training:   0.0000\n",
      "  Validation: 0.0000\n",
      "\n",
      "Text Generation Quality (val):\n",
      "  BLEU 0.0775: Low text similarity\n",
      "  ROUGE-1 0.4879: Good word overlap\n",
      "\n",
      "Text Generation Quality (train):\n",
      "  BLEU 0.0602: Low text similarity\n",
      "  ROUGE-1 0.5093: Good word overlap\n"
     ]
    }
   ],
   "source": [
    "# Additional analysis cell - customize as needed\n",
    "\n",
    "if evaluation_results:\n",
    "    print(\"Additional Analysis:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Compare train vs validation performance\n",
    "    if 'train' in evaluation_results and 'val' in evaluation_results:\n",
    "        train_ppl = evaluation_results['train'].get('perplexity', 0)\n",
    "        val_ppl = evaluation_results['val'].get('perplexity', 0)\n",
    "        \n",
    "        print(f\"\\nPerplexity Comparison:\")\n",
    "        print(f\"  Training:   {train_ppl:.4f}\")\n",
    "        print(f\"  Validation: {val_ppl:.4f}\")\n",
    "        \n",
    "        if train_ppl > 0 and val_ppl > 0:\n",
    "            ratio = val_ppl / train_ppl\n",
    "            print(f\"  Val/Train ratio: {ratio:.4f}\")\n",
    "            \n",
    "            if ratio > 1.5:\n",
    "                print(\"  High validation perplexity suggests overfitting\")\n",
    "            elif ratio < 1.1:\n",
    "                print(f\"  Good generalization - low overfitting\")\n",
    "            else:\n",
    "                print(f\"  Moderate generalization gap\")\n",
    "    \n",
    "    # Text generation quality assessment\n",
    "    for split in evaluation_results:\n",
    "        results = evaluation_results[split]\n",
    "        bleu = results.get('bleu', 0)\n",
    "        rouge1 = results.get('rouge1', 0)\n",
    "        \n",
    "        print(f\"\\nText Generation Quality ({split}):\")\n",
    "        if bleu > 0.3:\n",
    "            print(f\"  BLEU {bleu:.4f}: Good text similarity\")\n",
    "        elif bleu > 0.1:\n",
    "            print(f\"  BLEU {bleu:.4f}: Moderate text similarity\")\n",
    "        else:\n",
    "            print(f\"  BLEU {bleu:.4f}: Low text similarity\")\n",
    "        \n",
    "        if rouge1 > 0.3:\n",
    "            print(f\"  ROUGE-1 {rouge1:.4f}: Good word overlap\")\n",
    "        elif rouge1 > 0.15:\n",
    "            print(f\"  ROUGE-1 {rouge1:.4f}: Moderate word overlap\")\n",
    "        else:\n",
    "            print(f\"  ROUGE-1 {rouge1:.4f}: Low word overlap\")\n",
    "else:\n",
    "    print(\"No results available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3551d6",
   "metadata": {},
   "source": [
    "## 9. Export Results (Optional)\n",
    "\n",
    "Save the evaluation results to a file for later analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac77e559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results exported to: evaluation_results_20250919_144940.json\n"
     ]
    }
   ],
   "source": [
    "# Export results to JSON file\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "if evaluation_results:\n",
    "    # Add metadata\n",
    "    export_data = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'config': CONFIG,\n",
    "        'results': evaluation_results,\n",
    "        'model_info': {\n",
    "            'model_path': CONFIG['model_path'],\n",
    "            'meta_path': CONFIG['meta_path'],\n",
    "            'vocab_size': evaluator.vocab_size if evaluator else None,\n",
    "            'device': evaluator.device if evaluator else None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    output_file = f\"evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    \n",
    "    try:\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(export_data, f, indent=2)\n",
    "        print(f\"Results exported to: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting results: {e}\")\n",
    "else:\n",
    "    print(\"No results to export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e6e3eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
