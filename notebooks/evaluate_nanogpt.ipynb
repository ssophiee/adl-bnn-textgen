{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e684c217",
   "metadata": {},
   "source": [
    "# NanoGPT Model Evaluation\n",
    "\n",
    "This notebook loads a trained NanoGPT model and evaluates it on train.bin and val.bin datasets using multiple metrics:\n",
    "- **Perplexity**: Measures how well the model predicts the next token\n",
    "- **BLEU Score**: Measures similarity between generated and reference text\n",
    "- **ROUGE Scores**: Measures overlap of n-grams between generated and reference text\n",
    "\n",
    "## Usage\n",
    "1. Configure the paths and parameters in the configuration cell\n",
    "2. Run all cells to perform the evaluation\n",
    "3. View results in the final summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8e75dc",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7abcd78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-12 02:56:06 - INFO - Current directory: c:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Configure logging to both file and console\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create formatters\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "file_handler = logging.FileHandler('nanogpt_evaluation.log', mode='a')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "\n",
    "# Add the required paths for importing\n",
    "current_dir = Path.cwd()\n",
    "# sys.path.append(str(current_dir / \"baselines/nanogpt/shakespeare-char/models\"))\n",
    "# sys.path.append(str(current_dir / \"notebooks\"))\n",
    "\n",
    "logging.info(f\"Current directory: {current_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61fe1329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\bnn\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5634e17d",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set your model paths and evaluation parameters here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f51745b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  data_dir: nanoGPT/data/shakespeare_char\n",
      "  model_path: ../checkpoints/baseline_nanogpt/baseline_nanogpt.pt\n",
      "  meta_path: ../checkpoints/baseline_nanogpt/nanogpt_meta.pkl\n",
      "  batch_size: 16\n",
      "  max_eval_samples: 1000\n",
      "  device: auto\n",
      "  splits: ['val', 'train']\n",
      "  num_text_samples: 30\n",
      "  prompt_length: 20\n",
      "  generation_length: 30\n",
      "  max_tokens: None\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # 'data_dir': \"/Users/sofianikolenko/Downloads\",\n",
    "    # 'model_path': parent_dir / 'baselines/nanogpt/shakespeare-char/models/baseline_model_2k.pt',\n",
    "    # 'meta_path': parent_dir / 'baselines/nanogpt/shakespeare-char/models/meta.pkl',\n",
    "    'data_dir': 'nanoGPT/data/shakespeare_char',\n",
    "    'model_path': '../checkpoints/baseline_nanogpt/baseline_nanogpt.pt',\n",
    "    'meta_path': '../checkpoints/baseline_nanogpt/nanogpt_meta.pkl',\n",
    "\n",
    "    'batch_size': 16,\n",
    "    'max_eval_samples': 1_000,\n",
    "    'device': 'auto',  # 'auto', 'cpu', or 'cuda'\n",
    "    'splits': ['val', 'train'],  # Dataset splits to evaluate\n",
    "    'num_text_samples': 30,  # Number of text samples for BLEU/ROUGE\n",
    "    'prompt_length': 20,  # Length of prompt for text generation\n",
    "    'generation_length': 30,  # Length of generated text,\n",
    "    \"max_tokens\": None # for fast debug, None = all\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Check if paths exist\n",
    "for path_key in ['data_dir', 'model_path', 'meta_path']:\n",
    "    path = Path(CONFIG[path_key])\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Required path not found: {path}\")\n",
    "\n",
    "logger.debug(f\"Configuration: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa5259",
   "metadata": {},
   "source": [
    "<!-- ## 3. Alternative Utility Functions\n",
    "\n",
    "These functions provide fallback implementations if the utils module is not available: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66c2c364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from nanogpt_utils import load_model, load_tokenizer, encode, decode\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3765de",
   "metadata": {},
   "source": [
    "## 4. NanoGPT Evaluator Class\n",
    "\n",
    "This class handles model loading and evaluation with multiple metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "678f9ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NanoGPTEvaluator class defined\n"
     ]
    }
   ],
   "source": [
    "class NanoGPTEvaluator:\n",
    "    \"\"\"Evaluator for NanoGPT models with multiple metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, meta_path: str, device: str = 'auto'):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the model checkpoint\n",
    "            meta_path: Path to the meta.pkl file containing tokenizer info\n",
    "            device: Device to use ('cpu', 'cuda', or 'auto')\n",
    "        \"\"\"\n",
    "        # Set device\n",
    "        if device == 'auto':\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "\n",
    "        self.model, self.checkpoint = load_model(Path(model_path), self.device)\n",
    "        self.stoi, self.itos = load_tokenizer(Path(meta_path))\n",
    "        \n",
    "        self.vocab_size = len(self.itos)\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Initialize metrics if available\n",
    "        self.metrics = {}\n",
    "        \n",
    "        # Load evaluation metrics from HuggingFace evaluate\n",
    "        self.bleu_metric = evaluate.load(\"bleu\")\n",
    "        self.rouge_metric = evaluate.load(\"rouge\")\n",
    "        self.perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "        print(\"HuggingFace evaluation metrics loaded successfully\")\n",
    "        \n",
    "            \n",
    "            \n",
    "print(\"NanoGPTEvaluator class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f531a251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading methods added to NanoGPTEvaluator\n"
     ]
    }
   ],
   "source": [
    "# Add data loading methods to the evaluator\n",
    "def load_data(self, data_dir: str, split: str = 'val', max_tokens: int = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load train.bin or val.bin data\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing the data files\n",
    "        split: 'train' or 'val'\n",
    "        max_tokens: Optional limit on number of tokens to load (returns first x tokens)\n",
    "        \n",
    "    Returns:\n",
    "        Numpy array of token indices\n",
    "    \"\"\"\n",
    "    filename = f\"{split}.bin\"\n",
    "    filepath = os.path.join(data_dir, filename)\n",
    "    # filepath = \"/Users/sofianikolenko/Downloads/val.bin\"\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Data file not found: {filepath}\")\n",
    "    \n",
    "    if max_tokens is not None:\n",
    "        # Load only the first max_tokens tokens\n",
    "        data = np.memmap(filepath, dtype=np.uint16, mode='r', shape=(max_tokens,))\n",
    "    else:\n",
    "        data = np.memmap(filepath, dtype=np.uint16, mode='r')\n",
    "    print(f\"Loaded {split} data: {len(data):,} tokens\")\n",
    "    return data\n",
    "\n",
    "def get_batch(self, data: np.ndarray, batch_size: int, block_size: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Get a random batch of data for evaluation\n",
    "    \n",
    "    Args:\n",
    "        data: Token data array\n",
    "        batch_size: Number of sequences in the batch\n",
    "        block_size: Length of each sequence\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (input_tokens, target_tokens)\n",
    "    \"\"\"\n",
    "    if len(data) <= block_size:\n",
    "        # If data is smaller than block_size, just use what we have\n",
    "        ix = [0] * batch_size\n",
    "        max_len = len(data) - 1\n",
    "        x = torch.stack([torch.from_numpy(data[0:max_len].astype(np.int64)) for _ in range(batch_size)])\n",
    "        y = torch.stack([torch.from_numpy(data[1:max_len+1].astype(np.int64)) for _ in range(batch_size)])\n",
    "    else:\n",
    "        ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "        x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "        y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    \n",
    "    x, y = x.to(self.device), y.to(self.device)\n",
    "    return x, y\n",
    "\n",
    "# Add methods to the class\n",
    "NanoGPTEvaluator.load_data = load_data\n",
    "NanoGPTEvaluator.get_batch = get_batch\n",
    "\n",
    "print(\"Data loading methods added to NanoGPTEvaluator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1596f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generation method added\n"
     ]
    }
   ],
   "source": [
    "# Add text generation and metric calculation methods\n",
    "def generate_samples_for_metrics(self, data: np.ndarray, num_samples: int = 50, \n",
    "                               prompt_length: int = 20, generation_length: int = 30) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Generate text samples for BLEU/ROUGE evaluation\n",
    "    \n",
    "    Args:\n",
    "        data: Token data array\n",
    "        num_samples: Number of samples to generate\n",
    "        prompt_length: Length of prompt in tokens\n",
    "        generation_length: Length of generated text in tokens\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (references, predictions)\n",
    "    \"\"\"\n",
    "    print(f\"Generating {num_samples} samples for BLEU/ROUGE evaluation...\")\n",
    "    \n",
    "    references = []\n",
    "    predictions = []\n",
    "    \n",
    "    # Limit samples based on data size\n",
    "    max_possible_samples = max(1, (len(data) - prompt_length - generation_length) // 100)\n",
    "    num_samples = min(num_samples, max_possible_samples)\n",
    "    \n",
    "    print(f\"Generating {num_samples} text samples...\")\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        try:\n",
    "            # Select a random starting position\n",
    "            if len(data) > prompt_length + generation_length + 10:\n",
    "                start_idx = np.random.randint(0, len(data) - prompt_length - generation_length - 10)\n",
    "            else:\n",
    "                start_idx = 0\n",
    "            \n",
    "            # Extract prompt and reference\n",
    "            prompt_tokens = data[start_idx:start_idx + prompt_length].astype(np.int64)\n",
    "            reference_tokens = data[start_idx + prompt_length:start_idx + prompt_length + generation_length].astype(np.int64)\n",
    "            \n",
    "            # Decode reference\n",
    "            reference_text = decode(reference_tokens.tolist(), self.itos)\n",
    "            \n",
    "            # Generate prediction\n",
    "            x = torch.tensor(prompt_tokens, dtype=torch.long, device=self.device)[None, ...]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                generated_tokens = []\n",
    "                for _ in range(generation_length):\n",
    "                    # Crop if sequence gets too long\n",
    "                    x_cond = x if x.size(1) <= self.model.config.block_size else x[:, -self.model.config.block_size:]\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    logits, _ = self.model(x_cond)\n",
    "                    logits = logits[:, -1, :] / 0.8  # temperature\n",
    "                    \n",
    "                    # Sample next token\n",
    "                    probs = F.softmax(logits, dim=-1)\n",
    "                    next_token = torch.multinomial(probs, num_samples=1)\n",
    "                    generated_tokens.append(next_token.item())\n",
    "                    \n",
    "                    # Append to sequence\n",
    "                    x = torch.cat((x, next_token), dim=1)\n",
    "            \n",
    "            # Decode prediction\n",
    "            prediction_text = decode(generated_tokens, self.itos)\n",
    "            \n",
    "            # Clean up texts\n",
    "            reference_text = reference_text.strip()\n",
    "            prediction_text = prediction_text.strip()\n",
    "            \n",
    "            if len(reference_text) > 0 and len(prediction_text) > 0:\n",
    "                references.append(reference_text)\n",
    "                predictions.append(prediction_text)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  Generated {i + 1}/{num_samples} samples\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating sample {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Successfully generated {len(references)} sample pairs\")\n",
    "    return references, predictions\n",
    "\n",
    "# Add method to the class\n",
    "NanoGPTEvaluator.generate_samples_for_metrics = generate_samples_for_metrics\n",
    "\n",
    "print(\"Text generation method added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec4cebbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom tokenizer method added to NanoGPTEvaluator\n"
     ]
    }
   ],
   "source": [
    "# Add custom tokenizer method for evaluation metrics\n",
    "def get_tokenizer(self):\n",
    "    \"\"\"\n",
    "    Get a tokenizer function that matches the model's vocabulary\n",
    "    \n",
    "    This works for both character-level and token-level models:\n",
    "    - Character-level: stoi maps each character to an index\n",
    "    - Token-level: stoi maps tokens/words to indices\n",
    "    \n",
    "    Returns:\n",
    "        Tokenizer function that splits text according to model's vocabulary\n",
    "    \"\"\"\n",
    "    def custom_tokenizer(text):\n",
    "        \"\"\"Tokenizer that matches the model's vocabulary\"\"\"\n",
    "        tokens = []\n",
    "        for char in text:\n",
    "            if char in self.stoi:\n",
    "                tokens.append(char)\n",
    "        return tokens\n",
    "    \n",
    "    return custom_tokenizer\n",
    "\n",
    "# Add method to the class\n",
    "NanoGPTEvaluator.get_tokenizer = get_tokenizer\n",
    "\n",
    "print(\"Custom tokenizer method added to NanoGPTEvaluator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "44f9fe6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed perplexity calculation method - using only HuggingFace evaluate library\n"
     ]
    }
   ],
   "source": [
    "# Fix the perplexity calculation method\n",
    "def calculate_perplexity(self, data: np.ndarray, batch_size: int = 16, max_batches: int = 100) -> float:\n",
    "    \"\"\"\n",
    "    Calculate perplexity using HuggingFace evaluate library only\n",
    "    \n",
    "    Args:\n",
    "        data: Token data array\n",
    "        batch_size: Batch size for evaluation\n",
    "        max_batches: Maximum number of batches to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        Perplexity value or None if calculation fails\n",
    "    \"\"\"\n",
    "    print(f\"Calculating perplexity with {batch_size} batch size...\")\n",
    "    \n",
    "    if self.perplexity_metric is not None:\n",
    "        try:\n",
    "            # Prepare data for HuggingFace perplexity metric\n",
    "            # Convert tokens to text for the metric\n",
    "            \n",
    "            # Calculate number of batches\n",
    "            seq_len = 256  # Standard sequence length\n",
    "            max_start = len(data) - seq_len\n",
    "            \n",
    "            if max_start <= 0:\n",
    "                print(\"Dataset too small for perplexity calculation\")\n",
    "                return None\n",
    "                \n",
    "            # Limit the number of batches\n",
    "            num_batches = min(max_batches, max_start // batch_size)\n",
    "            \n",
    "            # Collect text samples for perplexity calculation\n",
    "            texts = []\n",
    "            for i in range(num_batches * batch_size):\n",
    "                start_idx = i * (max_start // (num_batches * batch_size))\n",
    "                if start_idx + seq_len <= len(data):\n",
    "                    tokens = data[start_idx:start_idx + seq_len].astype(np.int64)\n",
    "                    text = decode(tokens.tolist(), self.itos)\n",
    "                    if len(text.strip()) > 0:\n",
    "                        texts.append(text)\n",
    "            \n",
    "            if len(texts) == 0:\n",
    "                print(\"No valid texts extracted for perplexity calculation\")\n",
    "                return None\n",
    "                \n",
    "            print(f\"Computing perplexity for {len(texts)} text samples...\")\n",
    "            \n",
    "            # Use HuggingFace evaluate perplexity metric\n",
    "            result = self.perplexity_metric.compute(\n",
    "                predictions=texts,\n",
    "                model_id=\"gpt2\"  # Use a standard reference model\n",
    "            )\n",
    "            perplexity_value = result.get('mean_perplexity', None)\n",
    "            if perplexity_value is not None:\n",
    "                print(f\"Perplexity: {perplexity_value:.4f}\")\n",
    "                return float(perplexity_value)\n",
    "            else:\n",
    "                print(\"Perplexity calculation returned None\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with evaluate library perplexity: {e} | {traceback.format_exc()}\")\n",
    "            print(\"Perplexity calculation failed - returning None\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Perplexity metric not available\")\n",
    "        return None\n",
    "\n",
    "# Update the method in the class\n",
    "NanoGPTEvaluator.calculate_perplexity = calculate_perplexity\n",
    "\n",
    "print(\"Fixed perplexity calculation method - using only HuggingFace evaluate library\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e9217fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU and ROUGE score calculation methods added\n"
     ]
    }
   ],
   "source": [
    "# Add BLEU and ROUGE score calculation methods\n",
    "def calculate_bleu_score(self, references: List[str], predictions: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate BLEU score using HuggingFace evaluate library with custom tokenization\n",
    "    \n",
    "    Args:\n",
    "        references: List of reference texts\n",
    "        predictions: List of predicted texts\n",
    "        \n",
    "    Returns:\n",
    "        BLEU score (0-1 range)\n",
    "    \"\"\"\n",
    "    if self.bleu_metric is None:\n",
    "        raise Exception(\"BLEU metric not available\")\n",
    "    \n",
    "    try:\n",
    "        # Get tokenizer that matches the model's vocabulary\n",
    "        custom_tokenizer = self.get_tokenizer()\n",
    "        \n",
    "        # HuggingFace BLEU expects references as list of lists\n",
    "        formatted_references = [[ref] for ref in references]\n",
    "        \n",
    "        # Use custom tokenizer for BLEU calculation\n",
    "        result = self.bleu_metric.compute(\n",
    "            predictions=predictions,\n",
    "            references=formatted_references,\n",
    "            tokenizer=custom_tokenizer\n",
    "        )\n",
    "        \n",
    "        bleu_score = result.get('bleu', 0.0)\n",
    "        print(f\"  BLEU details: {result}\")\n",
    "        return float(bleu_score)\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error calculating BLEU score: {e} | {traceback.format_exc()}\")\n",
    "\n",
    "def calculate_rouge_score(self, references: List[str], predictions: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate ROUGE scores using HuggingFace evaluate library with custom tokenization\n",
    "    \n",
    "    Args:\n",
    "        references: List of reference texts\n",
    "        predictions: List of predicted texts\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with ROUGE-1, ROUGE-2, and ROUGE-L scores\n",
    "    \"\"\"\n",
    "    if self.rouge_metric is None:\n",
    "        raise ValueError(\"ROUGE metric not available\")\n",
    "    \n",
    "    try:\n",
    "        # Get tokenizer that matches the model's vocabulary\n",
    "        custom_tokenizer = self.get_tokenizer()\n",
    "        \n",
    "        # Use custom tokenizer for ROUGE calculation\n",
    "        result = self.rouge_metric.compute(\n",
    "            predictions=predictions,\n",
    "            references=references,\n",
    "            tokenizer=custom_tokenizer\n",
    "        )\n",
    "        \n",
    "        # Extract F1 scores for each ROUGE variant\n",
    "        rouge_scores = {\n",
    "            'rouge1': float(result.get('rouge1', 0.0)),\n",
    "            'rouge2': float(result.get('rouge2', 0.0)),\n",
    "            'rougeL': float(result.get('rougeL', 0.0))\n",
    "        }\n",
    "        \n",
    "        print(f\"  ROUGE details: {result}\")\n",
    "        return rouge_scores\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error calculating ROUGE scores: {e} | {traceback.format_exc()}\"\n",
    "        \n",
    "        raise Exception(error_msg)\n",
    "\n",
    "# Add methods to the class\n",
    "NanoGPTEvaluator.calculate_bleu_score = calculate_bleu_score\n",
    "NanoGPTEvaluator.calculate_rouge_score = calculate_rouge_score\n",
    "\n",
    "print(\"BLEU and ROUGE score calculation methods added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b239b32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed main evaluation method - properly handles None perplexity values\n"
     ]
    }
   ],
   "source": [
    "def evaluate_dataset(self, data_dir: str, split: str = 'val', batch_size: int = 16, \n",
    "                    max_eval_samples: int = 1000, num_text_samples: int = 50,\n",
    "                    prompt_length: int = 20, generation_length: int = 30, max_tokens = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset split with proper None handling\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing train.bin and val.bin\n",
    "        split: 'train' or 'val'\n",
    "        batch_size: Batch size for evaluation\n",
    "        max_eval_samples: Maximum number of samples for evaluation\n",
    "        num_text_samples: Number of text samples for BLEU/ROUGE\n",
    "        prompt_length: Length of prompt for text generation\n",
    "        generation_length: Length of generated text\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Evaluating on {split} set\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Load data\n",
    "    data = self.load_data(data_dir, split, max_tokens)\n",
    "    \n",
    "    results = {'split': split, 'total_tokens': len(data)}\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    print(\"\\n1. Calculating Perplexity...\")\n",
    "    start_time = time.time()\n",
    "    perplexity = self.calculate_perplexity(data, batch_size, max_batches=min(100, max_eval_samples//batch_size))\n",
    "    results['perplexity'] = perplexity if perplexity is not None else 0.0\n",
    "    \n",
    "    # Handle None perplexity properly\n",
    "    if perplexity is not None:\n",
    "        print(f\"Perplexity: {perplexity:.4f} (took {time.time() - start_time:.2f}s)\")\n",
    "    else:\n",
    "        print(f\"Perplexity: Failed to calculate (took {time.time() - start_time:.2f}s)\")\n",
    "    \n",
    "    # Generate samples and calculate BLEU/ROUGE\n",
    "    if len(data) > 100:  # Only if we have enough data\n",
    "        print(\"\\n2. Generating samples for BLEU/ROUGE evaluation...\")\n",
    "        start_time = time.time()\n",
    "        num_samples = min(num_text_samples, max_eval_samples//20, len(data)//100)  # Reasonable number of samples\n",
    "        references, predictions = self.generate_samples_for_metrics(\n",
    "            data, num_samples, prompt_length, generation_length\n",
    "        )\n",
    "        \n",
    "        if references and predictions:\n",
    "            print(\"\\n3. Calculating BLEU score...\")\n",
    "            bleu_score = self.calculate_bleu_score(references, predictions)\n",
    "            results['bleu'] = bleu_score\n",
    "            print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "            \n",
    "            print(\"\\n4. Calculating ROUGE scores...\")\n",
    "            rouge_scores = self.calculate_rouge_score(references, predictions)\n",
    "            results.update(rouge_scores)\n",
    "            print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "            print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "            print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "            \n",
    "            # Show some example generations\n",
    "            print(\"\\n5. Example generations:\")\n",
    "            for i in range(min(3, len(references))):\n",
    "                print(f\"\\nExample {i+1}:\")\n",
    "                print(f\"Reference: {references[i][:100]}...\")\n",
    "                print(f\"Generated: {predictions[i][:100]}...\")\n",
    "        else:\n",
    "            raise Exception(\"Could not generate samples for BLEU/ROUGE evaluation\")\n",
    "    else:\n",
    "        raise Exception(\"Dataset too small for text generation evaluation\")\n",
    "    \n",
    "    print(f\"\\nEvaluation completed in {time.time() - start_time:.2f}s\")\n",
    "    return results\n",
    "\n",
    "# Replace the method in the class\n",
    "NanoGPTEvaluator.evaluate_dataset = evaluate_dataset\n",
    "\n",
    "print(\"Fixed main evaluation method - properly handles None perplexity values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab6659b",
   "metadata": {},
   "source": [
    "## 5. Initialize the Evaluator\n",
    "\n",
    "Load the model and initialize the evaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7f77bf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing NanoGPT Evaluator...\n",
      "==================================================\n",
      "Model: ../checkpoints/baseline_nanogpt/baseline_nanogpt.pt\n",
      "Data: nanoGPT/data/shakespeare_char | ['input.txt', 'meta.pkl', 'prepare.py', 'readme.md', 'train.bin', 'val.bin']\n",
      "Meta: ../checkpoints/baseline_nanogpt/nanogpt_meta.pkl\n",
      "Batch size: 16\n",
      "Max eval samples: 1000\n",
      "Splits: ['val', 'train']\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model arguments: {'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'block_size': 256, 'bias': False, 'vocab_size': 65, 'dropout': 0.2}\n",
      "number of parameters: 10.65M\n",
      "Model loaded successfully!\n",
      "Number of parameters: 10,745,088\n",
      "number of parameters: 10.65M\n",
      "Model loaded successfully!\n",
      "Number of parameters: 10,745,088\n",
      "HuggingFace evaluation metrics loaded successfully\n",
      "\n",
      "Evaluator initialized successfully!\n",
      "HuggingFace evaluation metrics loaded successfully\n",
      "\n",
      "Evaluator initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize evaluator\n",
    "print(\"Initializing NanoGPT Evaluator...\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model: {CONFIG['model_path']}\")\n",
    "print(f\"Data: {CONFIG['data_dir']} | {os.listdir(CONFIG['data_dir'])}\")\n",
    "print(f\"Meta: {CONFIG['meta_path']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Max eval samples: {CONFIG['max_eval_samples']}\")\n",
    "print(f\"Splits: {CONFIG['splits']}\")\n",
    "\n",
    "try:\n",
    "    evaluator = NanoGPTEvaluator(\n",
    "        CONFIG['model_path'], \n",
    "        CONFIG['meta_path'], \n",
    "        CONFIG['device'],\n",
    "    )\n",
    "    print(\"\\nEvaluator initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing evaluator: {e}\")\n",
    "    evaluator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c039b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1d4ad1a",
   "metadata": {},
   "source": [
    "## 6. Run Evaluation\n",
    "\n",
    "Evaluate the model on the specified dataset splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3df396e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating val split...\n",
      "\n",
      "==================================================\n",
      "Evaluating on val set\n",
      "==================================================\n",
      "Loaded val data: 111,540 tokens\n",
      "\n",
      "1. Calculating Perplexity...\n",
      "Calculating perplexity with 16 batch size...\n",
      "Computing perplexity for 992 text samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–‹         | 4/62 [01:29<21:35, 22.34s/it]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m split...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     results = \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata_dir\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax_eval_samples\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnum_text_samples\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprompt_length\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgeneration_length\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     all_results[split] = results\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m evaluation completed\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mevaluate_dataset_fixed\u001b[39m\u001b[34m(self, data_dir, split, batch_size, max_eval_samples, num_text_samples, prompt_length, generation_length, max_tokens)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m1. Calculating Perplexity...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m perplexity = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcalculate_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_batches\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_eval_samples\u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m results[\u001b[33m'\u001b[39m\u001b[33mperplexity\u001b[39m\u001b[33m'\u001b[39m] = perplexity \u001b[38;5;28;01mif\u001b[39;00m perplexity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Handle None perplexity properly\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mcalculate_perplexity\u001b[39m\u001b[34m(self, data, batch_size, max_batches)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mComputing perplexity for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m text samples...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Use HuggingFace evaluate perplexity metric\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mperplexity_metric\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use a standard reference model\u001b[39;49;00m\n\u001b[32m     52\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m perplexity_value = result.get(\u001b[33m'\u001b[39m\u001b[33mmean_perplexity\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m perplexity_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\bnn\\Lib\\site-packages\\evaluate\\module.py:467\u001b[39m, in \u001b[36mEvaluationModule.compute\u001b[39m\u001b[34m(self, predictions, references, **kwargs)\u001b[39m\n\u001b[32m    465\u001b[39m inputs = {input_name: \u001b[38;5;28mself\u001b[39m.data[input_name][:] \u001b[38;5;28;01mfor\u001b[39;00m input_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._feature_names()}\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m temp_seed(\u001b[38;5;28mself\u001b[39m.seed):\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcompute_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.buf_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    470\u001b[39m     \u001b[38;5;28mself\u001b[39m.buf_writer = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--perplexity\\2d13ebb2e7fff46bcdb4f6893c83bc434f97a9ef7b23f0ed3e16892256232326\\perplexity.py:186\u001b[39m, in \u001b[36mPerplexity._compute\u001b[39m\u001b[34m(self, predictions, model_id, batch_size, add_start_token, device, max_length)\u001b[39m\n\u001b[32m    182\u001b[39m     shift_labels = labels[..., \u001b[32m1\u001b[39m:].contiguous()\n\u001b[32m    183\u001b[39m     shift_attention_mask_batch = attn_mask[..., \u001b[32m1\u001b[39m:].contiguous()\n\u001b[32m    185\u001b[39m     perplexity_batch = torch.exp(\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m         (\u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshift_logits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[43m)\u001b[49m * shift_attention_mask_batch).sum(\u001b[32m1\u001b[39m)\n\u001b[32m    187\u001b[39m         / shift_attention_mask_batch.sum(\u001b[32m1\u001b[39m)\n\u001b[32m    188\u001b[39m     )\n\u001b[32m    190\u001b[39m     ppls += perplexity_batch.tolist()\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mperplexities\u001b[39m\u001b[33m\"\u001b[39m: ppls, \u001b[33m\"\u001b[39m\u001b[33mmean_perplexity\u001b[39m\u001b[33m\"\u001b[39m: np.mean(ppls)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\bnn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\bnn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\bnn\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1310\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1309\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1311\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\bnn\\Lib\\site-packages\\torch\\nn\\functional.py:3462\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3461\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3462\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3466\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3469\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run evaluation on all specified splits\n",
    "if evaluator is not None:\n",
    "    all_results = {}\n",
    "    \n",
    "    for split in CONFIG['splits']:#[CONFIG['splits'][0]]:\n",
    "        print(f\"\\nEvaluating {split} split...\")\n",
    "        try:\n",
    "            results = evaluator.evaluate_dataset(\n",
    "                CONFIG['data_dir'], \n",
    "                split, \n",
    "                CONFIG['batch_size'], \n",
    "                CONFIG['max_eval_samples'],\n",
    "                CONFIG['num_text_samples'],\n",
    "                CONFIG['prompt_length'],\n",
    "                CONFIG['generation_length'],\n",
    "                max_tokens=CONFIG['max_tokens']\n",
    "            )\n",
    "            all_results[split] = results\n",
    "            print(f\"{split} evaluation completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {split} split: {e}\")\n",
    "            continue\n",
    "else:\n",
    "    print(\"Cannot run evaluation - evaluator not initialized\")\n",
    "    all_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f355c",
   "metadata": {},
   "source": [
    "## 7. Results Summary\n",
    "\n",
    "Display a comprehensive summary of all evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abf50c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION SUMMARY\n",
      "============================================================\n",
      "Split Total Tokens Perplexity   BLEU ROUGE-1 ROUGE-2 ROUGE-L\n",
      "  VAL      111,540     0.0000 0.0775  0.4879  0.1386  0.3138\n",
      "TRAIN    1,003,854     0.0000 0.0602  0.5093  0.1217  0.3126\n",
      "\n",
      "Evaluation completed successfully!\n",
      "\n",
      "Results stored in 'evaluation_results' variable for further analysis\n"
     ]
    }
   ],
   "source": [
    "# Print comprehensive summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if all_results:\n",
    "    # Create a summary table\n",
    "    import pandas as pd\n",
    "    \n",
    "    summary_data = []\n",
    "    for split, results in all_results.items():\n",
    "        summary_data.append({\n",
    "            'Split': split.upper(),\n",
    "            'Total Tokens': f\"{results.get('total_tokens', 0):,}\",\n",
    "            'Perplexity': f\"{results.get('perplexity', 0):.4f}\",\n",
    "            'BLEU': f\"{results.get('bleu', 0):.4f}\",\n",
    "            'ROUGE-1': f\"{results.get('rouge1', 0):.4f}\",\n",
    "            'ROUGE-2': f\"{results.get('rouge2', 0):.4f}\",\n",
    "            'ROUGE-L': f\"{results.get('rougeL', 0):.4f}\"\n",
    "        })\n",
    "    \n",
    "    try:\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        print(df.to_string(index=False))\n",
    "    except:\n",
    "        # Fallback if pandas is not available\n",
    "        for split, results in all_results.items():\n",
    "            print(f\"\\n{split.upper()} SET:\")\n",
    "            print(f\"  Total tokens: {results.get('total_tokens', 0):,}\")\n",
    "            print(f\"  Perplexity:   {results.get('perplexity', 0):.4f}\")\n",
    "            print(f\"  BLEU:         {results.get('bleu', 0):.4f}\")\n",
    "            print(f\"  ROUGE-1:      {results.get('rouge1', 0):.4f}\")\n",
    "            print(f\"  ROUGE-2:      {results.get('rouge2', 0):.4f}\")\n",
    "            print(f\"  ROUGE-L:      {results.get('rougeL', 0):.4f}\")\n",
    "    \n",
    "    print(f\"\\nEvaluation completed successfully!\")\n",
    "    \n",
    "    # Store results for further analysis\n",
    "    evaluation_results = all_results\n",
    "    print(f\"\\nResults stored in 'evaluation_results' variable for further analysis\")\n",
    "else:\n",
    "    print(\"No evaluation results to display\")\n",
    "    evaluation_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bb8bb1",
   "metadata": {},
   "source": [
    "## 8. Additional Analysis (Optional)\n",
    "\n",
    "You can use this cell for additional analysis of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb8cf385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional Analysis:\n",
      "==============================\n",
      "\n",
      "Perplexity Comparison:\n",
      "  Training:   0.0000\n",
      "  Validation: 0.0000\n",
      "\n",
      "Text Generation Quality (val):\n",
      "  BLEU 0.0775: Low text similarity\n",
      "  ROUGE-1 0.4879: Good word overlap\n",
      "\n",
      "Text Generation Quality (train):\n",
      "  BLEU 0.0602: Low text similarity\n",
      "  ROUGE-1 0.5093: Good word overlap\n"
     ]
    }
   ],
   "source": [
    "# Additional analysis cell - customize as needed\n",
    "\n",
    "if evaluation_results:\n",
    "    print(\"Additional Analysis:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Compare train vs validation performance\n",
    "    if 'train' in evaluation_results and 'val' in evaluation_results:\n",
    "        train_ppl = evaluation_results['train'].get('perplexity', 0)\n",
    "        val_ppl = evaluation_results['val'].get('perplexity', 0)\n",
    "        \n",
    "        print(f\"\\nPerplexity Comparison:\")\n",
    "        print(f\"  Training:   {train_ppl:.4f}\")\n",
    "        print(f\"  Validation: {val_ppl:.4f}\")\n",
    "        \n",
    "        if train_ppl > 0 and val_ppl > 0:\n",
    "            ratio = val_ppl / train_ppl\n",
    "            print(f\"  Val/Train ratio: {ratio:.4f}\")\n",
    "            \n",
    "            if ratio > 1.5:\n",
    "                print(\"  High validation perplexity suggests overfitting\")\n",
    "            elif ratio < 1.1:\n",
    "                print(f\"  Good generalization - low overfitting\")\n",
    "            else:\n",
    "                print(f\"  Moderate generalization gap\")\n",
    "    \n",
    "    # Text generation quality assessment\n",
    "    for split in evaluation_results:\n",
    "        results = evaluation_results[split]\n",
    "        bleu = results.get('bleu', 0)\n",
    "        rouge1 = results.get('rouge1', 0)\n",
    "        \n",
    "        print(f\"\\nText Generation Quality ({split}):\")\n",
    "        if bleu > 0.3:\n",
    "            print(f\"  BLEU {bleu:.4f}: Good text similarity\")\n",
    "        elif bleu > 0.1:\n",
    "            print(f\"  BLEU {bleu:.4f}: Moderate text similarity\")\n",
    "        else:\n",
    "            print(f\"  BLEU {bleu:.4f}: Low text similarity\")\n",
    "        \n",
    "        if rouge1 > 0.3:\n",
    "            print(f\"  ROUGE-1 {rouge1:.4f}: Good word overlap\")\n",
    "        elif rouge1 > 0.15:\n",
    "            print(f\"  ROUGE-1 {rouge1:.4f}: Moderate word overlap\")\n",
    "        else:\n",
    "            print(f\"  ROUGE-1 {rouge1:.4f}: Low word overlap\")\n",
    "else:\n",
    "    print(\"No results available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3551d6",
   "metadata": {},
   "source": [
    "## 9. Export Results (Optional)\n",
    "\n",
    "Save the evaluation results to a file for later analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac77e559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results exported to: evaluation_results_20250919_144940.json\n"
     ]
    }
   ],
   "source": [
    "# Export results to JSON file\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "if evaluation_results:\n",
    "    # Add metadata\n",
    "    export_data = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'config': CONFIG,\n",
    "        'results': evaluation_results,\n",
    "        'model_info': {\n",
    "            'model_path': CONFIG['model_path'],\n",
    "            'meta_path': CONFIG['meta_path'],\n",
    "            'vocab_size': evaluator.vocab_size if evaluator else None,\n",
    "            'device': evaluator.device if evaluator else None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    output_file = f\"evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    \n",
    "    try:\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(export_data, f, indent=2)\n",
    "        print(f\"Results exported to: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting results: {e}\")\n",
    "else:\n",
    "    print(\"No results to export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e6e3eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
