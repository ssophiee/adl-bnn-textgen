{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e684c217",
   "metadata": {},
   "source": [
    "# NanoGPT Model Evaluation\n",
    "\n",
    "This notebook loads a trained NanoGPT model and evaluates it on train.bin and val.bin datasets using multiple metrics:\n",
    "- **Perplexity**: Measures how well the model predicts the next token\n",
    "- **BLEU Score**: Measures similarity between generated and reference text\n",
    "- **ROUGE Scores**: Measures overlap of n-grams between generated and reference text\n",
    "\n",
    "## Usage\n",
    "1. Configure the paths and parameters in the configuration cell\n",
    "2. Run all cells to perform the evaluation\n",
    "3. View results in the final summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8e75dc",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7abcd78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\notebooks\n",
      "Python path updated with: c:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\notebooks\\baselines, c:\\Users\\hayk_\\OneDrive\\Desktop\\05_LMU_Masters\\04_applied_dl\\adl-bnn-textgen\\notebooks\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Add the required paths for importing\n",
    "current_dir = Path.cwd()\n",
    "sys.path.append(str(current_dir / \"baselines\"))\n",
    "sys.path.append(str(current_dir / \"notebooks\"))\n",
    "\n",
    "print(f\"Current directory: {current_dir}\")\n",
    "print(f\"Python path updated with: {current_dir / 'baselines'}, {current_dir / 'notebooks'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "61fe1329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5634e17d",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set your model paths and evaluation parameters here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51745b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  data_dir: nanoGPT/data/shakespeare\n",
      "  model_path: ../checkpoints/baseline_token_level_nano/token_level_1500_iter.pt\n",
      "  meta_path: ../checkpoints/baseline_nanogpt/nanogpt_meta.pkl\n",
      "  batch_size: 16\n",
      "  max_eval_samples: 1000\n",
      "  device: auto\n",
      "  splits: ['val', 'train']\n",
      "  num_text_samples: 50\n",
      "  prompt_length: 20\n",
      "  generation_length: 30\n",
      "‚úì data_dir: nanoGPT\\data\\shakespeare exists\n",
      "‚úì model_path: ..\\checkpoints\\baseline_token_level_nano\\token_level_1500_iter.pt exists\n",
      "‚úì meta_path: ..\\checkpoints\\baseline_nanogpt\\nanogpt_meta.pkl exists\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # 'data_dir': 'nanoGPT/data/shakespeare_char',\n",
    "    # 'model_path': '../checkpoints/baseline_nanogpt/baseline_nanogpt.pt',\n",
    "    # 'meta_path': '../checkpoints/baseline_nanogpt/nanogpt_meta.pkl',\n",
    "    'data_dir': 'nanoGPT/data/shakespeare',\n",
    "    'model_path': '../checkpoints/baseline_token_level_nano/token_level_1500_iter.pt',\n",
    "    'meta_path': '../checkpoints/baseline_nanogpt/nanogpt_meta.pkl',\n",
    "\n",
    "    'batch_size': 16,\n",
    "    'max_eval_samples': 1000,\n",
    "    'device': 'auto',  # 'auto', 'cpu', or 'cuda'\n",
    "    'splits': ['val', 'train'],  # Dataset splits to evaluate\n",
    "    'num_text_samples': 50,  # Number of text samples for BLEU/ROUGE\n",
    "    'prompt_length': 20,  # Length of prompt for text generation\n",
    "    'generation_length': 30  # Length of generated text\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Check if paths exist\n",
    "for path_key in ['data_dir', 'model_path', 'meta_path']:\n",
    "    path = Path(CONFIG[path_key])\n",
    "    if path.exists():\n",
    "        print(f\"Path exists {path_key}: {path}\")\n",
    "    else:\n",
    "        print(f\"Path not found {path_key}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa5259",
   "metadata": {},
   "source": [
    "## 3. Alternative Utility Functions\n",
    "\n",
    "These functions provide fallback implementations if the utils module is not available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "66c2c364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_model, load_tokenizer, decode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3765de",
   "metadata": {},
   "source": [
    "## 4. NanoGPT Evaluator Class\n",
    "\n",
    "This class handles model loading and evaluation with multiple metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678f9ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì NanoGPTEvaluator class defined\n"
     ]
    }
   ],
   "source": [
    "class NanoGPTEvaluator:\n",
    "    \"\"\"Evaluator for NanoGPT models with multiple metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, meta_path: str, device: str = 'auto'):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the model checkpoint\n",
    "            meta_path: Path to the meta.pkl file containing tokenizer info\n",
    "            device: Device to use ('cpu', 'cuda', or 'auto')\n",
    "        \"\"\"\n",
    "        # Set device\n",
    "        if device == 'auto':\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "\n",
    "        self.model, self.checkpoint = load_model(Path(model_path), self.device)\n",
    "        self.stoi, self.itos = load_tokenizer(Path(meta_path))\n",
    "        \n",
    "        self.vocab_size = len(self.itos)\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Initialize metrics if available\n",
    "        self.metrics = {}\n",
    "        try:\n",
    "            # Load evaluation metrics from HuggingFace evaluate\n",
    "            self.bleu_metric = evaluate.load(\"bleu\")\n",
    "            self.rouge_metric = evaluate.load(\"rouge\")\n",
    "            self.perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "            print(\"HuggingFace evaluation metrics loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load HuggingFace metrics: {e}\")\n",
    "            print(\"Falling back to individual metric libraries...\")\n",
    "            try:\n",
    "                # Fallback to rouge_score library\n",
    "                self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "                print(\"ROUGE scorer initialized\")\n",
    "            except Exception:\n",
    "                self.rouge_scorer = None\n",
    "            self.bleu_metric = None\n",
    "            self.rouge_metric = None\n",
    "            self.perplexity_metric = None\n",
    "\n",
    "print(\"NanoGPTEvaluator class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f531a251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data loading methods added to NanoGPTEvaluator\n"
     ]
    }
   ],
   "source": [
    "# Add data loading methods to the evaluator\n",
    "def load_data(self, data_dir: str, split: str = 'val') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load train.bin or val.bin data\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing the data files\n",
    "        split: 'train' or 'val'\n",
    "        \n",
    "    Returns:\n",
    "        Numpy array of token indices\n",
    "    \"\"\"\n",
    "    filename = f\"{split}.bin\"\n",
    "    filepath = os.path.join(data_dir, filename)\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Data file not found: {filepath}\")\n",
    "    \n",
    "    data = np.memmap(filepath, dtype=np.uint16, mode='r')\n",
    "    print(f\"Loaded {split} data: {len(data):,} tokens\")\n",
    "    return data\n",
    "\n",
    "def get_batch(self, data: np.ndarray, batch_size: int, block_size: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Get a random batch of data for evaluation\n",
    "    \n",
    "    Args:\n",
    "        data: Token data array\n",
    "        batch_size: Number of sequences in the batch\n",
    "        block_size: Length of each sequence\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (input_tokens, target_tokens)\n",
    "    \"\"\"\n",
    "    if len(data) <= block_size:\n",
    "        # If data is smaller than block_size, just use what we have\n",
    "        ix = [0] * batch_size\n",
    "        max_len = len(data) - 1\n",
    "        x = torch.stack([torch.from_numpy(data[0:max_len].astype(np.int64)) for _ in range(batch_size)])\n",
    "        y = torch.stack([torch.from_numpy(data[1:max_len+1].astype(np.int64)) for _ in range(batch_size)])\n",
    "    else:\n",
    "        ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "        x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "        y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    \n",
    "    x, y = x.to(self.device), y.to(self.device)\n",
    "    return x, y\n",
    "\n",
    "# Add methods to the class\n",
    "NanoGPTEvaluator.load_data = load_data\n",
    "NanoGPTEvaluator.get_batch = get_batch\n",
    "\n",
    "print(\"Data loading methods added to NanoGPTEvaluator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1e0740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Perplexity calculation method added\n"
     ]
    }
   ],
   "source": [
    "# Add perplexity calculation method\n",
    "@torch.no_grad()\n",
    "def calculate_perplexity(self, data: np.ndarray, batch_size: int = 16, max_batches: int = 100) -> float:\n",
    "    \"\"\"\n",
    "    Calculate perplexity on the given dataset\n",
    "    \n",
    "    Args:\n",
    "        data: Token data array\n",
    "        batch_size: Batch size for evaluation\n",
    "        max_batches: Maximum number of batches to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        Perplexity score\n",
    "    \"\"\"\n",
    "    print(f\"Calculating perplexity with {batch_size} batch size...\")\n",
    "    \n",
    "    # Try using evaluate library first\n",
    "    if self.perplexity_metric is not None:\n",
    "        try:\n",
    "            # Prepare text for evaluate library\n",
    "            text_samples = []\n",
    "            block_size = min(self.model.config.block_size, 512)  # Limit block size for memory\n",
    "            num_samples = min(max_batches * batch_size, len(data) // block_size)\n",
    "            \n",
    "            for i in range(0, num_samples * block_size, block_size):\n",
    "                if i + block_size < len(data):\n",
    "                    tokens = data[i:i+block_size].astype(np.int64)\n",
    "                    text = decode(tokens.tolist(), self.itos)\n",
    "                    text_samples.append(text)\n",
    "            \n",
    "            if text_samples:\n",
    "                # Use evaluate library\n",
    "                result = self.perplexity_metric.compute(predictions=text_samples, model_id=\"gpt2\")\n",
    "                return result['mean_perplexity']\n",
    "        except Exception as e:\n",
    "            print(f\"Error with evaluate library perplexity: {e}\")\n",
    "            print(\"Falling back to manual calculation...\")\n",
    "    \n",
    "    # Fallback to manual calculation\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    batches_processed = 0\n",
    "    \n",
    "    block_size = self.model.config.block_size\n",
    "    \n",
    "    # Calculate number of possible batches\n",
    "    if len(data) > block_size:\n",
    "        max_possible_batches = (len(data) - block_size) // batch_size\n",
    "    else:\n",
    "        max_possible_batches = 1\n",
    "    \n",
    "    num_batches = min(max_batches, max_possible_batches, 100)  # Limit to reasonable number\n",
    "    \n",
    "    print(f\"Processing {num_batches} batches for perplexity calculation...\")\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        try:\n",
    "            x, y = self.get_batch(data, batch_size, block_size)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, loss = self.model(x, y)\n",
    "            \n",
    "            total_loss += loss.item() * x.numel()\n",
    "            total_tokens += x.numel()\n",
    "            batches_processed += 1\n",
    "            \n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f\"  Processed batch {batch_idx + 1}/{num_batches}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if total_tokens == 0:\n",
    "        return float('inf')\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    print(f\"Processed {batches_processed} batches, {total_tokens:,} tokens\")\n",
    "    return perplexity\n",
    "\n",
    "# Add method to the class\n",
    "NanoGPTEvaluator.calculate_perplexity = calculate_perplexity\n",
    "\n",
    "print(\"Perplexity calculation method added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1596f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Text generation method added\n"
     ]
    }
   ],
   "source": [
    "# Add text generation and metric calculation methods\n",
    "def generate_samples_for_metrics(self, data: np.ndarray, num_samples: int = 50, \n",
    "                               prompt_length: int = 20, generation_length: int = 30) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Generate text samples for BLEU/ROUGE evaluation\n",
    "    \n",
    "    Args:\n",
    "        data: Token data array\n",
    "        num_samples: Number of samples to generate\n",
    "        prompt_length: Length of prompt in tokens\n",
    "        generation_length: Length of generated text in tokens\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (references, predictions)\n",
    "    \"\"\"\n",
    "    print(f\"Generating {num_samples} samples for BLEU/ROUGE evaluation...\")\n",
    "    \n",
    "    references = []\n",
    "    predictions = []\n",
    "    \n",
    "    # Limit samples based on data size\n",
    "    max_possible_samples = max(1, (len(data) - prompt_length - generation_length) // 100)\n",
    "    num_samples = min(num_samples, max_possible_samples)\n",
    "    \n",
    "    print(f\"Generating {num_samples} text samples...\")\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        try:\n",
    "            # Select a random starting position\n",
    "            if len(data) > prompt_length + generation_length + 10:\n",
    "                start_idx = np.random.randint(0, len(data) - prompt_length - generation_length - 10)\n",
    "            else:\n",
    "                start_idx = 0\n",
    "            \n",
    "            # Extract prompt and reference\n",
    "            prompt_tokens = data[start_idx:start_idx + prompt_length].astype(np.int64)\n",
    "            reference_tokens = data[start_idx + prompt_length:start_idx + prompt_length + generation_length].astype(np.int64)\n",
    "            \n",
    "            # Decode reference\n",
    "            reference_text = decode(reference_tokens.tolist(), self.itos)\n",
    "            \n",
    "            # Generate prediction\n",
    "            x = torch.tensor(prompt_tokens, dtype=torch.long, device=self.device)[None, ...]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                generated_tokens = []\n",
    "                for _ in range(generation_length):\n",
    "                    # Crop if sequence gets too long\n",
    "                    x_cond = x if x.size(1) <= self.model.config.block_size else x[:, -self.model.config.block_size:]\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    logits, _ = self.model(x_cond)\n",
    "                    logits = logits[:, -1, :] / 0.8  # temperature\n",
    "                    \n",
    "                    # Sample next token\n",
    "                    probs = F.softmax(logits, dim=-1)\n",
    "                    next_token = torch.multinomial(probs, num_samples=1)\n",
    "                    generated_tokens.append(next_token.item())\n",
    "                    \n",
    "                    # Append to sequence\n",
    "                    x = torch.cat((x, next_token), dim=1)\n",
    "            \n",
    "            # Decode prediction\n",
    "            prediction_text = decode(generated_tokens, self.itos)\n",
    "            \n",
    "            # Clean up texts\n",
    "            reference_text = reference_text.strip()\n",
    "            prediction_text = prediction_text.strip()\n",
    "            \n",
    "            if len(reference_text) > 0 and len(prediction_text) > 0:\n",
    "                references.append(reference_text)\n",
    "                predictions.append(prediction_text)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  Generated {i + 1}/{num_samples} samples\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating sample {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Successfully generated {len(references)} sample pairs\")\n",
    "    return references, predictions\n",
    "\n",
    "# Add method to the class\n",
    "NanoGPTEvaluator.generate_samples_for_metrics = generate_samples_for_metrics\n",
    "\n",
    "print(\"Text generation method added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8349cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì BLEU and ROUGE calculation methods added\n"
     ]
    }
   ],
   "source": [
    "# Add BLEU and ROUGE calculation methods\n",
    "def calculate_bleu_score(self, references: List[str], predictions: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate BLEU score using evaluate library\n",
    "    \n",
    "    Args:\n",
    "        references: List of reference texts\n",
    "        predictions: List of predicted texts\n",
    "        \n",
    "    Returns:\n",
    "        BLEU score\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Use HuggingFace evaluate library for BLEU\n",
    "        # Format for evaluate library: predictions and references should be lists\n",
    "        results = self.bleu_metric.compute(\n",
    "            predictions=predictions, \n",
    "            references=[[ref] for ref in references]\n",
    "        )\n",
    "        return results['bleu']\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating BLEU: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def calculate_rouge_score(self, references: List[str], predictions: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate ROUGE scores using evaluate library\n",
    "    \n",
    "    Args:\n",
    "        references: List of reference texts\n",
    "        predictions: List of predicted texts\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of ROUGE scores\n",
    "    \"\"\"\n",
    " \n",
    "    # Try HuggingFace evaluate library first\n",
    "    if self.rouge_metric is not None:\n",
    "        try:\n",
    "            results = self.rouge_metric.compute(\n",
    "                predictions=predictions, \n",
    "                references=references\n",
    "            )\n",
    "            return {\n",
    "                'rouge1': results.get('rouge1', 0.0),\n",
    "                'rouge2': results.get('rouge2', 0.0), \n",
    "                'rougeL': results.get('rougeL', 0.0)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error with HuggingFace ROUGE: {e}\")\n",
    "    \n",
    "    # Fallback to rouge_score library\n",
    "    if self.rouge_scorer is not None:\n",
    "        try:\n",
    "            rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "            \n",
    "            for ref, pred in zip(references, predictions):\n",
    "                if len(ref.strip()) > 0 and len(pred.strip()) > 0:\n",
    "                    scores = self.rouge_scorer.score(ref, pred)\n",
    "                    rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "                    rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "                    rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "            \n",
    "            # Calculate averages\n",
    "            avg_scores = {}\n",
    "            for key, values in rouge_scores.items():\n",
    "                if values:\n",
    "                    avg_scores[key] = sum(values) / len(values)\n",
    "                else:\n",
    "                    avg_scores[key] = 0.0\n",
    "            \n",
    "            return avg_scores\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating ROUGE: {e}\")\n",
    "    \n",
    "    return {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
    "\n",
    "# Add methods to the class\n",
    "NanoGPTEvaluator.calculate_bleu_score = calculate_bleu_score\n",
    "NanoGPTEvaluator.calculate_rouge_score = calculate_rouge_score\n",
    "\n",
    "print(\"BLEU and ROUGE calculation methods added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e406677e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Main evaluation method added\n",
      "\n",
      "üéâ NanoGPTEvaluator class is ready!\n"
     ]
    }
   ],
   "source": [
    "# Add main evaluation method\n",
    "def evaluate_dataset(self, data_dir: str, split: str = 'val', batch_size: int = 16, \n",
    "                    max_eval_samples: int = 1000, num_text_samples: int = 50,\n",
    "                    prompt_length: int = 20, generation_length: int = 30) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset split\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing train.bin and val.bin\n",
    "        split: 'train' or 'val'\n",
    "        batch_size: Batch size for evaluation\n",
    "        max_eval_samples: Maximum number of samples for evaluation\n",
    "        num_text_samples: Number of text samples for BLEU/ROUGE\n",
    "        prompt_length: Length of prompt for text generation\n",
    "        generation_length: Length of generated text\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Evaluating on {split} set\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Load data\n",
    "    data = self.load_data(data_dir, split)\n",
    "    \n",
    "    results = {'split': split, 'total_tokens': len(data)}\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    print(\"\\n1. Calculating Perplexity...\")\n",
    "    start_time = time.time()\n",
    "    perplexity = self.calculate_perplexity(data, batch_size, max_batches=min(100, max_eval_samples//batch_size))\n",
    "    results['perplexity'] = perplexity\n",
    "    print(f\"Perplexity: {perplexity:.4f} (took {time.time() - start_time:.2f}s)\")\n",
    "    \n",
    "    # Generate samples and calculate BLEU/ROUGE\n",
    "    if len(data) > 100:  # Only if we have enough data\n",
    "        print(\"\\n2. Generating samples for BLEU/ROUGE evaluation...\")\n",
    "        start_time = time.time()\n",
    "        num_samples = min(num_text_samples, max_eval_samples//20, len(data)//100)  # Reasonable number of samples\n",
    "        references, predictions = self.generate_samples_for_metrics(\n",
    "            data, num_samples, prompt_length, generation_length\n",
    "        )\n",
    "        \n",
    "        if references and predictions:\n",
    "            print(\"\\n3. Calculating BLEU score...\")\n",
    "            bleu_score = self.calculate_bleu_score(references, predictions)\n",
    "            results['bleu'] = bleu_score\n",
    "            print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "            \n",
    "            print(\"\\n4. Calculating ROUGE scores...\")\n",
    "            rouge_scores = self.calculate_rouge_score(references, predictions)\n",
    "            results.update(rouge_scores)\n",
    "            print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "            print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "            print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "            \n",
    "            # Show some example generations\n",
    "            print(\"\\n5. Example generations:\")\n",
    "            for i in range(min(3, len(references))):\n",
    "                print(f\"\\nExample {i+1}:\")\n",
    "                print(f\"Reference: {references[i][:100]}...\")\n",
    "                print(f\"Generated: {predictions[i][:100]}...\")\n",
    "        else:\n",
    "            print(\"Could not generate samples for BLEU/ROUGE evaluation\")\n",
    "            results['bleu'] = 0.0\n",
    "            results.update({'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0})\n",
    "    else:\n",
    "        print(\"Dataset too small for text generation evaluation\")\n",
    "        results['bleu'] = 0.0\n",
    "        results.update({'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0})\n",
    "    \n",
    "    print(f\"\\nEvaluation completed in {time.time() - start_time:.2f}s\")\n",
    "    return results\n",
    "\n",
    "# Add method to the class\n",
    "NanoGPTEvaluator.evaluate_dataset = evaluate_dataset\n",
    "\n",
    "print(\"Main evaluation method added\")\n",
    "print(\"\\nNanoGPTEvaluator class is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab6659b",
   "metadata": {},
   "source": [
    "## 5. Initialize the Evaluator\n",
    "\n",
    "Load the model and initialize the evaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f77bf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing NanoGPT Evaluator...\n",
      "==================================================\n",
      "Model: ../checkpoints/baseline_token_level_nano/token_level_1500_iter.pt\n",
      "Data: nanoGPT/data/shakespeare\n",
      "Meta: ../checkpoints/baseline_nanogpt/nanogpt_meta.pkl\n",
      "Batch size: 16\n",
      "Max eval samples: 1000\n",
      "Splits: ['val', 'train']\n",
      "Using device: cpu\n",
      "Loading model from: ..\\checkpoints\\baseline_token_level_nano\\token_level_1500_iter.pt\n",
      "Model arguments: {'n_layer': 8, 'n_head': 8, 'n_embd': 512, 'block_size': 512, 'bias': False, 'vocab_size': 50304, 'dropout': 0.1}\n",
      "number of parameters: 50.93M\n",
      "Model loaded successfully!\n",
      "Number of parameters: 51,192,320\n",
      "Warning: Could not load HuggingFace metrics: To be able to use evaluate-metric/perplexity, you need to install the following dependencies['transformers'] using 'pip install transformers' for instance'\n",
      "Falling back to individual metric libraries...\n",
      "ROUGE scorer initialized\n",
      "\n",
      "‚úÖ Evaluator initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize evaluator\n",
    "print(\"Initializing NanoGPT Evaluator...\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model: {CONFIG['model_path']}\")\n",
    "print(f\"Data: {CONFIG['data_dir']}\")\n",
    "print(f\"Meta: {CONFIG['meta_path']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Max eval samples: {CONFIG['max_eval_samples']}\")\n",
    "print(f\"Splits: {CONFIG['splits']}\")\n",
    "\n",
    "try:\n",
    "    evaluator = NanoGPTEvaluator(\n",
    "        CONFIG['model_path'], \n",
    "        CONFIG['meta_path'], \n",
    "        CONFIG['device']\n",
    "    )\n",
    "    print(\"\\nEvaluator initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing evaluator: {e}\")\n",
    "    evaluator = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d4ad1a",
   "metadata": {},
   "source": [
    "## 6. Run Evaluation\n",
    "\n",
    "Evaluate the model on the specified dataset splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3df396e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Evaluating val split...\n",
      "\n",
      "==================================================\n",
      "Evaluating on val set\n",
      "==================================================\n",
      "Loaded val data: 36,059 tokens\n",
      "\n",
      "1. Calculating Perplexity...\n",
      "Calculating perplexity with 16 batch size...\n",
      "Processing 62 batches for perplexity calculation...\n",
      "  Processed batch 1/62\n",
      "  Processed batch 21/62\n",
      "  Processed batch 41/62\n",
      "  Processed batch 61/62\n",
      "Processed 62 batches, 507,904 tokens\n",
      "Perplexity: 142.1983 (took 1254.42s)\n",
      "\n",
      "2. Generating samples for BLEU/ROUGE evaluation...\n",
      "Generating 50 samples for BLEU/ROUGE evaluation...\n",
      "Generating 50 text samples...\n",
      "Error generating sample 0: 16398\n",
      "Error generating sample 1: 198\n",
      "Error generating sample 2: 1498\n",
      "Error generating sample 3: 612\n",
      "Error generating sample 4: 198\n",
      "Error generating sample 5: 4957\n",
      "Error generating sample 6: 198\n",
      "Error generating sample 7: 391\n",
      "Error generating sample 8: 440\n",
      "Error generating sample 9: 1549\n",
      "Error generating sample 10: 198\n",
      "Error generating sample 11: 290\n",
      "Error generating sample 12: 7206\n",
      "Error generating sample 13: 278\n",
      "Error generating sample 14: 1334\n",
      "Error generating sample 15: 30779\n",
      "Error generating sample 16: 5446\n",
      "Error generating sample 17: 2396\n",
      "Error generating sample 18: 198\n",
      "Error generating sample 19: 4240\n",
      "Error generating sample 20: 865\n",
      "Error generating sample 21: 284\n",
      "Error generating sample 22: 345\n",
      "Error generating sample 23: 534\n",
      "Error generating sample 24: 198\n",
      "Error generating sample 25: 38733\n",
      "Error generating sample 26: 3069\n",
      "Error generating sample 27: 198\n",
      "Error generating sample 28: 1752\n",
      "Error generating sample 29: 198\n",
      "Error generating sample 30: 26246\n",
      "Error generating sample 31: 26190\n",
      "Error generating sample 32: 17655\n",
      "Error generating sample 33: 266\n",
      "Error generating sample 34: 290\n",
      "Error generating sample 35: 2156\n",
      "Error generating sample 36: 338\n",
      "Error generating sample 37: 198\n",
      "Error generating sample 38: 198\n",
      "Error generating sample 39: 262\n",
      "Error generating sample 40: 428\n",
      "Error generating sample 41: 198\n",
      "Error generating sample 42: 198\n",
      "Error generating sample 43: 284\n",
      "Error generating sample 44: 198\n",
      "Error generating sample 45: 67\n",
      "Error generating sample 46: 290\n",
      "Error generating sample 47: 37769\n",
      "Error generating sample 48: 42012\n",
      "Error generating sample 49: 3398\n",
      "Successfully generated 0 sample pairs\n",
      "Could not generate samples for BLEU/ROUGE evaluation\n",
      "\n",
      "Evaluation completed in 0.03s\n",
      "‚úÖ val evaluation completed\n",
      "\n",
      "üîÑ Evaluating train split...\n",
      "\n",
      "==================================================\n",
      "Evaluating on train set\n",
      "==================================================\n",
      "Loaded train data: 301,966 tokens\n",
      "\n",
      "1. Calculating Perplexity...\n",
      "Calculating perplexity with 16 batch size...\n",
      "Processing 62 batches for perplexity calculation...\n",
      "  Processed batch 1/62\n",
      "  Processed batch 21/62\n",
      "  Processed batch 41/62\n",
      "  Processed batch 61/62\n",
      "Processed 62 batches, 507,904 tokens\n",
      "Perplexity: 15.7473 (took 580.79s)\n",
      "\n",
      "2. Generating samples for BLEU/ROUGE evaluation...\n",
      "Generating 50 samples for BLEU/ROUGE evaluation...\n",
      "Generating 50 text samples...\n",
      "Error generating sample 0: 286\n",
      "Error generating sample 1: 477\n",
      "Error generating sample 2: 788\n",
      "Error generating sample 3: 465\n",
      "Error generating sample 4: 11386\n",
      "Error generating sample 5: 284\n",
      "Error generating sample 6: 30577\n",
      "Error generating sample 7: 1477\n",
      "Error generating sample 8: 1870\n",
      "Error generating sample 9: 1353\n",
      "Error generating sample 10: 312\n",
      "Error generating sample 11: 561\n",
      "Error generating sample 12: 14210\n",
      "Error generating sample 13: 198\n",
      "Error generating sample 14: 318\n",
      "Error generating sample 15: 262\n",
      "Error generating sample 16: 11027\n",
      "Error generating sample 17: 340\n",
      "Error generating sample 18: 198\n",
      "Error generating sample 19: 28042\n",
      "Error generating sample 20: 272\n",
      "Error generating sample 21: 2728\n",
      "Error generating sample 22: 17903\n",
      "Error generating sample 23: 198\n",
      "Error generating sample 24: 15275\n",
      "Error generating sample 25: 198\n",
      "Error generating sample 26: 612\n",
      "Error generating sample 27: 287\n",
      "Error generating sample 28: 1808\n",
      "Error generating sample 29: 198\n",
      "Error generating sample 30: 611\n",
      "Error generating sample 31: 43210\n",
      "Error generating sample 32: 2390\n",
      "Error generating sample 33: 7954\n",
      "Error generating sample 34: 6737\n",
      "Error generating sample 35: 36606\n",
      "Error generating sample 36: 198\n",
      "Error generating sample 37: 11906\n",
      "Error generating sample 38: 8395\n",
      "Error generating sample 39: 17495\n",
      "Error generating sample 40: 1015\n",
      "Error generating sample 41: 198\n",
      "Error generating sample 42: 1273\n",
      "Error generating sample 43: 198\n",
      "Error generating sample 44: 1208\n",
      "Error generating sample 45: 198\n",
      "Error generating sample 46: 47166\n",
      "Error generating sample 47: 290\n",
      "Error generating sample 48: 3148\n",
      "Error generating sample 49: 314\n",
      "Successfully generated 0 sample pairs\n",
      "Could not generate samples for BLEU/ROUGE evaluation\n",
      "\n",
      "Evaluation completed in 0.01s\n",
      "‚úÖ train evaluation completed\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation on all specified splits\n",
    "if evaluator is not None:\n",
    "    all_results = {}\n",
    "    \n",
    "    for split in CONFIG['splits']:\n",
    "        print(f\"\\nEvaluating {split} split...\")\n",
    "        try:\n",
    "            results = evaluator.evaluate_dataset(\n",
    "                CONFIG['data_dir'], \n",
    "                split, \n",
    "                CONFIG['batch_size'], \n",
    "                CONFIG['max_eval_samples'],\n",
    "                CONFIG['num_text_samples'],\n",
    "                CONFIG['prompt_length'],\n",
    "                CONFIG['generation_length']\n",
    "            )\n",
    "            all_results[split] = results\n",
    "            print(f\"{split} evaluation completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {split} split: {e}\")\n",
    "            continue\n",
    "else:\n",
    "    print(\"Cannot run evaluation - evaluator not initialized\")\n",
    "    all_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f355c",
   "metadata": {},
   "source": [
    "## 7. Results Summary\n",
    "\n",
    "Display a comprehensive summary of all evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf50c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä EVALUATION SUMMARY\n",
      "============================================================\n",
      "Split Total Tokens Perplexity   BLEU ROUGE-1 ROUGE-2 ROUGE-L\n",
      "  VAL       36,059   142.1983 0.0000  0.0000  0.0000  0.0000\n",
      "TRAIN      301,966    15.7473 0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "‚úÖ Evaluation completed successfully!\n",
      "\n",
      "üíæ Results stored in 'evaluation_results' variable for further analysis\n"
     ]
    }
   ],
   "source": [
    "# Print comprehensive summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if all_results:\n",
    "    # Create a summary table\n",
    "    import pandas as pd\n",
    "    \n",
    "    summary_data = []\n",
    "    for split, results in all_results.items():\n",
    "        summary_data.append({\n",
    "            'Split': split.upper(),\n",
    "            'Total Tokens': f\"{results.get('total_tokens', 0):,}\",\n",
    "            'Perplexity': f\"{results.get('perplexity', 0):.4f}\",\n",
    "            'BLEU': f\"{results.get('bleu', 0):.4f}\",\n",
    "            'ROUGE-1': f\"{results.get('rouge1', 0):.4f}\",\n",
    "            'ROUGE-2': f\"{results.get('rouge2', 0):.4f}\",\n",
    "            'ROUGE-L': f\"{results.get('rougeL', 0):.4f}\"\n",
    "        })\n",
    "    \n",
    "    try:\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        print(df.to_string(index=False))\n",
    "    except:\n",
    "        # Fallback if pandas is not available\n",
    "        for split, results in all_results.items():\n",
    "            print(f\"\\n{split.upper()} SET:\")\n",
    "            print(f\"  Total tokens: {results.get('total_tokens', 0):,}\")\n",
    "            print(f\"  Perplexity:   {results.get('perplexity', 0):.4f}\")\n",
    "            print(f\"  BLEU:         {results.get('bleu', 0):.4f}\")\n",
    "            print(f\"  ROUGE-1:      {results.get('rouge1', 0):.4f}\")\n",
    "            print(f\"  ROUGE-2:      {results.get('rouge2', 0):.4f}\")\n",
    "            print(f\"  ROUGE-L:      {results.get('rougeL', 0):.4f}\")\n",
    "    \n",
    "    print(f\"\\nEvaluation completed successfully!\")\n",
    "    \n",
    "    # Store results for further analysis\n",
    "    evaluation_results = all_results\n",
    "    print(f\"\\nResults stored in 'evaluation_results' variable for further analysis\")\n",
    "else:\n",
    "    print(\"No evaluation results to display\")\n",
    "    evaluation_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bb8bb1",
   "metadata": {},
   "source": [
    "## 8. Additional Analysis (Optional)\n",
    "\n",
    "You can use this cell for additional analysis of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8cf385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Additional Analysis:\n",
      "==============================\n",
      "\n",
      "üìà Perplexity Comparison:\n",
      "  Training:   15.7473\n",
      "  Validation: 142.1983\n",
      "  Val/Train ratio: 9.0300\n",
      "  ‚ö†Ô∏è  High validation perplexity suggests overfitting\n",
      "\n",
      "üìù Text Generation Quality (val):\n",
      "  BLEU 0.0000: ‚ùå Low text similarity\n",
      "  ROUGE-1 0.0000: ‚ùå Low word overlap\n",
      "\n",
      "üìù Text Generation Quality (train):\n",
      "  BLEU 0.0000: ‚ùå Low text similarity\n",
      "  ROUGE-1 0.0000: ‚ùå Low word overlap\n"
     ]
    }
   ],
   "source": [
    "# Additional analysis cell - customize as needed\n",
    "\n",
    "if evaluation_results:\n",
    "    print(\"Additional Analysis:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Compare train vs validation performance\n",
    "    if 'train' in evaluation_results and 'val' in evaluation_results:\n",
    "        train_ppl = evaluation_results['train'].get('perplexity', 0)\n",
    "        val_ppl = evaluation_results['val'].get('perplexity', 0)\n",
    "        \n",
    "        print(f\"\\nPerplexity Comparison:\")\n",
    "        print(f\"  Training:   {train_ppl:.4f}\")\n",
    "        print(f\"  Validation: {val_ppl:.4f}\")\n",
    "        \n",
    "        if train_ppl > 0 and val_ppl > 0:\n",
    "            ratio = val_ppl / train_ppl\n",
    "            print(f\"  Val/Train ratio: {ratio:.4f}\")\n",
    "            \n",
    "            if ratio > 1.5:\n",
    "                print(\"  High validation perplexity suggests overfitting\")\n",
    "            elif ratio < 1.1:\n",
    "                print(f\"  Good generalization - low overfitting\")\n",
    "            else:\n",
    "                print(f\"  Moderate generalization gap\")\n",
    "    \n",
    "    # Text generation quality assessment\n",
    "    for split in evaluation_results:\n",
    "        results = evaluation_results[split]\n",
    "        bleu = results.get('bleu', 0)\n",
    "        rouge1 = results.get('rouge1', 0)\n",
    "        \n",
    "        print(f\"\\nText Generation Quality ({split}):\")\n",
    "        if bleu > 0.3:\n",
    "            print(f\"  BLEU {bleu:.4f}: Good text similarity\")\n",
    "        elif bleu > 0.1:\n",
    "            print(f\"  BLEU {bleu:.4f}: Moderate text similarity\")\n",
    "        else:\n",
    "            print(f\"  BLEU {bleu:.4f}: Low text similarity\")\n",
    "        \n",
    "        if rouge1 > 0.3:\n",
    "            print(f\"  ROUGE-1 {rouge1:.4f}: Good word overlap\")\n",
    "        elif rouge1 > 0.15:\n",
    "            print(f\"  ROUGE-1 {rouge1:.4f}: Moderate word overlap\")\n",
    "        else:\n",
    "            print(f\"  ROUGE-1 {rouge1:.4f}: Low word overlap\")\n",
    "else:\n",
    "    print(\"No results available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3551d6",
   "metadata": {},
   "source": [
    "## 9. Export Results (Optional)\n",
    "\n",
    "Save the evaluation results to a file for later analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac77e559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Results exported to: evaluation_results_20250912_162939.json\n"
     ]
    }
   ],
   "source": [
    "# Export results to JSON file\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "if evaluation_results:\n",
    "    # Add metadata\n",
    "    export_data = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'config': CONFIG,\n",
    "        'results': evaluation_results,\n",
    "        'model_info': {\n",
    "            'model_path': CONFIG['model_path'],\n",
    "            'meta_path': CONFIG['meta_path'],\n",
    "            'vocab_size': evaluator.vocab_size if evaluator else None,\n",
    "            'device': evaluator.device if evaluator else None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    output_file = f\"evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    \n",
    "    try:\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(export_data, f, indent=2)\n",
    "        print(f\"Results exported to: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting results: {e}\")\n",
    "else:\n",
    "    print(\"No results to export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e6e3eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
