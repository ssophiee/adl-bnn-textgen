
======================================================================
BAYESIAN MODEL TRAINING SUMMARY
======================================================================
Sampler: VI
Generated: 2025-10-14T18:01:36.612907

CONFIGURATION:
----------------------------------------------------------------------
  batch_size: 32
  train_samples: 2000
  max_seq_length: 128
  num_epochs: 5
  learning_rate: 1e-05
  temperature: 0.01
  vi_n_samples: 1
  prior_std: 0.001
  prior_strength: 10.0
  init_log_scale: -10
  max_log_scale: -5
  num_samples: 10
  max_new_tokens: 100
  generation_temperature: 0.8
  save_dir: checkpoints/samplers
  wandb_project: bayesian-nanogpt
  weight_decay: 0.01
  kl_weight: 1.0

TRAINING SUMMARY:
----------------------------------------------------------------------
Total Epochs: 5
Final Training Loss: 1.2620
Final Log Posterior: -4208245.4762
Total Training Time: 2869.45s
Avg Time per Epoch: 573.89s

EVALUATION RESULTS:
----------------------------------------------------------------------

Deterministic Model (Original):
  Loss: 0.5185
  Perplexity: 1.6796

Bayesian Posterior Mean:
  Loss: 0.5168
  Perplexity: 1.6766
  Improvement: +0.0018
  Status: ✓ Better than deterministic

Posterior Sampling (5 samples):
  Mean Loss: 65.0818 ± 7.9688
  Loss Range: [56.7589, 75.0763]

Uncertainty Quantification:
  Avg Predictive Entropy: 1.7073
  Entropy Range: [1.4288, 2.0410]

======================================================================
