
======================================================================
BAYESIAN MODEL TRAINING SUMMARY
======================================================================
Sampler: VI
Generated: 2025-10-29T00:24:23.684422

CONFIGURATION:
----------------------------------------------------------------------
  batch_size: 16
  train_samples: 10000
  max_seq_length: 128
  num_epochs: 15
  learning_rate: 0.0001
  temperature: 1
  vi_n_samples: 1
  prior_std: 1
  prior_strength: 10.0
  init_log_scale: -5
  max_log_scale: 2
  prior_beta: 0.0005
  num_samples: 10
  max_new_tokens: 100
  generation_temperature: 0.8
  save_dir: checkpoints/samplers
  wandb_project: bayesian-nanogpt
  weight_decay: 0.01
  kl_weight: 1.0

TRAINING SUMMARY:
----------------------------------------------------------------------
Total Epochs: 15
Final Training Loss: 0.7728
Final Log Posterior: -12636.2332
Total Training Time: 47060.16s
Avg Time per Epoch: 3137.34s

EVALUATION RESULTS:
----------------------------------------------------------------------

Deterministic Model (Original):
  Loss: 0.5423
  Perplexity: 1.7199

Bayesian Posterior Mean:
  Loss: 0.2917
  Perplexity: 1.3387
  Improvement: +0.2505
  Status: ✓ Better than deterministic

Posterior Sampling (5 samples):
  Mean Loss: 0.3143 ± 0.0201
  Loss Range: [0.2832, 0.3450]

Uncertainty Quantification:
  Avg Predictive Entropy: 0.5606
  Entropy Range: [0.0021, 1.6723]

Parameter Uncertainty:
  Avg Parameter Std: 0.008821

======================================================================
