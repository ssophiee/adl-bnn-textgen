
======================================================================
BAYESIAN MODEL TRAINING SUMMARY
======================================================================
Sampler: VI
Generated: 2025-10-14T16:59:12.382654

CONFIGURATION:
----------------------------------------------------------------------
  batch_size: 16
  num_epochs: 1
  learning_rate: 5e-06
  max_new_tokens: 100
  generation_temperature: 0.8
  num_samples: 10
  max_seq_length: 128
  train_samples: 500
  save_dir: checkpoints/samplers
  wandb_project: bayesian-nanogpt
  temperature: 0.001
  vi_n_samples: 1
  prior_std: 0.01

TRAINING SUMMARY:
----------------------------------------------------------------------
Total Epochs: 1
Final Training Loss: 1.1638
Final Log Posterior: -91198.3491
Total Training Time: 82.98s
Avg Time per Epoch: 82.98s

EVALUATION RESULTS:
----------------------------------------------------------------------

Deterministic Model (Original):
  Loss: 0.5423
  Perplexity: 1.7199

Bayesian Posterior Mean:
  Loss: 0.5421
  Perplexity: 1.7197
  Improvement: +0.0001
  Status: ✓ Better than deterministic

Posterior Sampling (5 samples):
  Mean Loss: 70.6939 ± 4.8481
  Loss Range: [61.9799, 75.3669]

Uncertainty Quantification:
  Avg Predictive Entropy: 1.6918
  Entropy Range: [1.6137, 1.9768]

======================================================================
